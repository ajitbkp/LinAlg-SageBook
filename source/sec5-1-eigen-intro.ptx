<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec5-1-eigen-intro" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Eigenvalues and Eigenvectors</title>
<introduction>
  <p>
    In this chapter, we extend our study of linear transformations to the concepts of 
    <term>eigenvalues</term> and <term>eigenvectors</term>, essential for understanding 
    how certain vectors retain their direction under transformation—merely being scaled 
    by a scalar factor.
  </p>

  <p>
    These ideas lie at the heart of many core themes in linear algebra. In particular, 
    they lay the mathematical foundation for:
  </p>

  <ul>
    <li><em>Diagonalization</em> of matrices—simplifying linear transformations to their most transparent form.</li>
    <li><em>Dynamical systems</em>, including Markov chains and population models, where long-term behavior is driven by dominant eigenvalues.</li>
    <li>Methods for solving <em>systems of linear differential equations</em> via eigen-decomposition.</li>
    <li>Dimensional reduction techniques such as <em>Principal Component Analysis (PCA)</em>, where eigenvalues guide us in identifying principal directions.</li>
    <li>Web algorithms such as <em>PageRank</em>, leveraging eigenvectors to reveal stable structures.</li>
  </ul>

  <p>
    Throughout the chapter, we will combine conceptual insight with computational 
    exploration using <c>SageMath</c>, helping you see not only 
    <q>what</q> eigenvalues and eigenvectors are, but also <q>how</q> to compute them 
    effectively and <q>where</q> they matter in applications that span mathematics and 
    applied sciences.
  </p>

</introduction>   
<subsection xml:id="subsec-egenvalues-eigencecs">
  <title>Eigenvalues and Eigenvectors</title>

  
  <p>
  Suppose <m>T</m> is a linear map from a vector space <m>V</m> to itself. 
  For a given <m>v</m>, <m>T(v)</m> is a vector in <m>V</m>. 
  Look the <xref ref="fig-eigen1"/> in which image of vectors <m>v_1,\ldots v_{24}</m> 
  under the linear map <m>T(x,y)=(x/2+y,x+y/2)</m> are shown. Notice that 
  <m>v_3</m> and <m>T(v_3)</m> are parallel, similarly <m>v_{21}</m> and <m>T(v_{21})</m> are parallel. 
  
<figure xml:id="fig-eigen1">
            <image width="65%" source="images/eigen_fig1.png"/>
             <caption>Image of vectors under <m>T(x,y)=(x/2+y,x+y/2).</m></caption>.
</figure>
 You can also observe that <m>v_{15}</m> and <m>T(v_{15})</m> are parallel, 
 but this this expected as <m>v_{15}=-v_3</m>. Similarly  
 <m>v_{9}</m> and <m>T(v_{9})</m> are parallel. Such vectors are called 
 eigenvectors of <m>T</m>. In particular, <m>v</m> and <m>T(v)</m> are parallel, 
 then <m>T(v)=\lambda v</m> for some <m>\lambda\in \R</m>, such a <m>\lambda</m>
 is called the eigenvalue corresonidng to eigenvector <m>v</m>. What happens 
 if <m>v=0</m>? 
</p>

<p>
  You can also explore the same while rotating a vector <m>v</m> and observing 
  what happens to its image using the interactive diagram below 
  (see <xref ref="fig-eigen2"/>).
</p>

<figure xml:id="fig-eigen2">
    <caption>Exploration of eigenvectors</caption>
    <interactive dark-mode-enabled="yes" xml:id="Eigen-Applet1" platform="sage" width="80%" aspect="4:5">
      <slate surface="sage">
        @ interact
        def Vec_Circle(nframes=45, A=matrix(QQ,[[1/2, 1],[1, 1/2]])):
            angles = [2*pi*k/nframes for k in range(nframes)]

            frames = []
            
            for theta in angles:
                # point on the unit circle
                x = 1/2*cos(theta)
                y = 1/2*sin(theta)
                v = vector([x,y])
                Av = A*v
                
                G = plot(v, color="blue")  # moving vector v
                G += text("$v$",(v[0]+0.05*v[0],v[1]+0.05*v[1]),color='blue')
                G += plot(Av, color="red")  # moving vector Av
                G += text("$Av$",(Av[0]+0.05*Av[0],Av[1]+0.05*Av[1]),color='red')
                G += circle((0,0), 1/2, color="black",ticks=[[],[]])   # unit circle
                frames.append(G)

            # create and show animation
            a = animate(frames)
            show(a)
     </slate>
    </interactive>
</figure>

<p> These exploration leads to the following definition of 
  eigenvector and eigenvalues.
</p>

        <definition xml:id="def-eigenvector"><title>Eigenvalues and Eigenvectors</title>
               
            <statement>
              <p>
                Let <m>T</m> be a linear transformation from <m>\mathbb{R}^n\to \R^n</m>.
                A real number (scalar) is called an <em>eigenvalue</em>
                of <m>T</m> if there exists a non zero vector <m>v\in \R^n</m>
                (called an <term> eigenvector</term>
                corresponding to eigenvalue <m>\lambda</m>)
                if <m>T(v)=\lambda v</m>.
                That is, if <m>T(v)</m> is parallel to <m>v</m>.
              </p>
              <p>
                Thus if <m>T(v)=\lambda v</m>,
                then <m>{\lambda I-T}(v)=0</m>,
                where <m>I</m> is identity transformation on <m>V</m>.
              </p>
              <p>
                If <m>A</m> is an <m>n\times n</m> real matrix matrix,
                then we know that <m>T_A(x)=Ax</m> is a linear transformation induced by <m>A</m>.
                We can define eigenvalue of <m>A</m> as eigenvalue of <m>T_A</m>.
                In particular, real number is called an <em>eigenvalue</em>
                of <m>A</m> if there exists a non zero vector <m>v\in \R^n</m>
                (called an <em> eigenvector</em>
                corresponding to eigenvalue <m>\lambda</m>)
                if <m>Av=\lambda v</m>.
              </p>
            </statement>
          </definition>

          <example>
            <statement>
              <p>
                Let <m>A=\left(\begin{array}{rr} 1 \amp 2 \\ -1 \amp 4 \end{array} \right)</m>.
                Consider a vector <m>u=\begin{pmatrix}1\\1 \end{pmatrix}</m>.
                Then <m>Au=\begin{pmatrix}3\\3 \end{pmatrix} =3\begin{pmatrix}1\\1 \end{pmatrix} =3u</m>.
                Hence <m>u</m> is an eigenvector and <m>\lambda = 3</m> is an eigenvalue.
              </p>
              <p>
                Consider <m>v=\begin{pmatrix}2\\1 \end{pmatrix}</m>.
                Then it is easy to check that <m>Av=\begin{pmatrix}4\\2 \end{pmatrix} =2\begin{pmatrix}2\\1 \end{pmatrix}</m>.
                Hence <m>v</m> is also an eigenvector and <m>\lambda = 2</m> is an eigenvalue.
              </p>
            </statement>
          </example>

          <example>
          <statement>
            <p>
              If <m>T</m> is an identity transformation from <m>\R^n\to \R^n</m>, then 
              every nonzero vector is an eigenvector corresponding the eiegenvalue 1. 
              The same is true for <m>n\times n </m>  identity matrix.
            </p>
          </statement>
          </example>

          <example>
            <statement>
              <p>
                Consider the matrix of rotation
                <m>R_\theta</m> in anti-clock wise by an angle <m>\theta\neq n\pi</m> for <m>n\in \Z</m>.
                Then it is easy to see that
                <m>R_\theta</m> does not have an eigenvector.
                Thus not all square matrices have eigenvectors.
              </p>
            </statement>
          </example>

          <remark>
            <p>
              If <m>\lambda</m> is an eigenvalue of <m>A</m> with corresponding eigenvector <m>v\neq 0</m>.
              Then any scalar multiple of <m>v</m> is also an eigenvector corresponding to the same 
              eigenvalue <m>\lambda</m>.  </p>
              <p>
                If <m>Av=\lambda v</m>, then 
                <me>
                  A(\alpha v)=\alpha Av=\alpha \lambda v =\lambda (\alpha ).
                </me>
            </p>
          </remark>

          <p>
            Let us analyze the notion of eigenvalues and eigenvector.
            If <m>v</m> is a eigenvector corresponding to an eigenvalue <m>\lambda</m>.
            Then <m>Av=\lambda v</m>.
            This implies <m>(\lambda I-A)v=0</m>,
            where <m>I</m> is <m>n\times n</m> identity matrix.
            This means that the homogeneous system
            <m>(\lambda I-A)x=0</m> has a non zero solution, namely <m>v</m>.
            Hence <m>\det{(\lambda I-A)}=0</m>.
            Notice that <m>\det(A-\lambda I)</m> is a polynomial
            (called the characteristic polynomial of <m>A</m>)
            of degree <m>n</m> in <m>\lambda</m>.
            Thus if <m>Av=\lambda v</m>,
            then <m>\lambda</m> is a root of the the characteristic polynomial <m>\det(A-\lambda I)</m>.
            By fundamental theorem of algebra an
            <m>n\times n</m> real matrix can have at most <m>n</m> real eigenvalues.
            The equation <m>\det(A-\lambda I)=0</m> is called
            <em>characteristic equation</em> of <m>A</m>.
          </p>

          <p>
            We can write <m>\det(A-\lambda I)=0</m> as 
            <me>\det(A-\lambda I)=\lambda^n-c_1\lambda^{n-1}+c_2\lambda^{n-2}-\cdots+(-1)^nc_n=0.
            </me>
            If <m>\lambda_1,\ldots, \lambda_n</m> are roots of the characteristics equation,
            then using the theory of equations one can show that
            <mdn>
              <mrow>\lambda_1+\cdots+\lambda_n\amp = c_1={\rm trace}(A)</mrow>
                <mrow> \lambda_1\lambda_2+\cdots +\lambda_{n-1}\lambda_n \amp =  c_2</mrow>
                <mrow  number="no"> \vdots \amp </mrow>
                <mrow> \lambda_1\lambda_2\cdots\lambda_n \amp = c_n = \det(A)</mrow>
            </mdn>.
          </p>
          <p>
            Thus we have the following.
          </p>
          <theorem xml:id="thm-sum-produc-eiegn">
            <statement>
              <p>
               Let <m>A</m> be an <m>n\times n</m> real matrix. Then (i) the sum of eigenvalues of <m>A</m> is the 
               trace of <m>A</m> and (ii) the product of eigenvalues is the determeninat of <m>A</m>.
              </p>
            </statement>
          </theorem>
          
          <example xml:id="eigen_eg0">
            <statement>
              <p>
                Let <m>A = \begin{pmatrix}1 \amp 1 \amp 1\\ 1 \amp 1 \amp 1\\1 \amp 1 \amp 1 \end{pmatrix}</m>.
                What are eigenvalues and eigenvectors of <m>A</m>?
              </p>
              <p>
                Note that <m>Ae_1=Ae_2=Ae_3=e_1+e_2+e_3</m>.
                This means <m>A(e_1+e_2+e_3)=3(e_2+e_2+e_3)</m>.
                Hence <m>3</m> is an eigenvalue and
                <m>e_1+e_2+e_3=\begin{pmatrix}1 \\1\\1 \end{pmatrix}</m> is an eigenvectors w.r.t. eigenvalue 3.
              </p>
              <p>
                Also <m>A(e_1-e_2)=0</m>.
                Hence <m>0</m> is an eigenvalue and
                <m>e_1-e_2=\begin{pmatrix}1 \\-1\\0 \end{pmatrix}</m> is an eigenvector corresponding to the eigenvalue 0.
                Also, <m>e_1-e_3</m> and <m>e_2-e_3</m> are also eigenvectors corresponding to the eigenvalue 0.
              </p>
              <p>
                Note that in this example,
                we are able to find eigenvalues and eigenvectors by inspection and without going through characteristic polynomials.
              </p>
              <p>
                What will be generalization of this example?
              </p>
            </statement>
          </example>

          <example xml:id="eigen_eg0b">
            <statement>
              <p>
                Let <m>A = \begin{pmatrix}1 \amp t \amp t\\ t \amp 1 \amp t\\t \amp t \amp 1 \end{pmatrix}</m>.
                What are eigenvalues of <m>A</m>?
              </p>
              <p>
                The trace of <m>A</m> is 3.
                The <m>\det{(A)}=2t^3 - 3t^2 + 1=(2t + 1)(t - 1)^2</m>.
                Since sum of eigenvalues is 3 and the product of eigenvalues is <m>\det{(A)}</m>,
                it is easy to guess that <m>\lambda_1 =2t+1</m>,
                <m>\lambda_2=\lambda_3=1-t</m> are eigenvalues of <m>A</m>.
              </p>
              <p>
                We can adopt a procedure similar to <xref ref="eigen_eg0b"/> to show that 
                <m>e_1+e_2+e_3</m> is an eigenvector corresponding to the eigenvalue <m>1+2t</m>. Similarly, 
                <m>e_1-e_2,e_2-e_3, e_1-e_3</m> are eigenvectors corresponding to the eigenvalue <m>1-t</m>.
              </p>
            </statement>
          </example>

          <example xml:id="eigen_eg1">
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m>.
                Find eigenvalues and corresponding eigenvector of <m>A</m>.
              </p>
              <p>
                We have 
                <me>\det(A-\lambda I)=\begin{vmatrix}1-\lambda\amp 2\amp -2\\
                    1\amp 1-\lambda\amp 1\\1\amp 3\amp -1-\lambda
                    \end{vmatrix}=\lambda^3-\lambda^2-4\lambda+4</me>.
                It is easy to see that characteristic polynomial
                <m>\det(A-\lambda I)</m> has roots <m>\lambda=1, \lambda=-2, \lambda=2</m>.
                Thus <m>A</m> has eigenvalues <m>1, -2, 2</m>.
              </p>
              <p>
                Let us find eigenvectors with respect to the eigenvalue <m>\lambda=1</m>.
                Let <m>v=\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}</m> be an eigenvector corresponding to <m>\lambda=1</m>.
                Then <m>Av=\lambda v=v</m>.
                That is,
                <me>
                  \begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix} \begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix} =\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}
                </me>.
              </p>
              <p>
                This gives a system of linear equations
                <me>
                  x_1+2x_2-2x_3=x_1; x_1+x_2+x_3=x_2; x_1+3x_2-x_3=x_3
                </me>
              </p>
              <p>
                Solving the above system, we get <m>x_1=-x_2, x_2=x_3</m>.
                Thus <m>v=\begin{pmatrix}\alpha\\-\alpha\\-\alpha \end{pmatrix}</m> for <m>\alpha\in \R</m> is an eigenvector.
                In particular,
                <m>v=\begin{pmatrix}1\\-1\\-1 \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=1</m>.
              </p>
              <p>
                Similarly show that <m>\begin{pmatrix}0\\1\\1 \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=2</m> and
                <m>\begin{pmatrix}8/7\\-5/7\\1 \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=-2</m>
              </p>
            </statement>
          </example>

          <example>
            <statement>
              <p>
                Let <m>\begin{pmatrix}0\amp 1\\-1\amp 0 \end{pmatrix}</m>.
                Find eigenvalues and corresponding eigenvector of <m>A</m>.
              </p>
              <p>
                The characteristic equation of <m>A</m> is given by <m>\det(A-\lambda I)=\begin{vmatrix}-\lambda\amp 1\\-1\amp -\lambda\end{vmatrix}=\lambda^2+1</m>.
                Hence eigenvalues of <m>A</m> are <m>\lambda=\pm i</m>.
              </p>
              <p>
                Let us find eigenvectors with respect to the eigenvalue <m>\lambda=i</m>.
                Let <m>v=\begin{pmatrix}x_1\\x_2 \end{pmatrix}</m> be an eigenvector corresponding to <m>\lambda=i</m>.
                Then <m>Av=\lambda v=i v</m>.
                That is,
                <me>
                  \begin{pmatrix}0\amp 1\\-1\amp 0 \end{pmatrix} \begin{pmatrix}x_1\\x_2 \end{pmatrix} =\begin{pmatrix}i x_1\\ ix_2 \end{pmatrix} \Longrightarrow \begin{pmatrix}x_2\\-x_1 \end{pmatrix} =\begin{pmatrix}i x_1\\ ix_2 \end{pmatrix}
                </me>.
              </p>
              <p>
                Now it is easy to see that
                <m>v=\begin{pmatrix}1\\ i \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=i</m>.
                Similarly one can show that
                <m>v=\begin{pmatrix}1\\ -i \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=-i</m>.
              </p>
              <p>
                Note that in the above example,
                <m>A</m> is a real matrix but its eigenvalues and eigenvectors are complex.
              </p>
            </statement>
          </example>
          <example>
            <statement>
              <p>
                <ol>
                  <li>
                    <p>
                      Suppose <m>A</m> is a diagonal matrix, then eigenvalues of <m>A</m> are 
                      diagonal entries. 
                    </p>
                  </li>
                  <li>
                    <p>
                      Suppose <m>A</m> is an <m>n\times n</m> triangular (upper /lower) matrix, the eigenvalues of 
                      <m>A</m> are  the main diagonal entries.
                    </p>
                  </li>
                </ol>
              </p>
            </statement>
          </example>
          <reading-questions xml:id="rqs-eigenspace-subspace">
            <exercise>
              <statement>
                <p>
                  Let <m>A</m> an <m>n\times n</m> real matrix and
                  <m>\lambda\in \R</m> be an eigenvalue of <m>A</m>.
                  Then
                  <me>
                    E_\lambda = \{x\in \R^n: Ax=\lambda x\}
                  </me> is a subspace of <m>\R^n</m>.
                </p>
              </statement>
            </exercise>

            <exercise>
              <p>
                <m>E_\lambda</m> is the kernel of <m>A-\lambda I</m>.
              </p>
            </exercise>

            <hint>
              <p>
                Let <m>v_1,v_2 \in E_\lambda</m>. Then 
                <me>A(v_1+v_2)=Av_1+Av_2=\lambda v_1+\lambda_v2=
                  \lambda(v_1+v_2).</me>
                  <me>
                    \text{for } \alpha\in R, A(\alpha v)=\alpha Av=\alpha \lambda v=\lambda(\alpha v).
                  </me>
                  
              </p>
            </hint>
          </reading-questions>

          <definition xml:id="def-eigenspace"><title>Eigenspace</title>         
            <statement>
              <p>
                Let <m>A</m> an <m>n\times n</m> real matrix and
                <m>\lambda\in \R</m> be an eigenvalue of <m>A</m>.
                Then
                <me>
                  E_\lambda = \{x\in \R^n: Ax=\lambda x\}
                </me>
                the collection of all eigenvectors of <m>A</m> corresponding to <m>\lambda</m> is a subspace of <m>A</m>,
                called the <em>eigenspace of <m>A</m></em>.
                The dimension of <m>E_\lambda</m> is called the <em>geometric multiplicity</em> of <m>A</m>.
              </p>
              <p>
                Let  <m>x</m> be a enigenvector correspnding to the eigenvalue <m>\lambda</m>. 
                Let the characteristic polynomila of <m>A</m> splits as
                <me>p(x)=\det{(x I -A)}=(x-\lambda_1)^{m_1}(x-\lambda_2)^{m_2}\cdots (x-\lambda_k)^{m_k}.
                </me>
                Then <m>\lambda_i</m> are eigenvalue of <m>A</m> with multiplicity <m>m_i</m>,
                called the <em>algebraic multiplicity</em> of <m>\lambda_i</m>.
              </p>
            </statement>
          </definition>

          <p>
            We mention, the following theorem without proof at this stage. 
            We shall see the proof later.
          </p>

          <theorem xml:id="thm-alg-geom-multiplicity-relation">
            <statement>
              <p>
              Geometric multiplicity of an eigenvalue is always less than or equals to its 
              algebraic multiplicity. That is, if <m>m</m> is the geometric multiplicity of 
              <m>\lambda</m> then <m>\dim{(E_\lambda)} \leq  m</m>.
            </p>
            </statement>
          </theorem>

          <exercise xml:id="exer-gem-multiplicity1">
            <statement>
              <p>
                The geometric multiplicity of an eigenvalue <m>\lambda</m> is the nullity of
                <m>A-\lambda I</m> which is the dimension of null space of <m>A-\lambda I</m>.
              </p>
            </statement>
          </exercise>

          <example xml:id="eigen-eg117">
            <statement>
              <p>
                Consider the matrix <m>A=\begin{pmatrix}-1\amp  1 \amp  0\\0 \amp  -1 \amp  1\\0 \amp  0 \amp  -1 \end{pmatrix}</m>.
                It is easy to check that <m>\det{(xI-A)}=(x+1)^3</m>.
                That is, <m>A</m> has only one eigenvalue of
                <m>\lambda =-1</m> of algebraic multiplicity 3.
                It is easy to see that <m>e_3=(0,0,1)</m> is an eigenvector corresponding to <m>\lambda=-1</m>.
                We have
                <me>
                  A -\lambda I = \begin{pmatrix}0\amp  1 \amp  0\\0 \amp  0 \amp  1\\0 \amp  0 \amp  0 \end{pmatrix}
                </me>.
               Since the rank of <m>A-\lambda I</m> is 2. By rank-nullity theorem,
                nullity of <m>A -\lambda I</m> is 1. 
            
                Hence the geometric multiplicity of <m>\lambda</m> is 1 where as its algebraic multiplicity is 3.
              </p>
            </statement>
          </example>
     <p>  We list the following properties of eigenvalues and eigenvectors without proof. </p>
  
      <theorem xml:id="eigen-properties">
        <title>Properties of Eigenvalues and Eigenvectors</title>
        <statement>
          <p>
            <ol>
              <li xml:id="eigen-properties-1">
                <p>
                  <m>A</m> and <m>A^T</m> have the same eigenvalues.
                </p>
              </li>
              <li xml:id="eigen-properties-2">
                <p>
                  If <m>\lambda</m> is an eigenvalue of <m>A</m>,
                  then <m>\alpha \lambda</m> is an eigenvalue of <m>\alpha A</m>.
                </p>
              </li>
              <li xml:id="eigen-properties-3">
                <p>
                  If <m>\lambda</m> is an eigenvalue of <m>A</m>,
                  then <m>\lambda^2</m> is an eigenvalue of <m>A^2</m>. 
                  In general, <m>\lambda^k</m> is an eigenvalue of <m>A^2</m>
                  for any <m>k\in \N</m>. 
                </p>
              </li>
              <li xml:id="eigen-properties-4">
                <p>
                  If <m>\lambda</m> is an eigenvalue of a non singular matrix <m>A</m>,
                  then <m>1/ \lambda</m> is an eigenvalue of <m>A^{-1}</m>.
                </p>
              </li>
               <li xml:id="eigen-properties-5">
               <p>
                  If <m>\lambda</m> is an eigenvalue of <m>A</m>,
                  then <m>\lambda-k</m> is an eigenvalue of <m>A-kI</m> for any scalar <m>k</m>.
                </p>
              </li>
            
              <li xml:id="eigen-properties-6">

                <p>
                  If <m>\lambda</m> is an eigenvalue of <m>A</m> and
                  <m>p(x)=c_0+c_cx+c_2x^2+\cdots c_kx^k</m> is a polynomial in <m>x</m>,
                  then <m>p(\lambda)</m> is an eigenvalue of <m>p(A)=c_0I+c_cA+c_2A^2+\cdots c_kA^k</m>.
                </p>
              </li>
              <li xml:id="eigen-properties-7">
                <p>
                  Two matrices <m>A</m> and <m>B</m> are called <em>similar</em>
                  if there exists a matrix <m>P</m> such that <m>B=P^{-1}AP</m>.
                  Similar matrices have same eigenvalues.
                </p>
              </li>
               <li xml:id="eigen-properties-8">
                <p>
                  If <m>\lambda_1</m> and <m>\lambda_2</m> are distinct eigenvalues of <m>A</m> then eigenvectors <m>v_1</m> and <m>v_2</m> corresponding to <m>\lambda_1</m> and
                  <m>\lambda_2</m> are linearly independent.
                  Can you generalize this?
                </p>
              </li>
              <li xml:id="eigen-properties-9">
                <p>
                  If <m>T</m> is a linear transformation from <m>\R^n\to \R^m</m>.
                  Fix a basis <m>\beta</m> of <m>\R^n</m>.
                  Let <m>A=[T]_\beta</m> be the matrix of <m>T</m> with respect to <m>\beta</m>.
                  Then <m>A</m> and <m>T</m> have the same eigenvalues.
                  Furthermore,
                  eigenvalues of <m>T</m> are independent of the basis.
                </p>
              </li>
            </ol>
          </p> 
 </statement>
 <proof>
  <p>
   <em>Proof of <xref ref="eigen-properties-1"/></em>. </p>
   <p>Follows from <m>\det{A^T}=\det{A}</m>. For
   <m>
    \det{(xI-A)^T}=\det{(xI-A)}.
   </m>
   
  </p>
  <p>
    <em>Proof of <xref ref="eigen-properties-2"/></em>. 
  </p>

  <p>  <m>A(\alpha v)=\alpha Av =\alpha\lambda v=\lambda(\alpha v)</m>.
  </p>

  <p>
    <em>Proof of <xref ref="eigen-properties-3"/></em>. 
  </p>

  <p>  <m>A^2(v)=A(Av) =A(\lambda v)=\lambda(\lambda v)=\lambda^2 v</m>.
  </p>

  <p>
    <em>Proof of <xref ref="eigen-properties-4"/></em>. 
  </p>

  <p>  <m>A(v)=\lambda v\implies \frac{1}{\lambda}v=A^{-1}v</m>.
  </p>

   <p>
    <em>Proof of <xref ref="eigen-properties-5"/></em>. 
  </p>

  <p>  <m>(A-kI)(v)=Av -\lambda v =\lambda v-\lambda v=(\lambda-k)v</m>.
  </p>

   <p>
    <em>Proof of <xref ref="eigen-properties-6"/></em>. 
   </p>
<p>
    <me>
    p(A)v=c_0v+c_c\lambda v+c_2\lambda^2v+\cdots c_k\lambda^kv=p(\lambda). 
    </me>
   
  </p>
  
   <p>
    <em>Proof of <xref ref="eigen-properties-7"/></em>. 
  </p>
<p>
  Since
  <me>
    \det(\lambda I-A) = \det\left(P(\lambda I-A)P^{-1}\right),
  </me>
  the eigenvalues of <m>A</m> and <m>PAP^{-1}</m> are same.
</p>

 <p>
    <em>Proof of <xref ref="eigen-properties-8"/></em>. 
  </p>
<p>
  Let <m>\lambda_1</m> and <m>\lambda_2</m> be with distict eigenvalues of <m>A</m> corresponding to 
  the eignevectors, <m>v_1</m> and <m>v_2</m> respectively. Let <m>\alpha_1</m> and <m>\alpha_2</m> be scalars 
  such that <m>\alpha_1v_+\alpha_2 v_2=0.</m>. Applying <m>A</m> both sides we have
  <m>\alpha_1\lambda_1v_1+\alpha_2\lambda_2 v_2=0</m>. Mutliplying the 1st equation both sides by 
  <m>\lambda_1</m> both side and subtracting from the equation 2nd equation we get 
  <m>\alpha_2(\lambda_2-\lambda_1)v_2=0</m>. Since <m>v_2\neq 0 </m>, and <m>\lambda_1\neq \lambda_2</m>, 
  we have <m>\alpha_2=0</m>. Similarly, <m>\lambda_1=0</m>.
</p>
  <p>
We have the following generalization. If <m>v_i</m> for <m>i=1,\ldots, k</m> are 
eigenvvectors with respect to distinct eigenvalues <m>\lamabda_i</m> for <m>i=1,\ldots, k</m> respectively, 
then <m>v_1,\ldots, v_k</m> are linearly independent. The prove this follows from induction on <m>k</m>.
</p>
<p>
    <em>Proof of <xref ref="eigen-properties-9"/></em>. 
  </p>
<p>
Since <m>T\colon \R^n\to \R^n</m> is a linear transformation, it can be written as 
<m>T(x)=M</m>, where <m>M</m> is the matrix of <m>T</m> with respect to the 
standard basis of <m>\R^n</m> and that eigenvalues of <m>T</m>  are 
same as eigenvalues of <m>A</m>. Then <m>A</m> and <m>M</m> are similar matrices. Hence 
they have the same eigenvalues.
</p>
</proof>
</theorem>


          <example>
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m> and <m>B=A^3-3A+I</m>.
                Let us find eigenvalues of <m>B</m>.
              </p>
              <p>
                It is easy to the characteristic polynomial of <m>A</m> is given by
                <m>\lambda^3-\lambda^2-4\lambda+4</m> and <m>\lambda=-2, 1, 2</m>.
                Then eigenvalues of <m>B</m> are given by
                <me>
                  \{(-2)^3-3\times (-2)+1, 1^3-3\times 1+1, 2^3-3\times (2)+1\} =\{-1, -1, 3\}
                </me>
              </p>
            </statement>
          </example>
          

          
          <p>
            In genegarl, let <m> f(x)=\alpha_0+\alpha_1 x+\alpha_2 x^2+\cdots+\alpha_kx^k</m> be a 
            polynomial of degree <m>k</m> and <m>A</m> be an <m>n\times n</m> real matrix. Then  we can define 
            <m> f(A)=\alpha_0 I+\alpha_1 A+\alpha_2 A^2+\cdots+\alpha_k A^k</m>. If <m>\lambda</m> is an eigenvalue of 
            <m>A</m>, theh <m>f(\lambda)</m> is an eigenvalue of <m>f(A)</m>.
          </p>
          
          <theorem xml:base="thm-Cayley-Hamilton">
            <title>Cayley-Hamilton Theorem</title>
            <statement>
              <p>
                Every square matrix satisfies its characteristic equation.
                That is, if <m>p(x)=0</m> is characteristic equation of <m>A</m>,
                then <m>p(A)=0</m>.
              </p>
            </statement>

            <proof>
              <p>
                The proof is an application of the formula
                <me>
                  A\text{adj}(A)=\det{(A)}I
                </me>
                where <m>\det{(A)}</m> is the adjugate of a <m>A</m> and is defined as the transpose 
                of its cofactor matrix. 
              </p>
              <p>
                We apply the adjugate formula to the matrix <m>\lambda I-A</m>. Hence we have
                <men xml:id="eqn-Caley-Hamilton-eq1">
                  (\lambda I-A)\text{adj}(tI-A)=\det(\lambda I-A)\,I = p(\lambda)\,I. 
                </men> 
                Note that adjugate matrix <m>\text{adj}(\lambda I-A)</m> is an <m>n\times n</m> matrix 
      whose entries are polynomials in <m>\lambda</m> of degree at most <m>n-1</m>. 
      Hence we may write
      <me>
        \text{adj}( \lambda I-A)=\sum_{k=0}^{n-1} C_k \lambda^k,
      </me>
      where <m>C_k's</m> are matrices with real entries.  Hence by <xref ref="eqn-Caley-Hamilton-eq1"/>, 
      we have 
      <men xml:id="eqn-Caley-Hamilton-eq2">
        (\lambda I-A) \left(\sum_{k=0}^{n-1} C_k \lambda^k\right) = A^n+a_{n-1}A^{n-1}+a_1A+a_0I.
      </men>
      Now comparing the coefficients of power of <m>\lambda</m>, we get
      <md>
        <mrow> C_{n-1}\amp = I</mrow>
        <mrow> C_{n-2}-AC_{n-1}\amp = a_{n-1}I</mrow>
        <mrow> C_{n-3}-AC_{n-2}\amp = a_{n-2}I</mrow>
        <mrow>\amp \vdots</mrow>
        <mrow> C_{0}-AC_{1}\amp = a_{1}I</mrow>
        <mrow> -AC_{0}\amp = a_{0}I</mrow>
      </md>
      Multiplying the first equation by <m>A^n</m>, the second equation by <m>A^{n-1}</m>, 
      the second last equation by <m>A</m> and the last equation by <m>I</m> and additing, we get
      the desired result.
          </p>
            </proof>
          </theorem>
          
          <example>
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m>.
                From <xref ref="eigen_eg1">Example</xref>,
                the characteristic polynomial of <m>A</m> is given by <m>p(x)=\det{x I -A}=x^3-x^2-4x+4</m>.
                We have <m>A^2=\left(\begin{array}{rrr} 1 \amp  -2 \amp  2 \\ 3 \amp  6 \amp  -2 \\ 3 \amp  2 \amp  2 \end{array} \right)</m> and <m>A^3=\left(\begin{array}{rrr} 1 \amp  6 \amp  -6 \\ 7 \amp  6 \amp  2 \\ 7 \amp  14 \amp  -6 \end{array} \right)</m>.
                Hence
                <md>
                  <mrow>p(A) =\amp A^3-A^2-4A+4I</mrow>
                  <mrow> =\amp  \left(\begin{array}{rrr} 1 \amp  6 \amp  -6 \\ 
                    7 \amp  6 \amp  2 \\ 7 \amp  14 \amp  -6 \end{array} \right)-
                    \left(\begin{array}{rrr} 1 \amp  -2 \amp  2 \\
                     3 \amp  6 \amp  -2 \\ 3 \amp  2 \amp  2 \end{array} \right)</mrow>
                     <mrow> \amp -4\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix} +4\begin{pmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{pmatrix}</mrow>
                  <mrow> =\amp  \begin{pmatrix}0\amp 0\amp 0\\0\amp 0\amp 0\\ 0\amp 0\amp 0 \end{pmatrix} 
                  </mrow>
                </md>.
              </p>
              <p>
                Hence <m>A</m> satisfies its characteristic equation.
              </p>
              <p>
                It is easy to check that <m>\det{(A)}=-4</m>,
                hence <m>A</m> is non singular.
                Since <m>A^3-A^2-4A+4I=0</m>,
                multiplying both sides by its inverse,
                we get <m>A^2-A+4I+4A^{-1}=0</m>.
                Hence
                <me>
                  A^{-1}=-\frac{-1}{4}A^2+\frac{1}{4}A+I=\left(\begin{array}{rrr} 1 \amp  1 \amp  -1 \\ -\frac{1}{2} \amp  -\frac{1}{4} \amp  \frac{3}{4} \\ -\frac{1}{2} \amp  \frac{1}{4} \amp  \frac{1}{4} \end{array} \right)
                </me>.
              </p>
              <p>
                We can also find higher powers of a matrix,
                using the Cayley-Hamilton theorem.
                For example multiplying by <m>A</m> to the equation,
                <m>A^3-A^2-4A+4I=0</m>, we get <m>A^4-A^3-4A^2+4A=0</m>,
                from this we have
                <me>
                  A^4 = A^3+4A^2-4A=\left(\begin{array}{rrr} 1 \amp  -10 \amp  10 \\ 15 \amp  26 \amp  -10 \\ 15 \amp  10 \amp  6 \end{array} \right)
                </me>.
              </p>
              <p>
                Can you find <m>A^5</m>?
              </p>
            </statement>
          </example>

          <exercise xml:id="exer-Cayley-Hamilton1">
            <statement>
              <p>
                (i) Consider the matrix <m>A = \begin{pmatrix}3 \amp -2 \\-4\amp 3 \end{pmatrix}</m>.
                Show that <m>A</m> satisfies its characteristics equation.
                Hence find <m>A^{-1}, A^3, A^4</m>.
              </p>
              <p>
                (i) Consider the matrix <m>A = \begin{pmatrix}1 \amp 0 \amp 0 \\ -4 \amp -3 \amp 4 \\ -2 \amp -2 \amp 3 \end{pmatrix}</m>.
                Show that <m>A</m> satisfies its characteristics equation.
                Hence find <m>A^{-1}, A^4, A^5</m>.
              </p>
            </statement>
          </exercise>
          
          <definition xml:id="def-spectral-radius">
            <title>Spectral Radius</title>
                       
            <statement>
              <p>
                Let <m>A</m> an <m>n\times n</m> and <m>\lambda_i</m> for
                <m>1\leq i\leq n</m> be eigenvalues of <m>A</m> then the <em>spectral radius</em> of <m>A</m> is define as <m>\rho(A):=\displaystyle\max_{1\leq i\leq n}\{ |\lambda_i| \}</m>.
              </p>
            </statement>
          </definition>

          <example>
            <statement>
              <p>
                Let <m>A= \begin{pmatrix}0 \amp  -1\\1 \amp  0 \end{pmatrix}</m>.
                Then the characteristics polynomial of <m>A</m> is <m>\det{(xI-A)}=x^2+1</m>.
                Hence <m>x=\{i,-i\}</m> are roots of the characteristic polynomial.
                Hence <m>i</m> and <m>-i</m> are eigenvalues of <m>A</m>.
                Hence
                <me>
                  \rho(A) = \max\{|i|,|-i|\}=1
                </me>.
              </p>
            </statement>
          </example>

          <example>
            <statement>
              <p>
                Consider the matrix <m>A=\left(\begin{array}{rrr} 4 \amp  -3 \amp  0 \\ 3 \amp  4 \amp  0 \\ 5 \amp  10 \amp  10 \end{array} \right)</m>.
                Then the characteristics polynomial of <m>A</m> is <m>\det{xI-A}=x^{3} - 18 x^{2} + 105 x - 250</m>.
                Which has roots, <m>10, 3-4i,3+4i</m>.
                Hence
                <me>
                  \rho(A) = \max\{10,|3-4i|,|3+4i\}=10
                </me>.
              </p>
            </statement>
          </example>

          <exercise xml:id="exser-spectral-radius1">
            <statement>
              <p>
                Find the spectral radius of
                <m>\left(\begin{array}{rr} 2 \amp -3 \\ 3 \amp 2 \end{array} \right), \left(\begin{array}{rrr} 2 \amp 3 \amp 1 \\ -3 \amp 2 \amp 2 \\ 0 \amp 0 \amp 2 \end{array} \right)</m> and <m>\left(\begin{array}{rrr} 2 \amp 3 \amp 1 \\ 3 \amp 2 \amp 2 \\ 0 \amp 0 \amp 1 \end{array} \right)</m>.
              </p>
            </statement>
          </exercise>
    
    
          
          <definition><title>Positive definite matrix</title>
            <statement>
              <p>
                Let <m>A</m> be an <m>n\times n</m> symmetric matrix.
                Then <m>A</m> is said to be <em>positive definite</em>
                if <m>x^TAx\gt 0</m> for all <m>x\in \R^n\setminus \{0\}</m> and <m>x^TAx=0</m> if and only if <m>x=0</m>.
                <m>A</m> is called <em>negative definite</em>
                if <m>-A</m> is positive definite.
              </p>
            </statement>
          </definition>

          <example>
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}1\amp 2\\2\amp 5 \end{pmatrix}</m>.
                Let <m>x=\begin{pmatrix}x_1\\x_2 \end{pmatrix}</m>.
                Then
                <md>
                 <mrow> x^TAx=\amp \begin{pmatrix}x_1\amp x_2 \end{pmatrix} 
                    \begin{pmatrix}1\amp 2\\2\amp 5 \end{pmatrix} 
                    \begin{pmatrix}x_1\\x_2 \end{pmatrix}</mrow>
                    <mrow> =\amp \begin{pmatrix}x_1\amp x_2 \end{pmatrix} 
                        \begin{pmatrix}x_1+2x_2\\2x_1+5x_2 \end{pmatrix} </mrow>
                        <mrow> =\amp x_1^2+4x_1x_2+5x_2^2=(x_1+2x_2)^2+x_2^2</mrow>
                </md>.
              </p>
              <p>
                Clearly <m>x^TAx>0</m> for all non zero vector <m>x</m> and <m>x^TAx=0</m> if and only if <m>x=0</m>.
                Hence <m>A</m> is positive definite.
              </p>
            </statement>
          </example>

          <example>
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}0\amp 1\\1\amp 0 \end{pmatrix}</m>.
                Let <m>x=\begin{pmatrix}x_1\\x_2 \end{pmatrix}</m>.
                Then
                <me>
                  x^TAx=\begin{pmatrix}x_1\amp x_2 \end{pmatrix} \begin{pmatrix}0\amp 1\\1\amp 0 \end{pmatrix} \begin{pmatrix}x_1\\x_2 \end{pmatrix} = \begin{pmatrix}x_1\amp x_2 \end{pmatrix} \begin{pmatrix}x_2\\x_1 \end{pmatrix} =2x_1x_2
                </me>
              </p>
              <p>
                Thus if <m>x=\begin{pmatrix}1\\-1 \end{pmatrix}</m> then <m>x^TAx=-2\lt 0</m>.
                Hence it is not a positive definite.
                Its easy to see that <m>A</m> is also not negative definite.
              </p>
            </statement>
          </example>
          
          <exercise xml:id="exer-eigen-positive-definite">
          <statement>
            <p>
              If <m>A</m> is a positive definite matrix then all its eigenvalues are positive.
            </p>
          </statement>
          </exercise>

         
          <exercise xml:id="exer-eigen-negative-definite">
            <statement>
              <p>
                If <m>A</m> is a negative definite matrix then all its eigenvalues are negative.
              </p>
            </statement>
            </exercise>

            <p>
            We have the following result about positive definite matrices known as <em>Sylvester's criterion</em>. It allows us to determine if a given matrix in positive definite using the leading principal minors of the matrix.
          </p>
          <p>
           The leading principal minors of a matrix <m>A</m> are <m>{\rm det}(A)</m> and the minors obtained by successively removing the last row and the last columns. That is, the leading principal miniors of a matrix <m>A</m> are
           <me>
            \Delta_1 =a_{11}, 
            \Delta_2 =\begin{vmatrix}a_{11} \amp a_{12}\\a_{21} \amp a_{22}\end{vmatrix}, \Delta_3=
            \begin{vmatrix}a_{11} \amp a_{12} \amp a_{13}\\a_{21} \amp a_{22} \amp a_{23}\\a_{31} \amp a_{32}\amp a_{33}\end{vmatrix},
            \cdots, \Delta_n ={\rm det}{(A)}
           </me>.
           
          </p>

            <theorem xml:id="thm-sylvyster"><title>Sylvester's Criterion</title>
              <statement>
                <p>
                  If <m>A</m> is a real symmetric matrix then <m>A</m> is positive definite if and only if all leading minor of <m>A</m> are positive.
                </p>
              </statement>
            </theorem>
            
            

          <example>
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}2 \amp  1 \amp 1\\1\amp 2\amp 1\\1\amp 1\amp 2 \end{pmatrix}</m>.
                For any <m>x=\begin{pmatrix}x_1\\x_2\\x_2 \end{pmatrix} \in \R^2</m>, we have
                <me>
                  x^TAx=x_1^2+x_2^2+x_3^2+(x_1+x_2+x_3)^2
                </me>
              </p>
              <p>
                Henc e <m>A</m> is positive definite.
              </p>
            </statement>
          </example>

          <exercise xml:id="exer-eigen-positive-definite2">
            <statement>
              <p>
                Let <m>A=\begin{pmatrix}2 \amp -1 \amp 0\\-1\amp 2\amp -1\\0\amp -1\amp 2 \end{pmatrix}</m>.
                Show that <m>A</m> is positive definite.
              </p>
            </statement>
          </exercise>
      <p>
        Note that if <m>A</m> is not a symmetric matrix, then the Sylvester's criteria cannot be used to check positive definiteness. For, condider the matrix <m>A=\begin{pmatrix} 1 \amp 0\\-3 \amp 1\end{pmatrix}</m>. It is easy to see that all principal minors of <m>A</m> are positive. For <m>u=\begin{pmatrix}1\\2\end{pmatrix}</m>, <m>u^TAu=-1</m>, however for <m>v=\begin{pmatrix}3\\1\end{pmatrix}</m>, <m>v^TAv=1</m>. 
      </p>
     
      <reading-questions xml:id="rqs-eigenvalues-poly-space">
        <exercise>
          <p>
            Consider a linear transpfrmation <m>D\colon {\cal P}_n(\R)\to {\cal P}_n(\R)</m> defined 
          by <m>D(f(x))=f'(x)</m>. Whar are eigenvalues and eigenvector of <m>D</m>?

          </p>        
        </exercise>
          
          <exercise>
            <p>
         Consider a linear transpfrmation <m>T\colon {\cal P}_2(\R)\to {\cal P}_n(\R)</m> defined 
          by <m>T(f(x))=xf'(x)-f(x)</m>. Whar are eigenvalues and eigenvector of <m>T</m>?
          </p>
            
          </exercise>   
      
           <exercise>
           <p>
             Consider a linear transformation <m>T\colon M_2(\R)\to M_2(\R)</m> defined as 
            <m>T(A)=A^T</m>. What are eigenvalues and eigenvectors of <m>T</m>?
            </p>
          </exercise>   

          <exercise>
           <p>
             Consider a linear transformation <m>T\colon M_2(\R)\to M_2(\R)</m> defined as 
            <m>T(A)=A+A^T</m>. What are eigenvalues and eigenvectors of <m>T</m>?
            </p>
          </exercise>   

           <exercise>
           <p>
             Consider a linear transformation <m>T\colon M_2(\R)\to M_2(\R)</m> defined as 
            <m>T(A)=A-A^T</m>. What are eigenvalues and eigenvectors of <m>T</m>?
            </p>
          </exercise>
          <hint>
            <p>
              Write matrix of each of the linear transformation with respect to the 
              standard basis and find the eigenvalues and eigenvectors with respect to 
              that see what will the corresponding  eigenvalues and eigenvectors  of <m>T</m>.
            </p>
          </hint>
      </reading-questions>

      <reading-questions xml:id="rqs-eigenvalues-function-space">
        <p>
        <exercise>
         <p> 
          Let <m>V</m> be the set of all twice continuously differentaible functions 
          from <m>\R</m> to <m>\R</m>. It is easy to check that it is a  vector soace over <m>\R</m>. 
          Consider the following linear maps:
          <ol>
            <li>
              <p>
                <m>T(f(x))=f'(x)</m>.
              </p>
            </li>
            <li>
              <p>
                <m>S(f(x))=f''(x)</m>.
              </p>
            </li>
          </ol>
          Can you find some eigenvalues and eigenvectors of <m>T</m> and <m>S</m>?
        </p>
        </exercise>
      </p>
      </reading-questions>
      
      
      <definition xml:id="def-invariant-subspace">
        <statement>
          <p>
            Let <m>T\colon \R^n\to\R^n</m> be a liear map. A subspace <m>W</m> of <m>\R^n</m> 
            is called an <em>invariant </em> under <m>T</m> if <m>T(w)\in W</m> for all <m>w\in W</m>.
          </p>
        </statement>
      </definition>
    <p>
      It is easy to check that if <m>T\colon \R^n\to\R^n</m> is a linear map then <m>\{0\}</m> and 
      <m>\R^n</m> are invariant subspaces. Furthermore, <m>{\rm ker}(T)</m> and <m>{\rm Im}(T)</m> are 
      also <m>T</m>-invariant subspaces.
    </p>


    <reading-questions xml:id="rqs-invariant-subspace">
      
      <exercise xml:id="exer-invarince-eigenspace">
      <p>
        Let <m>T\colon \R^n\to\R^n</m> be a liear map. Then the eigenspace <m>E_\lambda</m> 
        is <m>T</m> invariant.
      </p>
    </exercise>
    <p>
    <exercise>
      <p>
        Suppose the exist a one dimensional invariant subspace of a linear 
        map <m>T\colon \R^n\to\R^n</m>, then it has an eigenvector.
      </p>
    </exercise>
  </p>
  <p>
    <exercise>
      <p>
      Suppose <m>T,S\colon \R^n \to \R^n</m> be linear maps such that <m>TS=ST</m>, that is they 
      commute. Then show that <m>{\rm ker}(A)</m> is <m>S</m>-invariant.  
      </p>
    </exercise>
  </p>
  <p>
    <exercise>
      <p>
      Let <m>T\colon \R^n\to\R^n</m> be a liear map and <m>W_1</m> and <m>W_2</m> are
       <m>T</m>-invariant suspaces such that <m>V=W_1\oplus W_2</m>. Let 
       <m>\beta_1=\{u_1,\ldots,u_k\}</m> and <m>\beta_2=\{v_1,\ldots,v_m\}</m> be basis of 
       <m>W_1</m> and <m>W_2</m> respectively. Then it is easy to check that 
       <m>\beta=\beta_1\cup \beta_2</m> is a basis of <m>\R^n</m>. What will be the matrix of 
       <m>T</m> with respect to <m>\beta.</m>
      </p>
    </exercise>
  </p>
  </reading-questions> 
</subsection>

   </section>
     