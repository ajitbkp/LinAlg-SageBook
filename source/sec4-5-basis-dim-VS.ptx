<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec4-5-basis-dim-VS" xmlns:xi="http://www.w3.org/2001/XInclude">
        <title> Basis and dimension</title>
       <introduction>
        <p>
          In this section, we define basis and dimension of  a vector spaces.
        </p>
       </introduction>
       
        <subsection xml:id="subsec-basis-VS">
          <title>Basis of a Vector Space</title>
          
             <p>
        We can defined basis of a vector space similar to basis of subspaces in <m>\R^n</m>.
      </p>

      <definition xml:id="def-basis-VS">
        <title>Basis of a vector space</title>      
        <statement>
          <p>
            Let <m>V</m> be a vector space over <m>\R</m>.
            A set of vectors <m>\beta=\{v_1,v_2,\ldots,v_n\}\subset V</m> is called a basis of <m>V</m> if every vector
            <m>v\in \R^n</m> can be expressed uniquely as linear combinations of <m>v_1,v_2,\ldots,v_n</m>.
          </p>
          <p>
            Thus <m>\beta</m> is basis of <m>V</m> if
            <ol marker="(i)">
              <li>
                <p>
                 <m>L(\beta)=\R^n</m>,
            that is,  every vector <m>v\in \R^n</m> can be expressed as linear 
            combinations of <m>v_1,v_2,\ldots,v_n</m> 
                </p>
              </li>
              <li>
                <p>
                  If <m>v=\alpha_1v_1+\alpha_2v_2+\cdots +\alpha_nv_n</m> and <m>v=\beta_1v_1+\beta_2v_2+\cdots +\beta_nv_n</m>,
            then <m>\alpha_1=\beta_1, \alpha_2=\beta_2=\cdots,\alpha_n=\beta_n</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </definition>
<exercise xml:id="Exer-4-5-1">
  <p>
  If <m>\beta=\{v_1,\ldots,v_n\}</m> is a basis of vector spave <m>V</m> over <m>\R</m>, 
  then (i) <m>L(\beta)=V</m> and (ii) <m>\beta</m> is linearly independent.
  </p>
  <hint>
    <p>
      Follow the arguments simialr to <xref ref="thm-sec2-3-basis-thm1"/>.
    </p>
  </hint>
 </exercise>
 
      <p>
        We have already seen several examples of bases in <m>\R^n</m> and some subspaces of <m>\R^n</m>.
      </p>

      <example>
        <statement>
          <p>
            Let <m>V={\cal P}_n(\R)</m>.
            The set <m>\{1,x,x^2,\ldots,
            x^n\}</m> is basis of <m>V</m>,
            called the standard basis.
          </p>
        </statement>
      </example>
   
      <example>
        <statement>
          <p>
            <m>\{1,i\}</m> is a basis of
            <m>\mathbb{C}</m> as a vector space over <m>\R</m>.
          </p>
        </statement>
      </example>
   
   
      <example>
        <statement>
          <me>
            S=\left\{ \begin{bmatrix}1 \amp  0 \\0 \amp  0
            \end{bmatrix} ,
            \begin{bmatrix}0 \amp  1 \\0 \amp  0
            \end{bmatrix} , \begin{bmatrix}0 \amp  0 \\1 \amp  0
            \end{bmatrix} ,
            \begin{bmatrix}0 \amp  0 \\0 \amp  1
            \end{bmatrix} \right\}
          </me>
          <p>
            is a basis <m>M_2(\R)</m>, called the standard basis.
          </p>
        </statement>
      </example>

   
      <lemma xml:id="lemma-4-5-5">
        <statement>
          <p>
            Any <m>n</m> linearly independent set of vectors forms a basis of <m>\R^n</m>.
          </p>
        </statement>

         <proof>
          <p>
            Let <m>\beta=\{v_1,\ldots, v_n\}</m> be a linearly independent. Enough to show that 
            <m>L(\beta)=V</m>. Let <m>v\in \R^n</m>. Then we know that <m>\beta\cup \{ v\}</m> is linearly
            dependent. That is, there exist scalars, <m>\alpha_1,\ldots, \alpha_n,\alpha_{n+1}</m>  not 
            all zero such that 
            <me>
              \alpha_1v_1+\vdots+\alpha_n v_n+\alpha_{n+1}v=0.
            </me>
            We calim that <m>\alpha_{n+1}\neq 0</m>. If <m>\alpha_{n+1}=0</m>, then we have scalars 
             <m>\alpha_1,\ldots, \alpha_n</m>  not all zero such that such that
            <me>
              \alpha_1v_1+\cdots+\alpha_n v_n=0.
            </me>
           This is a contraction, as <m>\beta</m> is linearly independent.    Thus 
           <m>\alpha_{n+1}\neq0</m>. Hence we have 
           <me>
              v=-\frac{1}{\alpha_{n+1}}\left(\alpha_1v_1+\cdots+\alpha_n v_n\right).
            </me>
            In otherwords, <m>v\in L(\beta)</m>.
          </p>
        </proof>

      </lemma>

  <p>
    Can you generalize the above lemma?
  </p>
    
      <theorem xml:id="sec4-4-thm2">
        <statement>
          <p>
            If a vector space <m>V</m> has a basis consisting of <m>n</m> 
            elements then any set of <m>n + 1</m> vectors is linearly dependent.
          </p>
        </statement>

        <proof>
          <p>
            Follow the steps similar to proof of <xref ref="lemma-4-5-5"/>.
          </p>
        </proof>
      </theorem>
    
      <corollary xml:id="cor-4-5-6">
        <statement>
          <p>
        Let <m>V</m> be a vector space over <m>\R</m> with a basis <m>\{v_1, \ldots, v_n\}</m> 
        consisting of <m>n</m> vectors. If <m>\{w_1,\ldots,w_m\}</m> is a linearly 
        independet subset in <m>V</m>, then <m>m\leq n</m>.   
          </p>
        </statement>
      <proof>
        <p>
          Follows from <xref ref="sec4-4-thm2"/>.
        </p>
      </proof>
      </corollary>

 <theorem xml:id="sec4-4-thm1">
        <statement>
          <p>
            Let <m>V</m> be a vector space over <m>\R</m>.
            Let <m>\beta=\{v_1,\ldots,
            v_n \}</m> and <m>\gamma=\{u_1,\ldots,
            u_m\}</m> be two bases of <m>V</m>.
            Then <m>m=n</m>.
          </p>
        </statement>

        <proof>
          <p>
            Let <m>\beta</m> be a basis and since <m>\gamma</m> is linearly independen, 
            by the <xref ref="cor-4-5-6"/>, we have <m>m\leq n</m>. Interchanging the role of 
            <m>\beta</m> and <m>\gamma</m>, we have <m>n\leq m</m>. Hence <m>m=n</m>.
          </p>
        </proof>
      </theorem>
    
     
    

      <definition xml:id="def-finite-dim-VS">
        <title>Finite Dimensional Vector Space</title>       
        <statement>
          <p>
            A vector space <m>V</m> is called
            <em>finite dimensional</em>
            if there exists a finite subset <m>S</m> of <m>V</m> such that <m>L(S)=V</m>.
          </p>
          <p>
            A vector space which is not finite dimensional is called an <em>infinite dimensional</em>.
          </p>
        </statement>
      </definition>

      <definition xml:id="def-dimension-VS">
        <statement>
          <p>
            We say a vector space <m>V</m> is of dimension <m>n</m> if it has a basis <m>\beta</m> consisting of <m>n</m> elements.
          </p>
        </statement>
      </definition>

      
      <exercise xml:id="exer-dim-of-zero-space">
        <statement>
          <p>
            What is the dimension of <m>V=\{0\}</m>, the zero space?
          </p>
        </statement>
      </exercise>
      
      <example>
        <statement>
          <p>
            <ol marker="(i)">
              <li>
                <p>
              <m>\R^n</m> is a <m>n</m> dimensional vectors space over <m>\R</m>.      
                </p>
              </li>
              <li>
                <p>
                  <m>M_n(\R)</m>, the set of all <m>n\times n</m> matrices pver
                   <m>\R</m> is a <m>n^2</m>-dimensional vector space over <m>\R</m>.
                </p>
              </li>
              <li>
                <p>
                  <m>{\cal P}_n(\R)</m>, the set of all polynomials of degree less than or 
                  equal to <m>n</m> over <m>\R</m> is <m>(n+1)</m>-dimensional vector space over <m>\R</m>.
                </p>
              </li>
            </ol>
          </p>
        </statement>
      </example>

      <example>
        <statement>
          <p>
            Let <m>W</m> be the set of all
            <m>3\times 3</m> real symmetric matrices.
            The set
            <md>
              <mrow>\beta=\left\{
                \begin{bmatrix}1 \amp  0 \amp  0 \\0 \amp  0 \amp  0 \amp  \\ 0 \amp  0 \amp  0 \end{bmatrix}, 
                \begin{bmatrix}0 \amp  0 \amp  0 \\0 \amp  1 \amp  0 \amp  \\ 0 \amp  0 \amp  0 \end{bmatrix}, 
                \begin{bmatrix}0 \amp  0 \amp  0 \\0 \amp  0 \amp  0 \amp  \\ 0 \amp  0 \amp  1 \end{bmatrix} \right.
              </mrow>
              <mrow>   \left. \begin{bmatrix}0 \amp  1 \amp  0 \\1 \amp  0 \amp  0  \\ 0 \amp  0 \amp  0 \end{bmatrix}, 
              \begin{bmatrix}0 \amp  0 \amp  1 \\0 \amp  0 \amp   0 \\ 1 \amp  0 \amp  0 \end{bmatrix}, 
              \begin{bmatrix}0 \amp  0 \amp  0 \\0 \amp  0 \amp  1 \amp  \\ 0 \amp  1 \amp  0 \end{bmatrix} \right\}
              </mrow>
            </md>
            is a basis of <m>W</m>.
            That is, <m>W</m> is 6 dimensional vector space over <m>\R</m>.
            What is dimension of the set of
            <m>n\times n</m> real symmetric matrices and dimension of <m>n\times n</m>
            real skew-symmetric matrices?
          </p>
        </statement>
      </example>
   
      <exercise xml:id="rqs-dim-skey-symm">
        <statement>
          Let <m>W</m> be the set of all
            <m>3\times 3</m> real skew-symmetric matrices.
            Find a basis and hence the dimension of <m>W</m>.
         </statement>
      </exercise>
    
  </subsection>

    <subsection>
      <title>How to find a basis of a finite dimensional vector space?</title>
      <p>
        First let us look at the following results.
      </p>
      <exercise xml:id="exer-4-5-16">
        <p>
          Let <m>u\in V</m> be a nonzero vector. Suppose <m>v\notin L(\{v\})</m>, then show that
          <m>\{u,v\}</m> is linearly independent.
        </p>        
      </exercise>
      <p>
        Can you generalize this result?
      </p>
      <lemma xml:id="lemma-4-5-17">
        <statement>
          <p>
            Let <m>\{v_1,\ldots,
            v_k\}</m> be a linearly independent set of vectors.
            Suppose <m>v\notin { span}(\{v_1,\ldots, v_k\})</m>.
            Then <m>\{v,v_1,\ldots, v_k\}</m> is linearly independent.
          </p>
        </statement>

        <proof>
          <p>
            Let <m>v\notin { span}(\{v_1,\ldots, v_k\})</m> and <m>\alpha_1,\ldots,\alpha_{k+1}</m> be 
            scalars such that 
            <me>
              \alpha_1 v_1+\cdots+\alpha_kv_k+\alpha_{k+1}v=0.
            </me>
            We claim that <m>\alpha_{k+1}=0</m>. For if <m>\alpha_{k+1}=0</m> implies 
            <m>v\in { span}(\{v_1,\ldots, v_k\})</m> a contradiction.
          </p>
        </proof>
      </lemma>
   <p>
    <xref ref="lemma-4-5-17"/> gives a way to construct  a basis of a finite dimensional 
    vector space which is the content of the next theorem. 
   </p>
     
    <theorem xml:id="thm-4-5-19">
        <statement>
          <p>
            Let <m>V</m> be a finite dimensional vector space over <m>\R</m>.
            Then any linearly independent set
            <m>S =\{v_1,\ldots,
            v_k\}</m> can be extended to a basis of <m>V</m>.
            More precisely, there exist vectors,
            <m>u_1,\ldots,
            u_{n-k}</m> where <m>n={ dim}(V)</m> such that
            <m>\beta=\{v_1,\ldots,
            v_k,u_1,\ldots, u_{n-k}\}</m> is a basis of <m>V</m>.
          </p>
        </statement>

        <proof>
          <p>
            Let <m>S=\{v_1,\ldots,v_k\}</m> be linearly independent set. 
            if <m>L(S)=V</m>, then we are done as <m>S</m> is a basis of <m>V</m>. Suppose 
            not, chose <m>u_1\notin L(S)</m>. Then by <xref ref="lemma-4-5-17"/>, 
            <m>\{v_1,\ldots, u_1\}</m> is linearly independet in <m>V</m>. 
            If <m>L(\{v_1,\ldots, u_1\})=V</m>, then <m>\{v_1,\ldots, u_1\}</m> is a basis of <m>V</m>.
          Otherwise we choose, <m>u_2\not in L(\{v_1,\ldots, u_1\})</m>. Again 
          by <xref ref="lemma-4-5-17"/>, 
            <m>\{v_1,\ldots, u_1,u_2\}</m> is linearly independet in <m>V</m>. If <m>L(\{v_1,\ldots, u_1,u_2\})=V</m>, 
            then <m>\{v_1,\ldots, u_1,u_2\}</m> is a basis of <m></m>. 
            Otherwise, we continue this process. Since <m>V</m>is finite dimensional vector space, 
            this process must end after a finite number of steps. In fact, the process ends exactly
            <m>n-k</m> steps where <m>n</m> is the dimension of <m>V</m>. (can you see it why?)
          </p>
        </proof>
      </theorem>
      
      <p>
        The above results give a way to find a basis of a finite dimensional vector 
        space starting with a nonzero vector in <m>V</m>.
      </p>
      
      <example>
        <statement>
          <p>
            Complete the set <m>S=\{v_1=(1, 2, 1, 0),
            v_2=(2, 2, 1, 0)\}</m> to a basis of <m>\R^4</m>.
            One way of achieving this to find <m>v_3\notin L(S)</m>.
            Then Chose <m>v_4\notin L(\{v_3\}\cup S)</m>.
            Then in view of <xref ref="lemma-4-5-17"/>,
            <m>\beta=\{v_1,v_2,v_3,v_4\}</m> is linearly independent.
            Since <m>\dim(\R^4)=4</m>, <m>\beta</m> is a basis of <m>\R^4</m>.
          </p>
          <p>
            Another way to achieve this is to look at the standard basis vectors <m>e_i</m> not in <m>L(S)</m>.
            In particular, <m>v_3,v_4\in\{e_1,e_2,e_3,e_4\}</m>.
            In order to find this we can apply RREF to the matrix
            <m>\begin{bmatrix}v_1\amp  v_2 \amp  e_1 \amp  e_2 \amp  e_3 \amp  e_4 \end{bmatrix}</m> and choose columns corresponding to the pivots.
            We have
            <me>
              \left[\begin{array}{rrrrrr} 1 \amp  2 \amp  1 \amp  0 \amp  0 \amp  0 \\ 2 \amp  2 \amp  0 \amp  1 \amp  0 \amp  0 \\ 1 \amp  1 \amp  0 \amp  0 \amp  1 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \end{array} \right]\xrightarrow{RREF} \left[ \begin{array}{rrrrrr} 1 \amp  0 \amp  -1 \amp  0 \amp  2 \amp  0 \\ 0 \amp  1 \amp  1 \amp  0 \amp  -1 \amp  0 \\ 0 \amp  0 \amp  0 \amp  1 \amp  -2 \amp  0 \\ 0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \end{array} \right]
            </me>.
          </p>
          <p>
            Clearly pivot columns are 1,2,4,6, which corresponds to vector <m>v_1,v_2,e_2, e_4</m>.
            Thus <m>\{v_1,v_2,e_2,e_4\}</m> is an extended basis of <m>\R^4</m>.
          </p>
        </statement>
      </example>
     
    <p>
      Note that <xref ref="thm-4-5-19"/> gives a way to find a basis of finite  dimensional 
      vector space starting from a linearly independet set. How about starting with s finite spanning 
      set?  
    </p>
      <theorem xml:id="thm-4-5-20">
        <statement>
          <p>
            Let <m>V</m> be a finite dimensional vector space over <m>\R</m>.
            Suppose <m>S</m> is a finite set such that <m>L(S)=V</m>.
            Then there exists a subset
            <m>S'\subset S</m> such that <m>S'</m> is a basis of <m>V</m>.
          </p>
        </statement>

        <proof>
          <p>
            Assume that <m>0\notin S</m>. Otherwise <m>L(S)=L(S\setminus\{0\})</m>. 
            We shall use well-ordering principle of natural numbers which states that any non empty
            subset of natural number has a least element.
          </p>
          <p>
          Define 
          <me>
            {\cal A}:=\{A\subset S:L(A)=L(S)=V\}.
          </me>
          Clearly, <m>S\in {\cal A}</m>. Hence <m>T:=\{|A|: A\in {\cal A}\}</m> is a non empty subset of 
          natural numbers. By the Wel-ordering principle, <m>T</m> has a least element. Let <m>n</m>
          be the least element of <m>T</m> and <m>B\in {\cal A}</m> such that <m>|B|=n</m>. We claim 
          that <m>B</m> is a basis of <m>V</m>. By definition <m>L(B)=V</m>. We claim that <m>B</m> is linearly independet.
          Suppose not, there exist <m>v\in B</m> such that <m>v\in L(S\setminus \{v\})</m>. 
          This implies <m>S\setminus \{v\}\subset S</m> and <m>L(S\setminus \{v\})=V</m>. 
          Hence <m>S\setminus \{v\}\in {\cal A}</m> but <m>|S\setminus \{v\}|=n-1</m>, a contradiction. This 
          shows that <m>B</m> is linearly independent. Hence <m>B</m> is basis of <m>V</m>.
          </p>
        </proof>
      </theorem>

      <example>
        <statement>
          <p>
            Consider <m>v_1,\ldots, v_8</m> in <m>\R^5</m>, where
            <me>
              \begin{split} v_1=(2, -3, 4, -5, -2), v_2=(-6, 9, -12, 15, -6), v_3=(3, -2, 7, -9, 1),\\ v_4=(2, -8, 2, -2, 6),
               v_5=(-1, 1, 2, 1, -3), v_6=(0, -3, -18, 9, 12), \\
               v_7=(1, 0, -2, 3, -2), v_8=(2, -1, 1, -9, 7) \end{split}
            </me>
          </p>
          <p>
            We wish to find a subset of
            <m>\{v_1,\ldots,
            v_8\}</m> which is a basis of <m>\R^5</m>.
            We can achieve this by applying RREF to the column matrix <m>\begin{bmatrix}v_1\amp  v_2\amp \cdots \amp  v_8 \end{bmatrix}</m>.
            Thus
            <md>
              <mrow>\left[\begin{array}{rrrrrrrr} 2 \amp  -6 \amp  3 \amp  2 \amp  -1 \amp  0 \amp  1 \amp  2 \\ 
                -3 \amp  9 \amp  -2 \amp  -8 \amp  1 \amp  -3 \amp  0 \amp  -1 \\ 
                4 \amp  -12 \amp  7 \amp  2 \amp  2 \amp  -18 \amp  -2 \amp  1 \\
                 -5 \amp  15 \amp  -9 \amp  -2 \amp  1 \amp  9 \amp  3 \amp  -9 \\ 
                 -2 \amp  -6 \amp  1 \amp  6 \amp  -3 \amp  12 \amp  -2 \amp  7 \end{array}\right]</mrow>
                 <mrow>\xrightarrow{RREF} 
                 \left[\begin{array}{rrrrrrrr} 
                  1 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \\ 
                  0 \amp  1 \amp  0 \amp  -\frac{4}{3} \amp  0 \amp  -\frac{1}{3} \amp  0 \amp  \frac{1}{3} \\ 
                  0 \amp  0 \amp  1 \amp  -2 \amp  0 \amp  -2 \amp  0 \amp  1 \\ 
                  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  -4 \amp  0 \amp  -2 \\ 
                  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  0 \amp  1 \amp  -1 \end{array} \right]
                 </mrow>
            </md>
          </p>
          <p>
            Clearly pivot columns are 1, 2, 3, 5, 7.
            Hence <m>\{v_1,v_2,v_3,v_5,v_7\}</m> is basis of <m>\R^5</m>.
          </p>
        </statement>
      </example>

     
      <definition xml:id="maxLI">
        <statement>
          <p>
            Let <m>V</m> be a vector space.
            A set of vectors <m>S</m> of <m>V</m> is called a maximal linearly independent set if
            <m>S\cup \{v\}</m> is linearly dependent for any vector <m>v\in V</m>.
          </p>
        </statement>
      </definition>

      <example>
        <statement>
          <p>
            (i) Any set <m>S</m> with two linearly independent set of vectors in <m>\R^2</m> is a maximal linearly independent set.
          </p>
          <p>
            (ii) Any set <m>S</m> with three linearly independent set of vectors in <m>\R^3</m> is a maximal linearly independent set.
          </p>
        </statement>
      </example>

      <definition xml:id="minimalgenerator">
        <statement>
          <p>
            Let <m>V</m> be a vector space.
            A set of vectors <m>S</m> of <m>V</m> is called a minimal set of generators if (i) <m>L(S)=V</m> and (ii) for any <m>u\in S</m>,
            <m>L(S\setminus \{u\})\neq V</m>.
          </p>
        </statement>
      </definition>

      <example>
        <statement>
          <p>

            <ol marker="(i)">
              <li>
                <p>
                  Any set <m>S</m> with two linearly independent set of vectors in 
                  <m>\R^2</m> is a minimal set of generators.
                </p>
              </li>
              <li>
                <p>
                  Any set <m>S</m> with three linearly independent set of vectors in
                  <m>\R^3</m> is a minimal set of generators.
                </p>
              </li>
            </ol>
            
          </p>
        </statement>
      </example>
      <p>
        In the following theorem we mention the equivalent conditions for a set to be a basis of a
         finite dimensional vector space.
      </p>
      <theorem xml:id="thm-basis-equiv">
        <statement>
          <p>
            Let <m>V</m> be a finite dimensional vector space over <m>\R</m>.
            Then the following are equivalent.
          </p>
          <p>
            <ol>
              <li>
                <p>
                  <m>\beta=\{v_1,\ldots,v_n\}</m> is a basis of <m>V</m>.
                </p>
              </li>
              <li>
                <p>
                  <m>L(\beta)=V</m> and <m>\beta</m> is linearly independent.
                </p>
              </li>
              <li>
                <p>
                  <m>\beta</m> is maximal linearly independent set.
                </p>
              </li>
              <li>
                <p>
                  <m>\beta</m> is minimal set of generators.
                </p>
              </li>
            </ol>
          </p>
        </statement>

        <proof>
          <p>
          We prove this by showing <m>(i)\implies (ii)\implies (iii)\implies (iv)\implies (i)</m>.
          </p>
          <case direction="cycle">
            <p>
              Follows from <xref ref="Exer-4-5-1"/>.
            </p>
          </case>

          <case direction="cycle">
            <p>
            Let <m>\beta=\{v_1,\ldots, v_n\}</m> be a basis. Let <m>v\in V</m>, then by 
            <xref ref="sec4-4-thm2"/> <m>\beta\cup \{v\}</m> is linearly dependeny. 
            Hence <m>\beta</m> is maximal linearly independent set in <m>V</m>.
            </p>
          </case>
          <case direction="cycle">
            <p>
             Let <m>\beta=\{v_1,\ldots,v_n\}</m> be a maximal linearly independent set.  
             To show <m>\beta</m> is a minimal set of generators, we need to show two things 
             (i) <m>L(\beta)=V</m> and (ii) no proper subset of <m>\beta</m> can span <m>V</m>.
            </p>

             <p>
             To prove (i), let <m>v\in V</m>. Since <m>\beta</m> is a maximal set of generators, 
             <m>\{v,v_1,\ldots,v_n\}</m> is linearly dependent. Hence there exits scalars, 
             <m>\alpha_0,\alpha_1,\ldots,\alpha_n </m> not all zero such that 
             <me>
              \alpha_0 v+\sum_{i=1}^n\alpha_i v_i=0.
             </me>
             We claim, that <m>\alpha_0\neq 0</m>. For if <m>\alpha_0=0</m>, then we have 
             scalars <m>\alpha_1,\ldots,\alpha_n </m> not all zero such that 
             <me>
              \sum_{i=1}^n\alpha_i v_i=0
             </me>
             which implies <m>\beta</m> is linearly dependent, a contradiction. 
             Hence <m>\alpha_0\neq 0</m>. Thus we have 
             <me>
              v = -\frac{1}{\alpha_0} \sum_{i=1}^n\alpha_i v_i. 
             </me>
              This implies, <m>L(\beta)=V</m>.
            </p>
          <p>
            We prove (ii) by contradiction. Without loss of generality, we assume that 
            <m>\{v_1,v_2,\ldots, v_{n-1}\}</m> spans <m>V</m>. This would mean 
            <m>v</m> lies in the spanning set of <m>\{v_1,v_2,\ldots, v_{n-1}\}</m>. In particular,
            <m>\{v_1,v_2,\ldots, v_{n-1},v_n\}</m> is linearly dependent, a contradiction.  
            </p>
          </case>

           <case direction="cycle">
            <p>
            Let <m>\beta=\{v_1,\ldots,v_n\}</m> be a minimal generating set. We need to 
            show that it is a basis.
            </p>
            <p>
              Since <m>\beta</m> is a generating set, for any <m>v\in V</m>, there exist
              scalars <m>\alpha_i</m> such that <m>v=\sum \alpha_i v_i</m>. We need to show that 
              this expression is unique.  Suppose not, let <m>v=\sum \beta_i v_i</m> with  
              <m>\beta_i\neq \alpha_i</m> for at least one <m>i\in \{1,\ldots,n\}</m>. Then we have 
              <me>
                0=v-v = (\beta_i-\alpha_i)v_i+\sum_{j\neq i} (\beta_j-\alpha_j) v_j.
              </me>
              In paticular, 
              <me>
                v_i=-\frac{1}{(\beta_i-\alpha_i)}\sum_{j\neq i} (\beta_j-\alpha_j) v_j.
              </me>
              This would imply <m>\{v_1,\ldots, v_{i-1},v_{i+1},\ldots,v_n\}</m> is a generating set for 
              <m>V</m> (why?). This is a contradiction to the minimality of <m>\beta</m>.
            </p>
           </case>
        </proof>
      </theorem>
    


    </subsection>

  
    <subsection>
      <title>Lagrange Interpolation</title>
      <p>
        Consider the vector space <m>{\cal P}_n(\R)</m>.
        Fix <m>n+1</m> distinct real numbers <m>c_0,c_1,\ldots, c_n</m>.
        Define polynomials
        <men xml:id="lagrange-eq1">
          \ell_i(x)= \frac{(x-c_0)\cdots (x-c_{i-1})(x-c_{i+1})\cdots(x-c_n)}{(c_i-c_0)\cdots (c_i-c_{i-1})(c_i-c_{i+1})\cdots(c_i-c_n)}
          </men>
          for <m>i=0,1,\ldots n</m>.
        The above equation can be written as 
        <men xml:id="lagrange-eq1-1"> 
          \ell_i(x)=\prod_{j=0,j\neq i}^{n}\frac{x-c_j}{c_i-c_j}.
        </men>
        </p>
      <p>
        It is easy to see that <m>\ell_i(c_j)=1</m> if <m>j=i</m> and 0 otherwise.
        We claim that <m>\{\ell_i\}_{i=0}^n</m> is a linearly independent subset of <m>{\cal P}_n(\R)</m>.
        For
        <men xml:id="lagrange-eq2">
          \alpha_0\ell_0 + \alpha_1\ell_1+\cdots+\alpha_n\ell_n=\sum\alpha_i\ell_i=0
        </men>.
      </p>
      <p>
        Here the right hand side is the zero polynomial.
        This implies <m>\sum\alpha_i\ell_i(c_j)=0</m> for all <m>j=0,\ldots, n</m>.
        Since <m>\sum\alpha_i\ell_i(c_j)=\alpha_j</m>,
        it implies that <m>\alpha_j=0</m> for all <m>j=0,\ldots, n</m>.
        Hence the claim.
      </p>
      <p>
        Since <m>{\cal P}_n(\R)</m> is <m>(n+1)</m>-dimensional vector space,
        the set <m>\{\ell_i\}_{i=0}^n</m> is a basis.
        Hence every <m>n</m>-th degree polynomial can be expressed uniquely as linear combination of <m>\ell_i</m>.
        Suppose <m>g</m> is polynomial passing through points
        <m>\{(x_i,y_i)\}_{i=0}^n</m>, (that is <m>g(x_i)=y_i)</m>) where
        <m>x_0,\ldots,x_n</m> are <m>n</m> distinct real numbers.
        This unique polynomial is given by
        <men xml:id="lagrange-eq3">
          g(x)=\sum_{i=0}^n \ell_i(x)y_i
        </men>
        called the <em>Lagrange interpolation</em>
        polynomial passing through <m>\{(x_i,y_i)\}_{i=0}^n</m>.
      </p>
    </subsection>

    <subsection>
      <title>Dimension Formula</title>
      <p>
            Let <m>V</m> be a vector space over <m>\R</m>.
            Let <m>W_1</m> and <m>W_2</m> be subspaces of <m>V</m>.
           Define a subset
            <me>
              W_1+W_2:= \{x+y:x\in W_2,y\in W_2\}.
            </me>
        
        It is easy to check that <m>W_1+W_2</m> is a subspace of <m>V</m>. (why?)
          </p>

      <p>
        In addition if, we assume that <m>V</m> is a finite dimensional, then what can you say about the 
        dimension of <m>W_1+W_2</m>? 
      </p>

        <p>
          Recall, an analogous result from set theory. If <m>A</m> and <m>B</m> are finite 
          sets then 
          <me>
            |A\cup B|=|A|+|B|-|A\cap B|.
          </me>
        </p>
        <p>
        The 
        following theorem gives a way to find the dimension of <m>W_1+W_2</m>.
      </p>

      <theorem xml:id="thm-dimension-formula2">
        <statement>
          <p>
           Let  <m>W_1</m> and <m>W_2</m> be two subspaces of a finite dimensioanl vector 
            space  <m>V</m> over <m>\R</m>.  Then
            <md>
            <mrow xml:id="dimension-formula" number="yes">\dim{(W_1+W_2)}=\dim{(W_1)}+\dim{(W_2)}-\dim{(W_1\cap W_2)}</mrow>
            </md>
          </p>
        </statement>

        <proof>
          <p>
        The basic idea of the proof is to start with a basis of <m>W_1\cap W_2</m> and extend 
        this to a basis of <m>W_1</m> and <m>W_2</m> and them show that union of basis of 
        <m>W_1</m> and <m>W_2</m> gives a basis of <m>V</m>.
          </p>
          <p>
            Let <m>\{u_1,\ldots,u_k\}</m> be basis of <m>W_2\cap W_2</m>. 
            Let <m>\{u_1,\ldots,u_k,v_1,\ldots, v_m\}</m> <m>\{u_1,\ldots,u_k,w_1,\ldots, v_n\}</m> be 
            extended bases of <m>W_1</m> and <m>W_2</m> respectively. Then we have 
            <md>
              <mrow> \rm{dim} (W_1)+\rm{dim} (W_2)-\rm{dim} (W_1\cap W_2)\amp=\amp k+n+k+m-k </mrow>
              <mrow> \amp=\amp k+m+n. </mrow>
            </md>
           We calim that <m>\beta =\{u_1,\ldots,u_k,v_1,\ldots, v_m,w_1,\ldots,w_n\}</m> is a 
            bais of <m>W_1+W_2</m>.
           </p>
           <p>
            Let <m>x\in W_1+W_2</m>. Then <m>x=w_1+w_2</m> for some <m>w_1\in W_1</m> and 
            <m>w_2\in W_2</m>. Let <m>w_1=\sum\alpha_iu_i+\sum \beta_j v_j</m> and  
            <m>w_2=\sum\gamma_i u_i+\sum \delta_jw_j</m>. Then we have 
            <me>
              x=\sum(\alpha_i+\gamma_i)u_i+\sum \beta_j v_j+\sum \delta_jw_j.
            </me>
             This shows that <m>\beta</m> spans <m>W_1+W_2</m>. 
            </p>
            <p>
              Next, we show that <m>\beta</m> is linearly independent. 
            </p>

            <p>
              Let <m>\sum \alpha_iu_i+\sum \beta_j v_j+\sum \gamma_rw_r=0</m>.  Then we have 
              <men xml:id="eq-dim-formula1">
                \sum \alpha_iu_i+\sum \beta_j v_j=-\sum \gamma_rw_r
              </men>
              Since the expression on the left hand side is in <m>W_1</m>, we have 
              <m>\sum \gamma_rw_r \in W_1\cap W_2</m>. Hence there exist scalars <m>\delta_i</m> such that 
              <me>
                -\sum \gamma_rw_r = \sum \delta_iu_r
              </me>
              which implies 
              <me>
              \sum \delta_iu_r+\sum \gamma_rw_r=0.
              </me>
              Since <m>\{u_1,\ldots,u_k,w_1,\ldots,w_n\}</m> is linearly independet, we have 
              <m>\gamma_1=\cdots=\gamma_n=0</m>.  Hence by <xref ref="eq-dim-formula1"/>, we have 
              <me>
                \sum \alpha_iu_i+\sum \beta_j v_j=0.
              </me>
            Since <m>\{u_1,\ldots,u_k,v_1,\ldots,v_m\}</m> is linearly independet, we have 
            <m>\alpha_1=\cdots=\alpha_k=0</m> and <m>\beta_1=\cdots=\beta_m=0</m>.  This proves that 
            <m>\beta</m> is linearly independent. This completes the proof. 
            </p>
        </proof>
      </theorem>

      <example>
        <statement>
          <p>
            Let  <m>W_1=:\{(x_1,x_2,x_3)\in \R^3:x_1+x_2+x_3=0\}</m> and 
            <m>W_2:=\{(x_1,x_2,x_3)\in \R^3:x_1+x_2-x_3=0\}</m> of <m>\R^3</m>.  
            Clearly <m>\dim{(W_1)}=\dim{(W_2)}=2</m>.
            What is <m>W_1\cap W_2</m>?
            It is the line of intersection of the two planes,
            <m>x_1+x_2+x_3=0</m> and <m>x_1+x_2-x_3=0</m>.
            Thus <m>\dim{(W_1\cap W_2)}=1</m>.
            It is easy to see that
            <me>
              W_2\cap W_2=\{\alpha(1,-1,0):\alpha\in\R\}.
            </me>
          </p>
          <p>
            What is <m>W_1+W_2</m>?
            One can easily show that <m>W_1+W_2=\R^3=V</m>.
            However by dimension formula
            <md>
              <mrow>\dim{(W_1+W_2)}=\amp \dim{(W_1)}+\dim{(W_2)}-\dim{(W_1\cap W_2)}</mrow>
                <mrow>=\amp 2+2-1=3</mrow>
            </md>.
          </p>
          <p>
            Since <m>W_1+W_2</m> is a 3 dimensional subspace of <m>\R^3</m>,
            it is in fact <m>\R^3</m>.
          </p>
        </statement>
      </example>

<definition xml:id="def-direct-sum">
  <statement>  
    <p>
      Let <m>W_1</m>  and <m>W_2</m> be subspaces of a vector space <m>V</m> such that <m>W_1+W_2=V</m> and 
      <m>W_1\cap W_2=\emptyset</m>, then we say that <m>V</m> is a direct sum of <m>W_1</m> and <m>W_2</m>. 
      We write this as <m>V=W_1\oplus W_2</m>. 
</p>
 </statement>
</definition>

<example>
  <statement>
    <p>
      <ol>
        <li>
          <p>
            <m>\R^2=\R e_1\oplus \R e_2</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>W_1=\{(x_1,x_2,x_3)\in \R^3:x_1+2x_2-x_3=0\}</m> and <m>W_2=\{t(1,2,-1):t\in \R\}</m>. 
            Then <m>\R^3=W_1\oplus W_2</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>W_1</m> be the set of all <m>n\times n</m> symmetric matrices and 
            <m>W_2</m>, the set of all <m>n\times n</m> skew-symmetric matrices. Then 
            <m>M_n(\R)=W_1\oplus W_2</m>.
          </p>
        </li>
        <li>
          <p>
            Let <m>W_1=\{(x_1,x_2,x_3)\in \R^3:x_1+2x_2-x_3=0\}</m> and 
            <m>W_2=\{(x_1,x_2,x_3)\in \R^3:2x_1-x_2+x_3=0\}</m>, then <m>\R^3=W_1+W_2</m> however 
            it is not the direct sum.
          </p>
        </li>
        <li>
          <p>
            Let <m>V</m> be a fininite dimensional vector space with a basis 
            <m>\{v_1,\ldots, v_n\}</m>. Then 
            <me>
              V=\R v_1\oplus \R v_2 \oplus \cdots \oplus \R v_n.
            </me>
          </p>
        </li>
      </ol>
    </p>
  </statement>

</example>

    </subsection>
  

  </section>
 