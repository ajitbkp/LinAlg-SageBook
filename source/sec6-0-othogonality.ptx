<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec6-0-orthogonality" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Orthogonality</title>
<introduction>
    <p>
        In this chapter we deal with orthogonality of vectors and various properties.
<p>
        Recall, that if <m>S=\{v_1,\ldots,
            v_k\}</m> is linearly independent subset of <m>\R^n</m> and <m>v_{k+1}\notin span(S)</m>,
            then <m>S\cup \{v_{k+1}\}=\{v_1,\ldots,
            v_k,v_{k+1}\}</m> is linearly independent subset of <m>\R^n</m>.
    </p>
</p>
</introduction>

  <definition>
    <statement>
      <p>
        A set of vectors <m>\{v_1,\ldots,
        v_k\}</m> is called orthogonal if
        <me>
          v_i\cdot v_j = \delta_{ij}=\begin{cases}1 \amp  \text{ if \(i=j\) }  \\ 0 \amp \text{ otherwise } \end{cases}
        </me>
      </p>
    </statement>
  </definition>
  <p>
    If <m>x=(x_1,\ldots, x_n) \in \R^n</m>,
    then <m>\norm{x}^2: =x_1^2+\cdots+x_n^2</m>.
  </p>
  <exercise xml:id="orth-ex2-1">
    <statement>
      <p>
        Let <m>\{u_1,\ldots,
        u_k\}</m> be an orthogonal set of vectors in <m>\R^n</m>.
        Let <m>u\in \R^n</m> and define
        <me>
          u_{k+1}:=u-\frac{u_1\cdot u_{k+1}}{\norm{u_1}^2}u_1-\frac{u_2\cdot u_{k+1}}{\norm{u_2}^2}u_2-\cdots -\frac{u_k\cdot u_{k+1}}{\norm{u_k}^2}u_k
        </me>,
      </p>
      <p>
        Then
      </p>
      <p>
        (i) <m>u_i\cdot u_{k+1}=0</m> for all <m>i=1,\ldots, k</m>
      </p>
      <p>
        (ii) If <m>u\notin span(\{u_1,\ldots, u_k\})</m>,
        then <m>u_{k+1}\neq 0</m> and
        <m>\{u_1,\ldots,
        u_k,u_{k+1}\}</m> is an orthogonal set.
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        If <m>\{u_1,\ldots,
        u_n\}</m> is orthogonal set then it is linearly independent.
      </p>
    </statement>
  </exercise>
  <definition>
    <statement>
      <p>
        A basis <m>\beta =\{u_1,\ldots,
        u_n\}</m> is called an orthogonal basis if <m>\beta</m> is an orthogonal set in <m>\R^n</m>.
        In addition if <m>\norm{u_1}=1</m> for all <m>i</m>,
        then <m>\beta</m> is called an orthonormal basis.
      </p>
    </statement>
  </definition>
  <exercise>
    <statement>
      <ol>
        <li>
          <p>
            The standard basis <m>{\cal E} = \{e_1,\cdots,
            e_n\}</m> is an orthogonal basis of <m>\R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>\beta =\left\{\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right), \left(\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\right)\right\}</m> is an orthonormal basis of <m>\R^2</m>.
          </p>
        </li>
        <li>
          <p>
            <m>\beta=\{(2,1,1), (-1,1,1),(0,-1,1)\}</m> is an orthogonal basis of <m>\R^3</m>.
            However, it is not an orthonormal basis.
          </p>
        </li>
        <li>
          <p>
            <m>\beta'=\left\{\left(\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}},\frac{1}{\sqrt{6}}\right), \left(\frac{-1}{\sqrt{3}},\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}}\right),\left(0,\frac{-1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)\right\}</m> is an orthonormal basis of <m>\R^n</m>.
          </p>
        </li>
      </ol>
    </statement>
  </exercise>

<subsection>
  <title>What is an advantage of having an orthonormal basis?</title>
  <p>
    Let <m>\beta =\{u_1,\ldots, u_n\}</m> be an orthonormal basis of <m>\R^n</m>.
    Let <m>x\in \R^n</m>.
    Then there exists scalars <m>\alpha_1,\ldots, \alpha_n</m> such that <m>x=\alpha_1u_1+\cdots +\alpha_nu_n</m>.
    Then is is easy to check that <m>\alpha_i = x\cdot u_i</m> for all <m>i</m>.
    This is advantage of having an orthonormal basis.
  </p>
  <exercise>
    <statement>
      <p>
        (i) Find the coordinates of a vector <m>(1,2)</m> with respect to an orthonormal basis <m>\beta =\left\{\left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right), \left(\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\right)\right\}</m> of <m>\R^2</m>.
      </p>
      <p>
        (ii) Find the coordinates of the vector <m>(2,5,7)</m> with respect to an orthonormal basis <m>\beta'=\left\{\left(\frac{2}{\sqrt{6}},\frac{1}{\sqrt{6}},\frac{1}{\sqrt{6}}\right), \left(\frac{-1}{\sqrt{3}},\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}}\right),\left(0,\frac{-1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)\right\}</m> of <m>\R^3</m>.
      </p>
    </statement>
  </exercise>
  <p>
    Next we deal with to find an orthonormal basis of <m>\R^n</m> or any subspace of <m>\R^n</m>.
  </p>
</subsection>
</section>

<!--
<section>
  <title>Gram-Schmidt Orthogonalization Process</title>
  <introduction>
    <p>
     In this section, we look at how to construct an 
     orthogonal basis from a basis. 
    </p>
  </introduction>
  <p>
    Let <m>\{v_1,\ldots, v_n\}</m> be a basis of <m>\R^n</m>.
    Define
    <md>
      <mrow>u_1:  =\amp  v_1</mrow>
      <mrow>u_2:  =\amp  v_2 -\frac{u_2\cdot u_1}{\norm{u_1}^2}u_1</mrow>
      <mrow>u_3:  =\amp  v_3 -\frac{v_3\cdot u_1}{\norm{u_1}^2}u_1-\frac{v_3\cdot u_2}{\norm{u_2}^2}u_2</mrow>
      <mrow> \amp  \vdots</mrow>
      <mrow>u_n:  =\amp  v_n -\frac{v_n\cdot u_1}{\norm{u_1}^2}u_1-\cdots -\frac{v_n\cdot u_{n-1}}{\norm{u_{n-1}}^2}u_{n-1}</mrow>
    </md>
  </p>
  <p>
    In view of Ex.
    <xref ref="orth-ex2-1"></xref>,
    it is easy to see that <m>\{u_1,\ldots,
    u_n\}</m> is an orthogonal basis of <m>\R^n</m>.
    Now we normalize <m>u_i's</m>.
    Define <m>q_i=\frac{u_i}{\norm{u_i}}</m>.
    Then <m>\{q_1,\cdots, q_n\}</m> is an orthononal basis of <m>\R^n</m>.
    Note that we could have defined <m>q_i</m> immediately after defining <m>u_i</m>.
  </p>
  <p>
    This process is called the Gram-Schmidt orthogonalization process.
  </p>
  <p>
    Geometrically <m>u_2</m>,
    constructed by subtracting the orthogonal projection of <m>v_2</m> on to <m>u_1</m>.
    In order to construct <m>u_3</m>,
    we take sum of orthogonal projections of <m>v_3</m> onto <m>u-1</m> and <m>u_2</m>,
    which is the orthogonal projection of the plane spanned by <m>u_1</m> and <m>u_2</m> and subtract this from <m>v_3</m>.
    Readers are encouraged to draw figures.
  </p>
  <example xml:id="gram-schmidt-eg1">
    <statement>
      <p>
        Use the Gram-Schmidt orthogonalization process to find an orthonormal basis of <m>\R^3</m> starting with a basis <m>\{(0,1,1), (1,1,1), (1,-2,2)\}</m>.
        Let <m>v_1 = (0, 1, 1), v_2=(1, 1, 1), v_3=(1, -2, 2)</m>.
        Then we have
        <md>
          <mrow>u_1: =  (0, 1, 1)</mrow>
          <mrow>u_2:  = v_2 -\frac{u_2\cdot u_1}{\norm{u_1}^2}u_1=(1, 1, 1)-\frac{2}{2}(0,1,1)=(1,0,0)</mrow>
          <mrow>u_3:= v_3 -\frac{u_3\cdot u_1}{\norm{u_1}^2}u_1-\frac{u_3\cdot u_2}{\norm{u_2}^2}u_2</mrow>
          <mrow> =   (1, -2, 2)-\frac{0}{2}(0,1,1)-\frac{1}{1}(1,0,0)=(0,-2,2)</mrow>
        </md>
      </p>
      <p>
        Thus the orthonormal basis is obtained from the given basis is
        <me>
          \left\{q_1 = \left(0, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right), q_2=(1,0,0), q_3 =\left(0,\frac{-1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right)\right\}
        </me>.
      </p>
    </statement>
  </example>
  <example xml:id="gram-schmidt-eg2">
    <statement>
      <p>
        Consider the matrix <m>A=\left(\begin{array}{rrrr} -1 \amp 1 \amp 0 \amp 1 \\ 1 \amp -1 \amp 0 \amp 1 \\ -1 \amp 0 \amp 2 \amp 1 \end{array} \right)</m>.
        Find an orthogonal basis of the row space of <m>A</m>.
        It is easy to check that rank of <m>A</m> is 3.
        Hence row are linearly independent vectors in <m>\R^4</m>.
      </p>
      <p>
        Let
        <m>
          v_1=(-1,1,0,1), v_2 = (1,-1,0,1),   v_3=(-1,0,2,1)
        </m>.
        <md><mrow>u_1: = \amp (-1,1,0,1)</mrow>
          <mrow>u_2: =  \amp v_2 -\frac{u_2\cdot u_1}{\norm{u_1}^2}u_1</mrow>
           <mrow> =\amp (1,-1,0,1)-\frac{-1}{3}(-1,1,0,1)=(2/3, -2/3, 0, 4/3)</mrow>
          <mrow>u_3: = \amp  v_3 -\frac{u_3\cdot u_1}{\norm{u_1}^2}u_1-\frac{u_3\cdot u_2}{\norm{u_2}^2}u_2</mrow>
          <mrow>  = \amp  (-1,0,2,1)-\frac{2}{3}(-1,1,0,1)-\frac{2/3}{9/2}(2/3, -2/3, 0, 4/3)</mrow>
          <mrow>=\amp (-1/2, -1/2, 2, 0)
          </mrow>
        </md>
      </p>
      <p>
        Hence
        <me>
          \{(-1, 1, 0, 1) (2/3, -2/3, 0, 4/3) (-1/2, -1/2, 2, 0)\}
        </me>
        is an orthogonal basis of the row space of <m>A</m>.
      </p>
    </statement>
  </example>
  <exercise>
    <statement>
      <p>
        Use the Gram-Schmidt orthogonalization process to find an orthonormal basis of <m>\R^3</m> starting with a basis <m>\beta=\{(1,1,1),(-1,1,1),(-1,0,1)\}</m>.
      </p>
    </statement>
  </exercise>
  <exercise>
    <statement>
      <p>
        Use the Gram-Schmidt orthogonalization process to find an orthonormal basis of the subspace <m>W\subset \R^4</m> with basis
        <me>
          \beta = \{ (-1,1,1,0),(-1,0,1,0),(1,0,0,1)\}
        </me>.
      </p>
    </statement>
  </exercise>
</subsection>

<subsection>
<title>Orthogonal Complements</title>
<introduction>
  <p>
    
  </p>
</introduction> 
<definition>
    <statement>
      <p>
        Let <m>U\subset \R^n</m>. Then <m>U^\perp:=\{x\in \R^n: x\cdot u=0, \forall u\in U\}</m> 
        is called the orthogonal complement of <m>U</m>.
      </p>
    </statement>
  </definition>
  
  <exercise>
    <statement>
      <p>
        (i) <m>\{0\}^\perp = \R^n</m>
      </p>
      <p>
        (ii) <m>{\R^n}^{\perp}=\{0\}</m>.
      </p>
      <p>
        (iii) Let <m>U\subset \R^n</m>.
        Then <m>U^\perp</m> is a subspace of <m>\R^n</m>.
        Note that <m>U</m> need not be a subspace of <m>\R^n</m>.
      </p>
    </statement>
  </exercise>

  <example>
    <statement>
      <p>
        Let <m>U=span(\{(2,-1,1),(1,2,-1)\})</m>.
        Then
        <md>
          <mrow>U^\perp  =\amp  \{(x_1,x_2,x_3): 2x_1-x_2+x_3=0,x_1+2x_2-x_3=0\}</mrow>
          <mrow> =\amp null\left(\begin{array}{rrr} 2 \amp  -1 \amp  1 \\ 1 \amp  2 \amp  -1 \end{array} \right)= \{t(1,-3,-5):t\in \R\}</mrow>
        </md>.
      </p>
    </statement>
  </example>
  <example>
    <statement>
      <p>
        Find the orthogonal complement of <m>span(\{\left(2,\,0,\,1,\,-1\right), \left(1,\,2,\,0,\,-1\right)\})</m>.
        <md>
          <mrow>U^\perp  =\amp  \{(x_1,x_2,x_3,x_3): 2x_1+x_3-x_4=0,x_1+2x_2-x_4=0\}</mrow>
          <mrow> =\amp null\left(\begin{array}{rrrr} 2 \amp  0 \amp  1 \amp  -1 \\ 1 \amp  2 \amp  0 \amp  -1 \end{array} \right)</mrow>
          <mrow> =\amp  span(\{(1,  0, -1,  1),(0,  1,  2,  2)\})</mrow>
        </md>
      </p>
    </statement>
  </example>

  <definition xml:id="subspace-projection1">
    <statement>
      <p>
        Let <m>U</m> be a subspace of <m>\R^n</m> with and orthogonal basis <m>\{u_1,\ldots,u_k\}</m>.
        If <m>x\in \R^n</m>, then
        <men xml:id="proj_onto_subspace">
          \proj_U(x)=\frac{x\cdot u_1}{\norm{u_1}^2}u_1+\frac{x\cdot u_2}{\norm{u_2}^2}u_2+\cdots + \frac{x\cdot u_k}{\norm{u_k}^2}u_k
        </men>.
      </p>
    </statement>
  </definition>
  <exercise>
    <statement>
      <p>
        Let <m>U</m> be a subspace of <m>\R^n</m> and <m>p=\proj_U(x)</m>.
        Then
      </p>
      <p>
        (i) <m>p\in U</m> and <m>(x-p) \in U^\perp</m>.
      </p>
      <p>
        (ii) <m>p</m> is a vector in <m>U</m>,
        which is closet to <m>x</m>.
        That is for all <m>y\in U</m>,
        <m>\norm{x-p}\leq \norm{x-y}</m>.
        Note that <m>(x-p)\perp (y-p)</m>.
        Hence by the Pythagoras theorem,
        <m>\norm{x-p}^2+\norm{p-y}^2=\norm{x-y}^2</m>.
        <me>
          \norm{x-y}^2=\norm{x-p+p-y}^2=\norm{x-p}^2+\norm{y-p}^2\geq \norm{x-p}^2
        </me>.
      </p>
    </statement>
  </exercise>
  <example>
    <statement>
      <p>
        Consider the plane <m>\pi=\{(x,y,z\in \R^3:3x+y-5z=0\}</m>.
        It is easy to see that <m>v_1=(2,-1,1),
        v_2=(1,2,1)</m> lie on the plane <m>\pi</m>.
        Using the Gram-Schmidt process we can find an orthogonal basis <m>u_1 = (2,-1,1),
        u_2=(2/3, 13/6, 5/6)</m> on <m>\pi</m>.
        Let us find the orthogonal projection of <m>v=(1,2,3)</m> onto <m>\pi</m>.
        The required vector
        <me>
          \proj_\pi(v)=\frac{v\cdot u_1}{\norm{u_1}^2}u_1+\frac{v\cdot u_2}{\norm{u_2}^2}u_2=(13/7, 16/7, 11/7)
        </me>.
      </p>
    </statement>
  </example>
  <p>
    How to find the orthogonal projection of a vector on to the subspace spanned by a set of vectors in <m>\R^n</m>?
    Let <m>\beta = \{w_1,\ldots, w_k\}</m> be a basis of <m>W</m>.
    We want to find the vector <m>p</m> which the orthogonal projection of <m>v</m> onto <m>W</m>.
  </p>
  <p>
    Note that <m>p\in W</m>, therefore,
    there exist scalars <m>x_1,\ldots, x_k</m> such that
    <me>
      p=x_1 v_1+\cdots + x_k v_k=Ax
    </me>
    where <m>A=[v_1~\cdots ~ v_k]</m> and <m>x=[x_1~\cdots~x_k]^T</m>.
  </p>
  <p>
    It is clear that <m>v-p=v-Ax\in W^T</m>.
    Hence <m>(v_i)^T(v-Ax)=0</m> for <m>1\leq i\leq k</m>.
    This is same as
    <me>
      A^T(v-Ax)=0 \implies x=(A^TA)^{-1}(A^Tv)
    </me>.
  </p>
  <p>
    Hence
    <men xml:id="orth-Proj-W">
      p=A(A^TA)^{-1}(A^Tv)
    </men>
  </p>
  <p>
    The matrix <m>Q=A(A^TA)^{-1}A^T</m> is called the
    <q>projection matrix for the subspace <m>W</m></q>.
  </p>

  </subsection>


  <subsection>

    <title>Orthogonal Diagonalization</title>
<statement>
  <p>
    
  </p>
</statement>
    <p>
    <theorem xml:id="orthogonal-ex1">
      <statement>
        <p>
          Let <m>P</m> be an <m>n\times n</m> matrix.
          Then the following are equivalent:
        </p>
        <p>
          (i) <m>P</m> is non-singular and <m>P^{-1}=P^T</m>.
        </p>
        <p>
          (ii) The rows of <m>P</m> are orthonormal vectors in <m>\R^n</m>. (iii) The columns of <m>P</m> are orthonormal vectors in <m>\R^n</m>.
        </p>
      </statement>
    </theorem>

  </p>
<p>
    <definition>
      <statement>
        <p>
          A square matrix <m>P</m> is called an orthogonal matrix if it satisfies any one
          (and hence all)
          the conditions of Theorem
          <xref ref="orthogonal-ex1"></xref>.
        </p>
      </statement>
    </definition>
  </p>

  <p>
     <example>
      <statement>
        <p>
          (i) The matrix <m>\begin{pmatrix}\cos \theta \amp -\sin\theta\\\sin\theta \amp \cos\theta \end{pmatrix}</m> is an orthogonal matrix.
        </p>
        <p>
          (ii) <m>\left(\begin{array}{rrr} -\frac{1}{3} \, \sqrt{3} \amp \sqrt{\frac{2}{3}} \amp 0 \\ \frac{1}{3} \, \sqrt{3} \amp \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp -\sqrt{\frac{1}{2}} \\ \frac{1}{3} \, \sqrt{3} \amp \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp \sqrt{\frac{1}{2}} \end{array} \right)</m> is an orthogonal matrix.
        </p>
      </statement>
    </example>
    
  </p>

  <p>
    <definition>
      <statement>
        <p>
          An <m>n\times n</m> matrix is called
          <em>orthogonally diagonalizable </em>
          if there exists an orthogonal matrix <m>P</m> such that
          <m>P^{-1}AP</m> is a diagonal matrix.
        </p>
      </statement>
    </definition>

  </p>
<p>
  <example>
      <statement>
        <p>
          Let <m>A</m> be a symmetric matrix and <m>\lambda_1</m> and
          <m>\lambda_2</m> are distinct eigenvalues of <m>A</m>.
          If <m>v_1</m> and <m>v_2</m> are eigenvectors corresponding to
          <m>\lambda_1</m> and <m>\lambda_2</m> respectively.
          Then <m>v_1</m> and <m>v_2</m> are orthogonal.
        </p>
      </statement>
    </example>
  </p>

  <p>
    <theorem>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> matrix.
          Then the following are equivalent.
        </p>
        <p>
          (i) <m>A</m> has an orthonormal set of eigenvectors.
        </p>
        <p>
          (ii) <m>A</m> is orthogonally diagonalizable.
        </p>
        <p>
          (iii) <m>A</m> is symmetric.
        </p>
      </statement>
    </theorem>
  </p>

  <p>
    <example>
      <statement>
        <p>
          Consider a matrix <m>A=\left(\begin{array}{rrr} 5 \amp  -2 \amp  -4 \\ -2 \amp  8 \amp  -2 \\ -4 \amp  -2 \amp  5 \end{array} \right)</m>.
          Clearly <m>A</m> is symmetric and hence it is orthogonally diagonalizable.
          The characteristic polynomial of <m>A</m> is
          <me>
            det(xI-A)=x^3 - 18*x^2 + 81*x=x(x-9)^2
          </me>.
        </p>
        <p>
          Hence <m>0, 9, 9</m> are eigenvalues of <m>A</m>.
          Its is easy to find that <m>v_1=(1, 1/2, 1)</m> is an eigenvector corresponding to the eigenvalue 0.
          <m>v_2=(1, 0, -1), v_2=(0, 1, -1/2)</m> are eigenvectors corresponding to eigenvalue 9.
          Hence <m>P:=\left(\begin{array}{rrr} 1 \amp  1 \amp  0 \\ \frac{1}{2} \amp  0 \amp  1 \\ 1 \amp  -1 \amp  -\frac{1}{2} \end{array} \right)</m>.
          Then
          <me>
            P^{-1}AP=\left(\begin{array}{rrr} 0 \amp  0 \amp  0 \\ 0 \amp  9 \amp  0 \\ 0 \amp  0 \amp  9 \end{array} \right)
          </me>.
        </p>
      </statement>
    </example>
  </p>

  <p>
    <problem>
      <statement>
        <p>
          For the following matrices find an orthogonal matrix <m>P</m> such that
          <m>P^{-1}AP</m> is a diagonal matrix.
          <me>
            \begin{pmatrix}2 \amp  -1 \\-1 \amp  1 \end{pmatrix} , \begin{pmatrix}1 \amp  0 \amp  -1\\0 \amp  1 \amp  2\\-1 \amp  2 \amp  5 \end{pmatrix}
          </me>
        </p>
      </statement>
    </problem>
  </p>
<p>
  <theorem>
      <statement>
        <p>
          The following are equivalent for an <m>n\times n</m> matrix <m>A</m>.
        </p>
        <p>
          (i) <m>A</m> is orthogonal
        </p>
        <p>
          (ii) <m>\norm{Ax}=\norm{A}</m> for all <m>x\in \R^n</m>.
        </p>
        <p>
          (iii) <m>\norm{Ax-Ay}=\norm{x-y}</m> for all <m>x,y\in \R^n</m>.
        </p>
        <p>
          (iv) <m>Ax\cdot Ay = (Ay)^TAx=x\cdot y</m>.
        </p>
      </statement>
    </theorem>
      
    </p>
  </subsection>
  
  <subsection>
    <title>QR-Factorization</title>
    <p>
      Let <m>A</m> be an <m>m\times n</m> matrix whose columns are <m>a_1,\ldots, a_n</m>.
      Further assume that columns of <m>A</m> are linearly independent.
      Using the Gram-Schmidt orthogonalization process
      <md>
        <mrow>u_1:\amp = a_1</mrow>
        <mrow>u_2:\amp = a_2 -\frac{a_2\cdot u_1}{\norm{u_1}^2}u_1 </mrow>
        <mrow>  \vdots\amp</mrow>
        <mrow>u_k: \amp=a_k-\frac{a_k\cdot u_1}{\norm{u_1}^2}u_1-\cdots -
          \frac{a_k\cdot u_{k-1}}{\norm{u_{k-1}}^2}u_{k-1}</mrow>
        <mrow>   \vdots\amp </mrow>
      </md>

      and  define <m>q_k:= \frac{u_k}{\norm{u_k}}, k=1,\ldots, n</m>.
      This implies
      <md>
        <mrow>\norm{u_k}q_k \amp =  u_k = a_k-\frac{a_k\cdot u_1}{\norm{u_1}^2}u_1-\cdots -\frac{a_k\cdot u_{k-1}}{\norm{u_{k-1}}^2}u_{k-1}</mrow>
        <mrow>\amp =a_k-(a_k\cdot q_1)q_1-(a_k\cdot q_2)q_2+\cdots - (a_k\cdot q_{k-1})q_{k-1}</mrow>
      </md>.
    </p>
    <p>
      Hence
      <me>
        a_k = (a_k\cdot q_1)q_1+(a_k\cdot q_2)q_2+\cdots + (a_k\cdot q_{k-1})q_{k-1}+\norm{u_k}q_k, k=1,2,\ldots, n
      </me>.
    </p>
    <p>
      Thus we have
      <md>
        <mrow>a_1: \amp = \norm{u_1}q_1</mrow>
        <mrow>a_2: \amp = (a_2\cdot q_1)q_1+\norm{u_2}q_2</mrow>
        <mrow>\vdots \amp  \vdots</mrow>
        <mrow>a_k: \amp = (a_k\cdot q_1)q_1+(a_k\cdot q_2)q_2+\cdots + (a_k\cdot q_{k-1})q_{k-1}+\norm{u_k}q_k</mrow>
        <mrow>\vdots \vdots</mrow>
      </md>
    </p>
    <p>
      Thus
      <md>
        <mrow>A \amp = [a_1~a_2~a_3~\cdots~a_n]</mrow>
        <mrow>\amp =  [q_1~q_2~q_3~\cdots~q_n] 
          \begin{bmatrix}\norm{u_1} \amp  a_2\cdot q_1 \amp   a_3\cdot q_1\amp  \cdots \amp  a_n\cdot q_1\\ 
          0\amp   \norm{u_2} \amp  a_3\cdot q_2 \amp   \cdots \amp  a_n\cdot q_2\\
          0\amp 0   \amp  \norm{u_3}\amp   \cdots \amp  a_n\cdot q_3\\ 
          \vdots \amp  \vdots \amp  \vdots \amp  \ddots \amp  \vdots\\ 0\amp  0 \amp  0 \amp  \cdots \amp  \norm{u_n} \end{bmatrix}</mrow>
        <mrow>\amp =QR</mrow>
      </md>
    </p>
    <p>
      Here <m>Q</m> has orthogonal columns and <m>R</m> is upper triangular whose diagonal entries which are positive,
      hence non-singular.
      This is what is known as <m>QR</m> factorization.
      Thus we have the following result.
    </p>
    <theorem>
      <statement>
        <p>
          Every <m>m\times n</m> matrix with linearly independent columns has a <m>QR</m> factorization,
          <m>A=QR</m>,
          where columns of <m>Q</m> are orthonormal and <m>R</m> is an upper triangular matrix with positive diagonal entries.
        </p>
      </statement>
    </theorem>
    <example xml:id="QR-eg1">
      <statement>
        <p>
          Let <m>A = \begin{pmatrix}0 \amp  1 \amp  1\\ 1 \amp  1 \amp  -2\\ 1 \amp  1 \amp  2 \end{pmatrix} = \begin{bmatrix}a_1 \amp  a_2 \amp  a_3 \end{bmatrix}</m>.
          Let us find the <m>QR</m> factorization of <m>A</m>.
          Note that columns of <m>A</m> are vectors
          <m>v_1, v_2, v_3</m> in the <xref ref="gram-schmidt-eg1">Example</xref>.
          We have found <m>q_1, q_2, q_3</m> in this example.
          Hence
          <me>
            Q = \begin{bmatrix}q_1 \amp  q_2 \amp  q_3 \end{bmatrix}  = \begin{pmatrix}0            \amp  1 \amp  0 \\ \frac{1}{\sqrt{2}} \amp  0 \amp  \frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \amp   0 \amp  \frac{1}{\sqrt{2}} \end{pmatrix}
          </me>.
          We also have
          <me>
            u_1= (0, 1, 1), u_2=(1,0,0), u_3=(0,-2,2)
          </me>
        </p>
        <p>
          Hence
          <me>
            R = \begin{pmatrix}\norm{u_1} \amp  a_2 \cdot q_1 \amp  a_3 \cdot q_1\\ 0 \amp  \norm{u_2} \amp  a_3 \cdot q_2\\ 0  \amp  0 \amp  \norm{u_3} \end{pmatrix}  = \begin{pmatrix}\sqrt{2} \amp  \sqrt(2) \amp  0\\ 0 \amp  1 \amp  1 \\ 0 \amp   0 \amp  2\sqrt{2} \end{pmatrix}
          </me>
        </p>
        <p>
          Note that once we have <m>Q</m>, then <m>R = Q^TA</m>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Find the QR-factorization of <m>A=\left(\begin{array}{rrr} -1 \amp  1 \amp  -1 \\ 1 \amp  -1 \amp  0 \\ 0 \amp  0 \amp  2 \\ 1 \amp  1 \amp  1 \end{array} \right)=\begin{bmatrix}a_1\amp  a_2\amp  a_3 \end{bmatrix}</m>.
          It is easy to check that columns of <m>A</m> are linearly independent.
          In fact, columns of <m>A</m> are rows of the matrix defined in the Example<nbsp/><xref ref="gram-schmidt-eg2"/>.
          From this example, we have
          <me>
            u_1 = \begin{pmatrix}-1\\ 1\\ 0\\ 1 \end{pmatrix}, 
            q_1 = \begin{pmatrix}-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{3}}\\0\\\frac{1}{\sqrt{3}} \end{pmatrix}, 
            u_2 = \begin{pmatrix}2/3\\ -2/3\\ 0\\ 4/3 \end{pmatrix},
          </me>
          <me> q_2 = \begin{pmatrix}\frac{1}{\sqrt{6}}\\ -\frac{1}{\sqrt{6}}\\0\\\sqrt{\frac{2}{3}} \end{pmatrix}, 
            u_3 = \begin{pmatrix}-1/2\\-1/2\\2\\0 \end{pmatrix}, 
            q_3 = \begin{pmatrix}-\frac{1}{3} \, \sqrt{\frac{1}{2}}\\-\frac{1}{3} \, \sqrt{\frac{1}{2}}\\\frac{4}{3} \, \sqrt{\frac{1}{2}}\\0 \end{pmatrix}
          </me>
        </p>
        <p>
          Hence
          <me>
            Q = [q_1~q_2~q_3]=\left(\begin{array}{rrr} -\frac{1}{3} \, \sqrt{3} \amp  \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp  -\frac{1}{3} \, \sqrt{\frac{1}{2}} \\ \frac{1}{3} \, \sqrt{3} \amp  -\frac{1}{2} \, \sqrt{\frac{2}{3}} \amp  -\frac{1}{3} \, \sqrt{\frac{1}{2}} \\ 0 \amp  0 \amp  \frac{4}{3} \, \sqrt{\frac{1}{2}} \\ \frac{1}{3} \, \sqrt{3} \amp  \sqrt{\frac{2}{3}} \amp  0 \end{array} \right)
          </me>.
          Also
          <me>
            R = Q^TA=\left(\begin{array}{rrr} \sqrt{3} \amp  -\frac{1}{3} \, \sqrt{3} \amp  \frac{2}{3} \, \sqrt{3} \\ 0 \amp  2 \, \sqrt{\frac{2}{3}} \amp  \frac{1}{2} \, \sqrt{\frac{2}{3}} \\ 0 \amp  0 \amp  3 \, \sqrt{\frac{1}{2}} \end{array} \right)
          </me>.
        </p>
      </statement>
    </example>
    <remark>
      <p>
        If a matrix <m>A</m> has independent rows,
        then we apply <m>QR</m> factorization to <m>A^T</m>.
        Thus
        <me>
          A^T = (QR)^T=R^TQ^T=LP
        </me>
        where <m>L</m> is the the invertible lower triangular matrix with positive diagonal entries and and <m>P</m> has orthogonal rows.
      </p>
    </remark>
    <remark>
      <p>
        In case a matrix <m>A</m> has linearly independent columns then the <m>QR</m> factorization is unique.
        That is, if <m>A= Q_1R_1=Q_2R_2</m>,
        then <m>Q_1=Q_2</m> and <m>R_1=R_2</m>.
      </p>
    </remark>
  <p>
    
  </p>
    <exercise>
      <statement>
        <p>
          Find the QR-factorization of the following matrices:
          <me>
            \begin{bmatrix}2 \amp  1 \\ 1 \amp  1  \end{bmatrix} ,  
            \begin{bmatrix}1 \amp  -1 \amp  1\\ 2 \amp  0  \amp  1\\2 \amp  1 \amp  -2 \end{bmatrix} , 
            \begin{bmatrix}1 \amp  1 \amp  0 \\-1 \amp  0 \amp 1\\0 \amp  1 \amp  1\\1 \amp  -1 \amp  0 \end{bmatrix}
          </me>
        </p>
      </statement>
    </exercise>
  
  </subsection>


</section>

-->
