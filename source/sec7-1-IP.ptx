<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec7-1-InnerProduct" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Inner Product</title>


  <p>
      In the last chapter,  we  dealt with notion of dot product and geometry in <m>\R^n</m>.
      The dot product and related notion can be generalized to an arbitrary vector space over 
      <m>\R</m> or <m>\mathbb{C}</m>.
      All the notions, we have learned in the last section can be generalized over an inner product space. 
      In this chapter we, shall introduce inner product on a vector space <m>V</m> over <m>\mathbb{R}</m> and 
      define extend the results studied in the last chapter on <m>V</m>.
    </p>

<p>
Note that the dot product of two vectors in <m>\R^n</m> is a scalar,
      in particular, dot product can be thought of as a function from
      <m>`\cdot' \colon \R^n\times \R^n \to\R</m> satisfying the following properties:
</p>
<p>
      <ol>
        <li>
          <p>
            <m>x \cdot x\geq 0</m> for all <m>x\in \R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x \cdot x= 0</m> if and only if <m>x=0</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x \cdot y=y\cdot x</m> for all <m>x,y \in \R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x\cdot (y+z) = x\cdot y +x\cdot z</m> for all <m>x,y,z\in \R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x\cdot (\alpha y)=\alpha x\cdot y = (\alpha x)\cdot y</m>.
          </p>
        </li>
      </ol>
    </p>

    <p>
      The notion of dot product on <m>\R^n</m> can ve generalized on vector space known as inner product.
      We have the following definition.
    </p>

    <definition xml:id="def-inner-product"><title>Inner Product</title>
       <statement>
        <p>
          Let <m>V</m> be a vector space over <m>\R</m>.
          An inner product on <m>V</m> is a function that assigns a real number
          <m>\langle x, y\rangle</m> to every pair <m>x,y</m> of vectors in <m>V</m> 
          (that is, a function <m>\langle \cdot, \cdot\rangle\colon V \times V \to \R</m>) satisfying the following properties. 
        </p>

          <p>
          <ol>
            <li>
            <p>
                <m>\langle x, x\rangle \geq 0</m> for all <m>x \in V</m> and
                <m>\langle x, x\rangle = 0</m> if and only if <m>x=0</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\langle x, y\rangle = \langle y, x\rangle</m> for all <m>x,y \in V</m>. (Symmetry)
              </p>
            </li>
            <li>   
              <p>
                <m>\langle x, (y+z)\rangle=\langle x, y\rangle+\langle x, z\rangle</m> for all <m>x,y,z\in V</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\langle x, (\alpha y)\rangle=\alpha\langle x, y\rangle</m> for all
                <m>x,y\in V</m> and <m>\alpha \in \R</m>.   
              </p>
            </li>
          </ol>
        </p>

          <p>
            If <m>V</m> is real vector space with inner product <m>\langle. , .\rangle</m>.
Then <m>(V, \langle . , .\rangle)</m> called in inner product space over <m>\R</m>.
          </p>
      </statement>
    </definition>



    <p>
      The last two properties make the inner product linear in the second variable.
      Using the symmetry property,
      it can also be shown that the inner product is linear in the first variable as well.
      That is,
      <me>
        \langle (x+y),z\rangle=\langle x, z\rangle+\langle y, z\rangle, \text{ and } \langle (\alpha x) y\rangle=\alpha\langle x, y\rangle
      </me>
    </p>

      <p>
        Next we look at several examples of inner procuct on various vector spaces that we have defined in 
        <xref ref="chap4-Vector-Space"/>.
      </p>



    <example xml:id="dot-product-as-IP">
      <statement>
        <p>
          On <m>\R^n</m>, the standard dot product is an inner product.
          Thus define
          <me>
            \langle x,  y\rangle:=x\cdot y
          </me>
        </p>
        <p>
          This is also called the Euclidean inner product on <m>\R^n</m>.
        </p>
      </statement>
    </example>

    <example xml:id="inner-product-on-matrixspace">
      <statement>
        <p>
          Let <m>V=M_n(\R)</m>, the set of all
          <m>n\times n</m> matrices over <m>\R</m>.
          Define
          <me>
            \langle A,  B\rangle:=tr(AB^T)
          </me>
        </p>

        <p>
          It is easy to show that this is an inner product on <m>M_n(\R)</m>.
        </p>

        <p>
          Note that this inner product can be thought of as the standard dot product on <m>\R^{n^2}</m>. 
          The elements of the matrix <m>A</m> can be thought of as a vector in <m>\R^{n^2}</m>. Then 
          <me>
            tr(AB^T) =\sum\sum a_{ij}b_{ij}\quad \quad  \text{ why?}
          </me>.
          Work with  <m>2\times 2</m> matrices and then try to prove this for <m>n\times n</m> matrices.
        </p>
      </statement>
    </example>
<sage>
  <input>
  def inpMnR(A,B):
      return (A*B.T).trace()
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
A = matrix(QQ,[[2,-1,0],[1,4,2],[7,9,2]])
B = matrix(QQ,[[3,2,1],[1,2,0],[5,3,1]])  
inpMnR(A,B)
  </input>
  <output>
    
  </output>
</sage>
    <example xml:id="inner-product-by-matrix">
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> symmetric and positive definite matrix.
          On <m>\R^n</m>, define
          <me>
            \langle x,  y \rangle := x\cdot Ay = x^TAy
          </me>
        </p>
        <p>
          Then this is an inner product on <m>\R^n</m>.  
          (Where do we need <m>A</m> to be positive definie?)
         
        </p>
      </statement>

    </example>

  <remark>
     <statement>
      <p>
         Since <m>A</m> is symmetric and positive definite matrix, there exists a matrix <m>B</m>  
      such that <m>B^2=A</m>. We call 
    <m>B</m> as positive definite square root of <m>A</m> and is denoted by <m>A^{1/2}</m>.  
    
      </p>
     </statement>
      <me>
        \langle x,y\rangle = x^T A y = (A^{1/2}x)\cdot(A^{1/2}y),
      </me>
      which shows this inner product is just the usual dot product after the linear change of 
      variables <m>x\mapsto A^{1/2}x</m>.
    </remark>
<sage>
  <input>
def inner_product_PD(M,u,v):
    # Check if M is square
    if M.nrows() != M.ncols():
        raise ValueError("Matrix must be square.")
    # Check symmetry
    if not M.is_symmetric():
        raise ValueError("Matrix must be symmetric for a real inner product.")
    eigvals = M.eigenvalues()
    if not all(e > 0 for e in eigvals):
        raise ValueError("Matrix is not positive definite.")
    
    if len(u) != M.nrows() or len(v) != M.nrows():
        raise ValueError("Vectors must have same dimension as M.")
    
    return (u * M * v)  # Equivalent to u^T M v
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
A=matrix([[2,-1,0],[-1,2,-1],[0,-1,2]])
e1=vector([1,0,0])
e2=vector([0,1,0])
inner_product_PD(A,e1,e2)

  </input>
  <output>
    
  </output>
</sage>
<example xml:id="inner-product-C01">
      <statement>
        <p>
          Let <m>V={\cal C}([0,1])</m> the set of all continuous function from <m>[0,1]</m> to <m>\R</m>.
          Define
          <me>
            \langle f,  g \rangle:=\int_0^1 f(t)g(t)dt
          </me>.
        </p>
        <p>
          This is an inner product on <m>V</m>.  (You may need real analysis to conclude that <m>\int_0^1 f(t)f(t)dt=0</m> then <m>f=0</m>.)
        </p>
      </statement>
    </example>
<sage>
  <input>
## inner product on C[0,1]
def inp(f,g):
    return integral(f(x)*g(x),x,0,1)
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
f(x)=x+1
g(x)=x^2
inp(f,g)
  </input>
  <output>
    
  </output>
</sage>

<sage>
  <input>
## Norm on C[0,1] w.r.t. to the above norm
def fnorm(f,inp):
    return sqrt(inp(f,f))  
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
g(x)=x^3-3*x+2
fnorm(g,inp)
  </input>
  <output>
    
  </output>
</sage>
<example xml:id="inner-product-on-polyspace">
      <statement>
        <p>
          Let <m>p</m> and <m>q</m> be two polynomials in <m>{\cal P}_n(\mathbb{R})</m>.
          Then define
          <me>
            \langle p,q \rangle := p(0)q(0)+p(1)q(1)+\cdots +p(n)q(n)
          </me>.
        </p>

        <p>
          It is easy to see that <m>\langle p,q \rangle</m> defined inner product on the vector space <m>{\cal P}_n(R)</m>.
        </p>

        <p>
          Here <m>0, 1, 2, \ldots, n</m> are nothing special.
          Instead, we can use any <m>n+1</m> distinct real numbers,
          <m>c_0,c_1,\ldots, c_n</m>.
        </p>
      </statement>
<p>
      <hint>
        <p>
        <m>\langle p,p \rangle =</m> mean that <m>p</m> has  <m>n+1</m> roots, which is not possible unless 
        <m>p=0</m> by the fundamental theorem of algebra.
        </p>
      </hint> 
    </p>    
  </example>

  <sage>
    <input> 
def inpPol(f,g,n):
    a = sum([f(x=i)*g(x=i) for i in range(n+1)])
    return a
    </input>
    <output>
      
    </output>
  </sage>

  <sage>
    <input>
f(x) = 3*x^3+4*x^2+2*x+1
g(x) = -2*x^3+x^2+2*x-1
inpPol(f,f,3)
    </input>
    <output>
      
    </output>
  </sage>
    <definition xml:id="definition-norm">
      <statement>
        <p>
          Let <m>(V, \langle .\rangle)</m> be a real inner product space.
          Then norm of any vector <m>x\in V</m> corresponding to the inner product <m>\langle . \rangle</m> is defined as
          <me>
            \norm{x}=\sqrt{\langle x, x\rangle}
          </me>.
          The distance between twp vectors <m>x</m> and <m>y</m> is defined as
          <me>
            d(x,y):=\norm{x-y}
          </me>.
        </p>
      </statement>
    </definition>



    <exercise xml:id="inp-7-1-8">
      <statement>
        <p>
          Let <m>V</m> be an inner product space.
          Then for any two vectors <m> x, y\in V</m>, show that
          <p> 1. 
          <m>\norm{x+y}^2=\norm{x}^2+\norm{y}^2+2\inner{x}{y}</m>
          </p>
          <p> 2. 
          <m> \norm{x-y}^2=\norm{x}^2+\norm{y}^2-2\inner{x}{y}</m>
          </p>
        </p>
      </statement>
      <hint>
        <p>
          The proof follows by looking at <m>\langle x+y,x+y \rangle </m>  and 
          <m>\langle x-y,x-y \rangle </m> and using the properties 
          of inner product. 
        </p>
      </hint>
    </exercise>

    <exercise xml:id="inp-7-1-9">
      <statement>
        <p>
          If <m>x, y</m> are two vectors in an inner product space <m>V</m> with inner product <m>\langle .\rangle</m>.
          Then show that
          <me>
            \norm{x+y}^2+\norm{x-y}^2=2(\norm{x}^2+\norm{y}^2)
          </me>.
        </p>
        <p>
          This is called the <alert>parallelogram identity.</alert>
          Geometrically, in a parallelogram,
          the sum of square of the diagonals is twice the sum of the squares of the side lengths.
        </p>
      </statement>
      <hint>
        <p>
          Proof follows from <xref ref="inp-7-1-8"/>.
        </p>
      </hint>
    </exercise>

    <exercise>
    <statement>
      <p>
        Let <m>a,b,c</m> be three positive real numbers and ,<m>x=(x_1,x_2,x_3), 
          y=(y_1,y_2,y_3)</m> be vectors in <m>\R^3</m>. Define
          <me>
          \langle x, y\rangle:=\frac{x_1y_1}{a^2}+\frac{x_2y_2}{b^2}+\frac{x_3y_3}{c^2}.
          </me>
         Show that this is an inner product on <m>\R^3</m>. Also what are the unit vectors with 
         respect to this inner product on <m>\R^3</m>?  
      </p>
    </statement>  
    </exercise>
    
    <theorem xml:id="thm-Cauchy-Schwarz-Inequality">
      <title>Cauchy-Schwarz Inequality</title>
      <statement>
        <p>
          Let <m>V</m> be an inner product space.
          The for any two vectors <m>x,y\in V</m>, we have
          <men xml:id="cauchy-swarch-ineq">
            |\inner{x}{y}|\leq \norm{x}\norm{y}
          </men>
        </p>
        <p>
          The equality holds if and only if <m>x</m> and <m>y</m> are linearly dependent.
        </p>
      </statement>

      <proof>
  <p>
    If <m>x = 0</m> or <m>y = 0</m>, then <m>\innprod{x}{y} = 0</m>. Also either
    <m>\innprod{x}{x} = 0</m> or <m>\innprod{y}{y} = 0</m>. Hence the result follows.
  </p>

  <p>
    First we consider the case when <m>\norm{x} = \norm{y} = 1</m> . 
    Then
   <md>
    <mrow>0 \leq  \amp\innprod{x - y}{x - y}</mrow>
    <mrow>= \amp\innprod{x}{x} + \innprod{y}{y} - 2\innprod{x}{y}</mrow>
    <mrow>= \amp 2 - 2\innprod{x}{y}</mrow>
  </md>

  This gives <m>\innprod{x}{y} \leq 1=\norm{x}\norm{y}.</m></p>
  <p>
    Similarly, 
    <md>
    <mrow>0 \leq  \amp\innprod{x + y}{x + y}</mrow>
    <mrow>= \amp\innprod{x}{x} + \innprod{y}{y} + 2\innprod{x}{y}</mrow>
    <mrow>= \amp 2 + 2\innprod{x}{y}.</mrow>
  </md>

   Thus  <m>\innprod{x + y}{x + y} \geq 0</m> implies
    <m>-\innprod{x}{y} \leq 1</m>.   From these two together we have 

  </p>

  <me>
  |\innprod{x}{y}|\leq 1 = \norm{x} \norm{y}.
  </me>

    <p>
    We now prove the statement for equality.
    Let <m>\mod{\innprod{x}{y}} = 1.</m> This implies either
    <m>\innprod{x}{y} = 1</m> or <m>\innprod{x}{y} = -1.</m>
    If <m>\innprod{x}{y} = 1</m>, from the above chain of inequalities we deduce
    that <m>\innprod{x - y}{x - y} = 0</m> or <m>x = y.</m>
    If <m>\innprod{x}{y} = -1</m>, we see that <m>x = -y.</m>
    Thus the equality holds if and only if <m>x = \pm y.</m>
  </p>

  <p>
    Next suppose <m>x</m> and <m>y</m> are nonzero (not necessarily of unit
    length). Then we can write <m>u = \frac{x}{\norm{x}}</m> and
    <m>v = \frac{y}{\norm{y}}</m> are of unit length. Hence by the previous case,
    <m>\mod{\innprod{u}{v}} \leq 1.</m> This implies,
    <me>
      \mod{\innprod{\frac{x}{\norm{x}}}{\frac{y}{\norm{y}}}} =
      \mod{\frac{1}{\norm{x}} \frac{1}{\norm{y}} \innprod{x}{y}} \leq 1.
  </me>
  thus we get <m>\mod{\innprod{x}{y}} \leq \norm{x} \norm{y}.</m>
  </p>

  <p>
    If <m>x</m> and <m>y</m> are nonzero, then the equality  from the earlier case means
    <m>\innprod{x}{y} = \norm{x} \norm{y}</m> or
    <m>-\innprod{x}{y} = \norm{x} \norm{y}</m>.  Let us look at one of the case.

  <md>
    <mrow>\amp \innprod{x}{y} =  \norm{x} \norm{y}</mrow>
    <mrow>\amp \Longleftrightarrow \innprod{\frac{x}{\norm{x}}}{\frac{y}{\norm{y}}} = 1 </mrow>
    <mrow>\amp \Longleftrightarrow \frac{x}{\norm{x}} = \frac{y}{\norm{y}}</mrow>
    <mrow>\amp \Longleftrightarrow x  = \frac{\norm{x}}{\norm{y}} y.</mrow>
    </md>

  The other case is similar. 
    </p>

  <p>
   <em>2nd Proof.</em> 
  </p>  

  <p>
    Fix <m>x</m> and <m>y</m> in <m>V</m>. If <m>y = 0</m>, then the result is
    obviously true. Without loss of generality  assume that <m>y \neq 0</m>. Consider the real-valued
    function <m>\varphi\colon \R \to \R</m>
   defined as    <me>\varphi(t) := \innprod{x + t y}{x + t y}</me>. 
   We shall use calclus and investigate the minimum value of <m>\varphi(t)</m> to prove the inequality. 
  <md>
    <mrow>\varphi(t) \amp= \amp\innprod{x + t y}{x + t y}</mrow>
    <mrow>\amp= \amp\innprod{x}{x} + 2 t \innprod{x}{y} + t^{2} \innprod{y}{y}.</mrow>
  </md>
Note that this is a differentiabke function of <m>t</m>. Differentiating <m>\varphi(t)</m> 
with respect to <m>t</m>, we get

  <me>
    \varphi'(t) = 2 \innprod{x}{y} + 2 t \innprod{y}{y}.
  </me>
  Setting <m>\varphi'(t) = 0</m>, we get the critical point at
  <me>
    t = -\frac{\innprod{x}{y}}{\innprod{y}{y}}.
  </me>
  Since <m>\varphi''(t) =\innprod{y}{y} \gt 0</m>, this critical point is a minimum. Thus
  <me>
    \varphi\left(-\frac{\innprod{x}{y}}{\innprod{y}{y}}\right) \geq 0.
  </me>
  Substituting for <m>t</m> in the expression for <m>\varphi(t)</m>, we get
  <md>
    <mrow>0 \amp\leq \innprod{x}{x} - \frac{\innprod{x}{y}^{2}}{\innprod{y}{y}}</mrow>
    <mrow>\Longleftrightarrow \innprod{x}{y}^{2} \amp\leq \innprod{x}{x} \innprod{y}{y}</mrow>
    <mrow>\Longleftrightarrow |\innprod{x}{y}| \amp\leq \norm{x} \norm{y}.</mrow>
  </md>
  This completes the 2nd proof of the Cauchy-Schwarz inequality.
  </p>
</proof>
    </theorem>

    <theorem xml:id="thm-Triangle-Inequality">
      <title>Triangle Inequality</title>
      <statement>
        <p>
          Let <m>x</m> and <m>y</m> be two vectors in an inner product space <m>V</m>.
          Then
          <md>
            <mrow>\norm{x+y}^2 \amp = \norm{x+y}^2=\norm{x}^2+\norm{y}^2+2\inner{x}{y}</mrow>
            <mrow>\amp \leq \norm{x}^2+\norm{y}^2+2|\inner{x}{y}|  \text{ \(a\leq |a|\) }</mrow>
            <mrow>\amp \leq  \norm{x}^2+\norm{y}^2+2\norm{x}\norm{y}  \text{ by the Cauchy-Schwarz inequality }</mrow>
            <mrow>\amp =  (\norm{x}+\norm{y})^2</mrow>
          </md>
        </p>
        <p>
          Hence for all <m>x,y</m>, we have 
          <md>
            <mrow xml:id="trangle_ineq" number="yes">\norm{x+y}\leq \norm{x}+\norm{y}</mrow>
          </md>
          is called the triangle inequality.
        </p>
      </statement>
    </theorem>

    <p>
      Let us play with the Cauchy-Schwarz inequality<nbsp/><xref ref="cauchy-swarch-ineq"/>.
      Suppose <m>x</m> and <m>y</m> are non zero vectors in <m>V</m>, Then
      <me>
        |\inner{x}{y}|\leq \norm{x}\norm{y}\Rightarrow \frac{|\inner{x}{y}|}{\norm{x}\norm{y}}\leq 1
      </me>
    </p>

    <p>
      Hence we have
      <me>
        -1\leq \frac{\inner{x}{y}}{\norm{x}\norm{y}}\leq 1
      </me>.
    </p>

    <p>
      Thus for any two non zero vectors, <m>x</m> and <m>y</m>,
      <m>\frac{\inner{x}{y}}{\norm{x}\norm{y}}</m> always lies between <m>-1</m> and 1.
      This allows us to define the angle between two non zero vectors.
      We assign this number to <m>\cos\theta</m> with
      <m>\theta\in[-\pi,\pi]</m> called the angle between <m>x</m> and <m>y</m>.
      Thus, if <m>\theta</m> is the angle between <m>x</m> and <m>y</m>,
      then we have
      <me>
        \cos \theta = \frac{\inner{x}{y}}{\norm{x}\norm{y}}
      </me>
    </p>

    <p>
      All the notions that we defined for dot product, namely,
      orthogonality,
      orthogonal projection, Gram-Schmidt orthogonalization process can we defined in a similar manner.
      All we need to do is,
      replace the dot product by the given inner product.
    </p>

<theorem xml:id="thm-norm-properties"> <title>Properties of Norm</title>
  <statement>
    <p>
      Let <m>(V,\langle ., .\rangle)</m> be an innepr product space. The norm defined as <xref ref="definition-norm"/> has the following properties:
    </p>
    <p>
      <ol>
        <li>
          <p>
            for all <m>x\in V</m>, <m>\norm{x}\geq 0</m> and <m>\norm{x}= 0</m> if and only if <m>x=0</m>.
          </p>
        </li>
        <li>
          <p>
            for all <m>\alpha \in \R</m> and <m>x\in V</m>, <m>\norm{\alpha x}=|\alpha|\norm{x}</m>.
          </p>
        </li>
        <li>
          <p>
            for all <m>x,y \in V</m>, <m>\norm{x+y}\leq \norm{x}+\norm{y}</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>

<definition xml:id="def-normed-lin-space">
  <statement>
    <p>
      Any vector space <m>V</m> over <m>\R</m> with a function <m>\norm{.} : V \to \R</m> which satisfies all the 
      properties mentioned in <xref ref="thm-norm-properties"/> is called a <alert>normed linear space.</alert>. Thus 
      any inner product space is also a normed linear space.
    </p>
  </statement>
</definition>

    <note>
      <statement>
        <p>
          The concepts such as orthogonality, orthogonal projection, orthogonal complement of 
any subset, orthogonal and orthonormal sets and Gram-Schmidt orthogonalization process etc that we 
defined and dealt with in the previuos chapter with respect to the dot product on <m>\R^n</m> can be 
defined on an inner product space. All we need to do is to replace the dot product by the corresponding 
inner product.  We encourage  readers to define each one of them. 

        </p>
      </statement>
  </note>

    <exercise xml:id="inp-7-1-10">
      <statement>
        <p>
          Let <m>(V, \inner{.}{.})</m> be a real inner product space.
          Define 
          <ol>
            <li>
              <p>
                orthogonality of two vectors <m>x</m>  and <m>y</m> in <m>V</m>,
              </p>
            </li>
            <li>
              <p>
                orthogonal complement of a subset <m>U</m> of <m>V</m>,
              </p>
            </li>
            <li>
              <p>
                Orthogonal projection of a vector <m>v</m> onto a non-zero 
          vector <m>u</m>,
              </p>
            </li>
            <li>
              <p>
                orthogonal set and orthonormal sets in <m>V</m> and 
              </p>
            </li>
            <li>
              <p>
                Gram-Schmidt orthogonalization process.
              </p>
            </li>
          </ol>   
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-11">
      <statement>
        <p>
          Let <m>x, y</m> be two vectors in an inner product space <m>V</m>.
          Then show that
       <ol>
        <li>
          <p>
            <m>x</m> and <m>y</m> are orthogonal if and only if
          <m>\norm{x+y}=\norm{x-y}</m>. (what does it mean geometrically?)
          </p>
        </li>
        <li>
          <p>
            <m>x+y</m> and <m>x-y</m> are orthogonal if and only if <m>\norm{x}=\norm{y}</m>.
          </p>
        </li>
       </ol>        
          
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-12">
      <statement>
        <p>
          Let <m>C([-\pi,\pi])</m> be the vectors space of set of continuous functions from <m>[-\pi,\pi]</m> to <m>\R</m>.
          Define the inner product on <m>C([-\pi,\pi])</m> as
          <me>
            \inner{f}{g}:=\int_{-\pi}^\pi f(t) g(t)`dt
          </me>.
        </p>
        <p>
          Show that under this inner product
          <m>\{1, \sin nx,\cos mx\}</m> is an orthogonal set.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-13">
      <title>Pythagoras Theorem</title>
      <statement>
        <p>
          Let <m>(V, \inner{.}{.})</m> be a real inner product space.
          Let <m>x_1, x_2, \ldots x_n</m> be <m>n</m> orthogonal vectors in <m>V</m>.
          Then
          <me>
            \norm{x_1+x_2+\cdots+x_n}^2=\norm{x_1}^2+\norm{x_2}^2+\cdots+\norm{x_n}^2
          </me>.
        </p>
        <p>
          This is called the Pythagoras theorem.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-14">
      <statement>
        <p>
          Let <m>\beta=\{u_1,\ldots,
          u_n\}</m> be an orthogonal basis of an inner product space <m>V</m>.
          Let <m>v\in V</m> and <m>\theta_1,\ldots, \theta_n</m> be the angle between <m>v</m> and <m>u_1,\ldots, u_n</m>,  respectively.
          Then
          <me>
            \cos\theta_1^2+\cdots+\cos\theta_n^2=1
          </me>.
        </p>
        <p>
          Here <m>\cos\theta_i</m> are called the direction cosines of <m>v</m> corresponding to <m>\beta</m>.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-15">
      <statement>
        <p>
          Let <m>\beta=\{u_1,\ldots,
          u_n\}</m> be an orthogonal basis of an inner product space <m>V</m>.
          Let <m>x</m> and <m>y</m> be two vectors such that
          <m>x=\sum x_i u_i</m> and <m>y=\sum y_i u_i</m>.
          Then
          <me>
            \inner{x}{y}=x_1y_1+x_2y_2+\cdots+x_ny_n
          </me>
          and
          <me>
            \norm{x}^2=x_1^2+x_2^2+\cdots +x_n^2
          </me>.
        </p>
      </statement>
    </exercise>

    <example xml:id="Legendre-poly">
        <title>Legendre Polynomials</title>
      <statement>
        <p>
          Consider <m>V ={\cal P}_3(\R)</m> with inner product <m>\inner{p}{q}:=\int_{-1}^1 p(x)q(x)\,dx</m>.
          Use the standard basis <m>\beta =\{v_1,v_2,v_3,v_4\} = \{1,x,x^2,x^3\}</m> to find an orthogonal basis of <m>{\cal P}_3(\R)</m>.
        </p>
        <p>
          First of all notice that <m>\beta</m> is not an orthogonal basis.
          For <m>\inner{v_1}{v_3}=\inner{1}{x^2} = \int_{-1}^1 x^2 dx = \frac23</m>,
          <m>\inner{v_2}{v_4}=\int_{-1}^1 x^4 dx = \frac25</m>.
          Also note that <m>\inner{v_1}{v_2}=\int_{-1}^1 xdx = 0</m>.
          <m>\inner{v_2}{v_3}=\int_{-1}^1 x^3dx = 0</m>.
          <m>\inner{v_1}{v_4}=\int_{-1}^1 x^3 dx = 0</m>. <m>\inner{v_3}{v_4}=\int_{-1}^1 x^5dx = 0</m>.
        </p>
        <p>
          Since <m>v_1</m> and <m>v_2</m> are already orthogonal,
          we can choose <m>u_1=v_1=1</m> and
          <m>u_2 = v_2=x</m> in the Gram-Schmidt process.
          Next
          <me>
            u_3 = v_3-\inner{v_3}{u_1}/\norm{u_1}^2u_1-\inner{v_3}{u_2}/\norm{u_2}^2u_2x^2
          </me>.
        </p>
        <p>
          We have
          <me>
            \inner{v_3}{u_1}= \int_{-1}^1 x^2 dx = \frac23, \inner{u_1}{u_1}=\int_{-1}^1 1dx = 2
          </me>.
        </p>
        <p>
          Hence
          <me>
            u_3 = x^2 - \frac{\frac23}{2} \times 1 = x^2-\frac13
          </me>.
        </p>
        <p>
          Next
          <md>
            <mrow>u_4 \amp =  u_4-\inner{v_4}{u_1}/\norm{u_1}^1u_1-\inner{v_4}{u_2}/\norm{u_2}^1u_2-\inner{v_4}{u_3}/\norm{u_3}^1u_3</mrow>
            <mrow>\amp =v_4-\inner{v_4}{u_2}/\norm{u_2}^1u_2-\inner{v_4}{u_3}/\norm{u_3}^1u_3</mrow>
            <mrow>\amp = x^3-\frac35 x</mrow>
          </md>.
        </p>
        <p>
          Hence an orthogonal basis is <m>\beta'=\{1,x,x^2-\frac{1}{3}, x^3-\frac35 x \}</m>.
          These are the first four <em>Legendre polynomials.</em>
        </p>
        <p>
          After normalizing the vectors,
          we get an orthonormal basis
          <me>
            \beta''=\left\{\frac{\sqrt{2}}{2}, \frac{\sqrt{6}}{2}x, \frac{3\sqrt{10}}{4}\left(x^2-\frac13\right), \frac{5\sqrt{14}}{4}\left(x^3-\frac35 x\right)\right\}
          </me>.
        </p>
        <figure xml:id="fig_legendrepoly" halign="center">
          <caption halign="center">Graph of Legendre polynomials</caption>
          <image width="50%" source="images/Legendre_poly.png"/>
        </figure>
      </statement>
    </example>

    <sage>
      <input>
var('x')
f1(x) = 1
f2(x) = x
f3(x) = x^2
f4(x) = x^3
f5(x) = x^4
B = [f1(x),f2(x),f3(x),f4(x),f5(x)]
def inp(f,g):
    return integral(f(x)*g(x),x,-1,1)
def inp_norm(f):
    return sqrt(integral(f(x)*f(x),x,-1,1))   
      </input>
      <output>
        
      </output>
    </sage>
<sage>
  <input>
g1(x) = f1(x)/inp_norm(f1);g1(x)
h(x) = f2(x)-inp(f2,g1)*g1(x)
g2(x) = h(x)/inp_norm(h);g2(x)
h(x) = f3(x)-inp(f3,g1)*g1(x)-inp(f3,g2)*g2(x)
g3(x) = h(x)/inp_norm(h);g3(x)
h(x) = f4(x)-inp(f4,g1)*g1(x)-inp(f4,g2)*g2(x)-inp(f4,g3)*g3(x)
g4(x) = h(x)/inp_norm(h);g4(x)
h(x) = f5(x)-inp(f5,g1)*g1(x)-inp(f5,g2)*g2(x)-inp(f5,g3)*g3(x)-inp(f5,g4)*g4(x)
g5(x) = h(x)/inp_norm(h);g5(x)
plot(g1(x),(x,-1,1))+plot(g2(x),(x,-1,1),color='red')+\
plot(g3(x),(x,-1,1),color='green')+plot(g4(x),(x,-1,1),color='pink')
  </input>
  <output>
    
  </output>
</sage>
    <exercise xml:id="inp-7-1-16">
      <statement>
        <p>
          Consider the standard basis <m>\beta=\{1,x,x^2,x^3\}</m> of
          <m>{\cal P}_3(\R)</m> with inner product <m>\inner{f}{g}:=\int_0^1 f(x)g(x)\,dx</m>.
          Find an orthonormal basis starting with <m>\beta</m> using the Gram-Schmidt orthogonalization process.
        </p>
        <p>
          <hint>
          
          <sage>
            <input>
def inp(f,g):
    return integral(f(x)*g(x),x,0,2*pi)

def projf(g,f):
    prgf=inp(g,f)/inp(f,f)*f
    return prgf

v1(x) = 1
v2(x) = x
v3(x) = x^2
v4(x) = x^3
f(x) = sin(x)
u1=v1
u2=v2-projf(v2,u1)
u3=v3-projf(v3,u1)-projf(v3,u2)
u4=v4-projf(v4,u1)-projf(v4,u2)-projf(v4,u3)
p=f-projf(f,u1)+projf(f,u2)+projf(f,u3)+projf(f,u4)
show(p(x))
            </input>
            <output>
              
            </output>
          </sage>    
        
          </hint>
        </p>
      </statement>
    </exercise>

    <example>
      <statement>
        <p>
          Let <m>A=\left(\begin{array}{rrr}2 \amp -1 \amp 0 \\-1 \amp 2 \amp -1 \\0 \amp -1 \amp 2 \end{array} \right)</m>.
          It is easy to check that <m>A</m> is a symmetric and positive definite matrix. (why?) Define an inner product on
          <m>\mathbb{R}^3</m> as <m>\inner{u}{v}:=v^TAu</m>.
        </p>
        <p>
          Use the the Gram-Schmidt orthogonalization process to find an orthonormal basis of from the standard basis vectors
          <m>\beta=\{e_1, e_2, e_3\}</m> with respect to the above inner product.
          <md>
            <mrow>u_1:  =\amp (1,0,0)</mrow>
            <mrow>u_2: =\amp  e_2-\frac{\inner{u_2\cdot e_2}{u_1}}{\norm{u_1}^2}u_1=(0,1,0)-\frac{-1}{2}(1,0,0)=(1/2,1,0)</mrow>
            <mrow>u_3: =\amp  e_3 -\frac{\inner{e_3}{u_1}}{\norm{u_1}^2}u_1-\frac{\inner{e_3}{u_1}}{\norm{u_2}^2}u_2</mrow>
            <mrow> = \amp  (0,0,1)-\frac{0}{2}(1,0,0)-\frac{-1}{3/2}(1/2,1,0)=(1/3, 2/3, 1)</mrow>
          </md>
        </p>
      </statement>

      <hint>
<sage>
  <input>
A=matrix([[2,-1,0],[-1,2,-1],[0,-1,2]])
A.is_positive_definite()
def inpA(u,v):
    return v.dot_product(A*u)
e1=vector([1,0,0])
e2=vector([0,1,0])
e3=vector([0,0,1])
u1=e1
u2=e2-inpA(e2,u1)/inpA(u1,u1)*u1
u3=e3-inpA(e3,u1)/inpA(u1,u1)*u1-inpA(e3,u2)/inpA(u2,u2)*u2
q1 = u1/sqrt(inpA(u1,u1))
q2 = u2/sqrt(inpA(u2,u2))
q3 = u3/sqrt(inpA(u3,u3))
print(q1,q2,q3) 
  </input>
</sage>

      </hint>

    </example>

    <example>
      <title>Lagrange Interpolating Polynomials</title>
      <statement>
        <p>
          Fix any <m>n+1</m> distinct real numbers,
          <m>c_0,c_1,\ldots, c_n</m> and define
          <me>
            \langle p,q \rangle := p(c_0)q(c_0)+p(c_1)q(c_1)+\cdots +p(c_n)q(c_n)
          </me>
          an inner product on <m>{\cal P}_n(\R)</m>.
        </p>
        <p>
          Recall the Lagrange Polynomial defined (Eqn.
          <xref ref="lagrange-eq1"></xref>).
          <me>
            \ell_i(x)=\prod_{j=0,j\neq i}^{n}\frac{x-c_j}{c_i-c_j}
          </me>.
        </p>
        <p>
          Then
          <me>
            \inner{\ell_i}{\ell_j}=\sum_{j=0}^n \ell_i(c_j)\ell_j(c_j)=\delta_{ij}
          </me>
        </p>
        <p>
          Hence <m>\{\ell_i\}</m> is an orthonormal basis of <m>{\cal P}_n(\R)</m>.
        </p>
        <p>
          Let <m>p(x)\in {\cal P}_n(\R)</m> be any polynomial, then
          <me>
            \inner{p}{\ell_k}=p(c_0)\ell_k(c_0)+p(c_1)\ell_k(c_1)+\cdots +p(c_n)\ell_k(c_n)=p(c_k)
          </me>.
          Since <m>\{\ell_i\}</m> is an orthonormal basis of <m>{\cal P}_n(\R)</m>, we have
          <md>
            <mrow>p(x)  = \inner{p(x)}{\ell_0(x)}\ell_0(x)+\inner{p(x)}{\ell_1(x)}\ell_1(x)+\cdots +\inner{p(x)}{\ell_n(x)}\ell_n(x)</mrow>
            <mrow> =  p(c_0)\ell_0(x)+p(c_1)\ell_1(x)+p(c_2)\ell_2(x)+\cdots +p(c_n)\ell_n(x)</mrow>
          </md>
          which is the Lagrange interpolation expansion of <m>p(x)</m>.
        </p>
      </statement>
    </example>

  <definition>
    <title>Projection onto a subspace</title>
    <p>
      Let <m>V</m> be an inner product space and <m>W\leq V</m>,
      a finite dimensional subspace of <m>V</m>.
      Let <m>\{u_1,\ldots, u_k\}</m> be an orthonormal basis of <m>W</m>.
      Suppose <m>v\in V</m>.
      Similar to <xref ref="subspace-projection1">definition</xref>,
      we can define the orthogonal projection of <m>v</m> onto <m>W</m> as
      <me>
        \proj_W(v):=  \inner{v}{u_1}u_1+\inner{v}{u_2}u_2+\cdots+\inner{v}{u_k}u_k
      </me>.
    </p>
    </definition>

    <exercise xml:id="inp-7-1-17">
      <statement>
        <p>
          Find the orthogonal projection of vector
          <m>b=\begin{bmatrix}1\\2\\3\\4 \end{bmatrix}</m> onto the subspace spanned by three vectors <m>\left\{\begin{bmatrix}1\\-1\\0\\1 \end{bmatrix} , \begin{bmatrix}0\\1\\1\\-1 \end{bmatrix} , \begin{bmatrix}1\\1\\-1\\0 \end{bmatrix} \right\}</m>.
        </p>
      </statement>
    </exercise>
 
    <activity>
      <p>
        <em>Notion of orthogonal projection and reflection in an inner product space.</em>
      </p>
      <p>
        Note that the concepts of Gramâ€“Schmidt orthogonalization, orthogonal projection, 
        and reflection can be naturally extended to an inner product space 
        <m>(V, \langle \cdot, \cdot \rangle)</m>. 
        Explore how these notions generalize in such spaces, and implement 
        solutions to related problems using Sage.
            </p>
    </activity>

</section>