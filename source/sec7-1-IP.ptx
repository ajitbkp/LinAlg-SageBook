<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec7-1-InnerProduct" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Inner Product</title>


  <p>
      In the last chapter,  we  dealt with notion of dot product and geometry in <m>\R^n</m>.
      The dot product and related notion can be generalized to an arbitrary vector space over 
      <m>\R</m> or <m>\mathbb{C}</m>.
      All the notions, we have learned in the last section can be generalized over an inner product space. 
      In this chapter we, shall introduce inner product on a vector space <m>V</m> over <m>\mathbb{R}</m> and 
      define extend the results studied in the last chapter on <m>V</m>.
    </p>

<p>
Note that the dot product of two vectors in <m>\R^n</m> is a scalar,
      in particular, dot product can be thought of as a function from
      <m>`\cdot' \colon \R^n\times \R^n \to\R</m> satisfying the following properties:
</p>
<p>
      <ol>
        <li>
          <p>
            <m>x \cdot x\geq 0</m> for all <m>x\in \R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x \cdot x= 0</m> if and only if <m>x=0</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x \cdot y=y\cdot x</m> for all <m>x,y \in \R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x\cdot (y+z) = x\cdot y +x\cdot z</m> for all <m>x,y,z\in \R^n</m>.
          </p>
        </li>
        <li>
          <p>
            <m>x\cdot (\alpha y)=\alpha x\cdot y = (\alpha x)\cdot y</m>.
          </p>
        </li>
      </ol>
    </p>

    <p>
      The notion of dot product on <m>\R^n</m> can ve generalized on vector space known as inner product.
      We have the following definition.
    </p>

    <definition xml:id="def-inner-product"><title>Inner Product</title>
       <statement>
        <p>
          Let <m>V</m> be a vector space over <m>\R</m>.
          An inner product on <m>V</m> is a function that assigns a real number
          <m>\langle x, y\rangle</m> to every pair <m>x,y</m> of vectors in <m>V</m> 
          (that is, a function <m>\langle \cdot, \cdot\rangle\colon V \times V \to \R</m>) satisfying the following properties. 
        </p>

          <p>
          <ol>
            <li>
            <p>
                <m>\langle x, x\rangle \geq 0</m> for all <m>x \in V</m> and
                <m>\langle x, x\rangle = 0</m> if and only if <m>x=0</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\langle x, y\rangle = \langle y, x\rangle</m> for all <m>x,y \in V</m>. (Symmetry)
              </p>
            </li>
            <li>   
              <p>
                <m>\langle x, (y+z)\rangle=\langle x, y\rangle+\langle x, z\rangle</m> for all <m>x,y,z\in V</m>.
              </p>
            </li>
            <li>
              <p>
                <m>\langle x, (\alpha y)\rangle=\alpha\langle x, y\rangle</m> for all
                <m>x,y\in V</m> and <m>\alpha \in \R</m>.   
              </p>
            </li>
          </ol>
        </p>

          <p>
            If <m>V</m> is real vector space with inner product <m>\langle. , .\rangle</m>.
Then <m>(V, \langle . , .\rangle)</m> called in inner product space over <m>\R</m>.
          </p>
      </statement>
    </definition>



    <p>
      The last two properties make the inner product linear in the second variable.
      Using the symmetry property,
      it can also be shown that the inner product is linear in the first variable as well.
      That is,
      <me>
        \langle (x+y),z\rangle=\langle x, z\rangle+\langle y, z\rangle, \text{ and } \langle (\alpha x) y\rangle=\alpha\langle x, y\rangle
      </me>
    </p>

      <p>
        Next we look at several examples of inner procuct on various vector spaces that we have defined in 
        <xref ref="chap4-Vector-Space"/>.
      </p>



    <example xml:id="dot-product-as-IP">
      <statement>
        <p>
          On <m>\R^n</m>, the standard dot product is an inner product.
          Thus define
          <me>
            \langle x,  y\rangle:=x\cdot y
          </me>
        </p>
        <p>
          This is also called the Euclidean inner product on <m>\R^n</m>.
        </p>
      </statement>
    </example>

    <example xml:id="inner-product-on-matrixspace">
      <statement>
        <p>
          Let <m>V=M_n(\R)</m>, the set of all
          <m>n\times n</m> matrices over <m>\R</m>.
          Define
          <me>
            \langle A,  B\rangle:=tr(AB^T)
          </me>
        </p>

        <p>
          It is easy to show that this is an inner product on <m>M_n(\R)</m>.
        </p>

        <p>
          Note that this inner product can be thought of as the standard dot product on <m>\R^{n^2}</m>. 
          The elements of the matrix <m>A</m> can be thought of as a vector in <m>\R^{n^2}</m>. Then 
          <me>
            tr(AB^T) =\sum\sum a_{ij}b_{ij}\quad \quad  \text{ why?}
          </me>.
          Work with  <m>2\times 2</m> matrices and then try to prove this for <m>n\times n</m> matrices.
        </p>
      </statement>
    </example>
<sage>
  <input>
  def inpMnR(A,B):
      return (A*B.T).trace()
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
A = matrix(QQ,[[2,-1,0],[1,4,2],[7,9,2]])
B = matrix(QQ,[[3,2,1],[1,2,0],[5,3,1]])  
inpMnR(A,B)
  </input>
  <output>
    
  </output>
</sage>

    <example xml:id="inner-product-by-matrix">
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> symmetric and positive definite matrix.
          On <m>\R^n</m>, define
          <me>
            \langle x,  y \rangle := x\cdot Ay = x^TAy
          </me>
        </p>
        <p>
          Then this is an inner product on <m>\R^n</m>.  
          (Where do we need <m>A</m> to be positive definie?)
         
        </p>
      </statement>

    </example>

  
<sage>
  <input>
def inner_product_PD(M,u,v):
    # Check if M is square
    if M.nrows() != M.ncols():
        raise ValueError("Matrix must be square.")
    # Check symmetry
    if not M.is_symmetric():
        raise ValueError("Matrix must be symmetric for a real inner product.")
    eigvals = M.eigenvalues()
    if not all(e > 0 for e in eigvals):
        raise ValueError("Matrix is not positive definite.")
    
    if len(u) != M.nrows() or len(v) != M.nrows():
        raise ValueError("Vectors must have same dimension as M.")
    
    return (u * M * v)  # Equivalent to u^T M v
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
A=matrix([[2,-1,0],[-1,2,-1],[0,-1,2]])
e1=vector([1,0,0])
e2=vector([0,1,0])
inner_product_PD(A,e1,e2)

  </input>
  <output>
    
  </output>
</sage>

<remark>
     <statement>
      <p>
         Since <m>A</m> is symmetric and positive definite matrix, there exists a matrix <m>B</m>  
      such that <m>B^2=A</m>. We call 
    <m>B</m> as positive definite square root of <m>A</m> and is denoted by <m>A^{1/2}</m>.  
      </p>
      <me>
        \langle x,y\rangle = x^T A y = (A^{1/2}x)\cdot(A^{1/2}y),
      </me>
      which shows this inner product is just the usual dot product after the linear change of 
      variables <m>x\mapsto A^{1/2}x</m>.
           </statement>


    </remark>

    <remark>
      <statement>
        <p>
          We have defined an inner product on <m>\R^n</m> using 
           a symmetric positive definite matrix <m>A</m>. In 
           fact any inner product on <m>\R^n</m> can be obtained in this way.
        </p>

        <p>
          Let <m>\innprod{x}{y}</m> be an inner product on  <m>\R^n</m>. 
          Let <m>e_1, e_2, \ldots, e_n</m> be the standard basis of <m>\R^n</m>. 
          For <m>x,y\in \R^n</m> with 
          <me>x=\sum x_i e_i, \quad y= \sum y_j e_j.</me>
          we have,  
          <me>
            \innprod{x}{y}=\sum_{i}\sum_j x_i\innprod{e_i}{e_j}y_j.
          </me>
          Let us define <m>a_{ij}=\innprod{e_i}{e_j}y_j</m>. Then it is easy to see that
          <me>
            \innprod{x}{y}=\begin{bmatrix} x_1 \amp x_2 \amp \cdots \amp x_n \end{bmatrix}
            A \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}=x^TAy,
          </me>
          where <m>A=[a_{ij}]</m> is a symmetric positive definite matrix.(why?)
        </p>
   

      </statement>
    </remark>

<example xml:id="inner-product-C01">
      <statement>
        <p>
          Let <m>V={\cal C}([0,1])</m> the set of all continuous function from <m>[0,1]</m> to <m>\R</m>.
          Define
          <me>
            \langle f,  g \rangle:=\int_0^1 f(t)g(t)dt
          </me>.
        </p>
        <p>
          This is an inner product on <m>V</m>.  (You may need real analysis to conclude that <m>\int_0^1 f(t)f(t)dt=0</m> then <m>f=0</m>.)
        </p>
      </statement>
    </example>
<sage>
  <input>
## inner product on C[0,1]
def inp(f,g):
    return integral(f(x)*g(x),x,0,1)
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
f(x)=x+1
g(x)=x^2
inp(f,g)
  </input>
  <output>
    
  </output>
</sage>

<sage>
  <input>
## Norm on C[0,1] w.r.t. to the above norm
def fnorm(f,inp):
    return sqrt(inp(f,f))  
  </input>
  <output>
    
  </output>
</sage>
<sage>
  <input>
g(x)=x^3-3*x+2
fnorm(g,inp)
  </input>
  <output>
    
  </output>
</sage>
<example xml:id="inner-product-on-polyspace">
      <statement>
        <p>
          Let <m>p</m> and <m>q</m> be two polynomials in <m>{\cal P}_n(\mathbb{R})</m>.
          Then define
          <me>
            \langle p,q \rangle := p(0)q(0)+p(1)q(1)+\cdots +p(n)q(n)
          </me>.
        </p>

        <p>
          It is easy to see that <m>\langle p,q \rangle</m> defined inner product on the vector space <m>{\cal P}_n(R)</m>.
        This inner product is called the discrete inner product on <m>{\cal P}_n(\mathbb{R})</m>.
        </p>

        <p>
          Here <m>0, 1, 2, \ldots, n</m> are nothing special.
          Instead, we can use any <m>n+1</m> distinct real numbers,
          <m>c_0,c_1,\ldots, c_n</m>.
        </p>
      </statement>
<p>
      <hint>
        <p>
        <m>\langle p,p \rangle =</m> mean that <m>p</m> has  <m>n+1</m> roots, which is not possible unless 
        <m>p=0</m> by the fundamental theorem of algebra.
        </p>
      </hint> 
    </p>    
  </example>

  <sage>
    <input> 
def inpPol(f,g,n):
    a = sum([f(x=i)*g(x=i) for i in range(n+1)])
    return a
    </input>
    <output>
      
    </output>
  </sage>

  <sage>
    <input>
f(x) = 3*x^3+4*x^2+2*x+1
g(x) = -2*x^3+x^2+2*x-1
inpPol(f,f,3)
    </input>
    <output>
      
    </output>
  </sage>
    <definition xml:id="definition-norm">
      <statement>
        <p>
          Let <m>(V, \langle .\rangle)</m> be a real inner product space.
          Then norm of any vector <m>x\in V</m> corresponding to the inner product <m>\langle . \rangle</m> is defined as
          <me>
            \norm{x}=\sqrt{\langle x, x\rangle}
          </me>.
          The distance between twp vectors <m>x</m> and <m>y</m> is defined as
          <me>
            d(x,y):=\norm{x-y}
          </me>.
        </p>
      </statement>
    </definition>



    <exercise xml:id="inp-7-1-8">
      <statement>
        <p>
          Let <m>V</m> be an inner product space.
          Then for any two vectors <m> x, y\in V</m>, show that
          <p> 1. 
          <m>\norm{x+y}^2=\norm{x}^2+\norm{y}^2+2\inner{x}{y}</m>
          </p>
          <p> 2. 
          <m> \norm{x-y}^2=\norm{x}^2+\norm{y}^2-2\inner{x}{y}</m>
          </p>
        </p>
      </statement>
      <hint>
        <p>
          The proof follows by looking at <m>\langle x+y,x+y \rangle </m>  and 
          <m>\langle x-y,x-y \rangle </m> and using the properties 
          of inner product. 
        </p>
      </hint>
    </exercise>

    <exercise xml:id="inp-7-1-9">
      <statement>
        <p>
          If <m>x, y</m> are two vectors in an inner product space <m>V</m> with inner product <m>\langle .\rangle</m>.
          Then show that
          <me>
            \norm{x+y}^2+\norm{x-y}^2=2(\norm{x}^2+\norm{y}^2)
          </me>.
        </p>
        <p>
          This is called the <alert>parallelogram identity.</alert>
          Geometrically, in a parallelogram,
          the sum of square of the diagonals is twice the sum of the squares of the side lengths.
        </p>
      </statement>
      <hint>
        <p>
          Proof follows from <xref ref="inp-7-1-8"/>.
        </p>
      </hint>
    </exercise>

    <exercise>
    <statement>
      <p>
        Let <m>a,b,c</m> be three positive real numbers and ,<m>x=(x_1,x_2,x_3), 
          y=(y_1,y_2,y_3)</m> be vectors in <m>\R^3</m>. Define
          <me>
          \langle x, y\rangle:=\frac{x_1y_1}{a^2}+\frac{x_2y_2}{b^2}+\frac{x_3y_3}{c^2}.
          </me>
         Show that this is an inner product on <m>\R^3</m>. Also what are the unit vectors with 
         respect to this inner product on <m>\R^3</m>?  
      </p>
    </statement>  
    </exercise>
    
    <theorem xml:id="thm-Cauchy-Schwarz-Inequality">
      <title>Cauchy-Schwarz Inequality</title>
      <statement>
        <p>
          Let <m>V</m> be an inner product space.
          The for any two vectors <m>x,y\in V</m>, we have
          <men xml:id="cauchy-swarch-ineq">
            |\inner{x}{y}|\leq \norm{x}\norm{y}
          </men>
        </p>
        <p>
          The equality holds if and only if <m>x</m> and <m>y</m> are linearly dependent.
        </p>
      </statement>

      <proof>
  <p>
    If <m>x = 0</m> or <m>y = 0</m>, then <m>\innprod{x}{y} = 0</m>. Also either
    <m>\innprod{x}{x} = 0</m> or <m>\innprod{y}{y} = 0</m>. Hence the result follows.
  </p>

  <p>
    First we consider the case when <m>\norm{x} = \norm{y} = 1</m> . 
    Then
   <md>
    <mrow>0 \leq  \amp\innprod{x - y}{x - y}</mrow>
    <mrow>= \amp\innprod{x}{x} + \innprod{y}{y} - 2\innprod{x}{y}</mrow>
    <mrow>= \amp 2 - 2\innprod{x}{y}</mrow>
  </md>

  This gives <m>\innprod{x}{y} \leq 1=\norm{x}\norm{y}.</m></p>
  <p>
    Similarly, 
    <md>
    <mrow>0 \leq  \amp\innprod{x + y}{x + y}</mrow>
    <mrow>= \amp\innprod{x}{x} + \innprod{y}{y} + 2\innprod{x}{y}</mrow>
    <mrow>= \amp 2 + 2\innprod{x}{y}.</mrow>
  </md>

   Thus  <m>\innprod{x + y}{x + y} \geq 0</m> implies
    <m>-\innprod{x}{y} \leq 1</m>.   From these two together we have 

  </p>

  <me>
  |\innprod{x}{y}|\leq 1 = \norm{x} \norm{y}.
  </me>

    <p>
    We now prove the statement for equality.
    Let <m>\mod{\innprod{x}{y}} = 1.</m> This implies either
    <m>\innprod{x}{y} = 1</m> or <m>\innprod{x}{y} = -1.</m>
    If <m>\innprod{x}{y} = 1</m>, from the above chain of inequalities we deduce
    that <m>\innprod{x - y}{x - y} = 0</m> or <m>x = y.</m>
    If <m>\innprod{x}{y} = -1</m>, we see that <m>x = -y.</m>
    Thus the equality holds if and only if <m>x = \pm y.</m>
  </p>

  <p>
    Next suppose <m>x</m> and <m>y</m> are nonzero (not necessarily of unit
    length). Then we can write <m>u = \frac{x}{\norm{x}}</m> and
    <m>v = \frac{y}{\norm{y}}</m> are of unit length. Hence by the previous case,
    <m>\mod{\innprod{u}{v}} \leq 1.</m> This implies,
    <me>
      \mod{\innprod{\frac{x}{\norm{x}}}{\frac{y}{\norm{y}}}} =
      \mod{\frac{1}{\norm{x}} \frac{1}{\norm{y}} \innprod{x}{y}} \leq 1.
  </me>
  thus we get <m>\mod{\innprod{x}{y}} \leq \norm{x} \norm{y}.</m>
  </p>

  <p>
    If <m>x</m> and <m>y</m> are nonzero, then the equality  from the earlier case means
    <m>\innprod{x}{y} = \norm{x} \norm{y}</m> or
    <m>-\innprod{x}{y} = \norm{x} \norm{y}</m>.  Let us look at one of the case.

  <md>
    <mrow>\amp \innprod{x}{y} =  \norm{x} \norm{y}</mrow>
    <mrow>\amp \Longleftrightarrow \innprod{\frac{x}{\norm{x}}}{\frac{y}{\norm{y}}} = 1 </mrow>
    <mrow>\amp \Longleftrightarrow \frac{x}{\norm{x}} = \frac{y}{\norm{y}}</mrow>
    <mrow>\amp \Longleftrightarrow x  = \frac{\norm{x}}{\norm{y}} y.</mrow>
    </md>

  The other case is similar. 
    </p>

  <p>
   <em>2nd Proof.</em> 
  </p>  

  <p>
    Fix <m>x</m> and <m>y</m> in <m>V</m>. If <m>y = 0</m>, then the result is
    obviously true. Without loss of generality  assume that <m>y \neq 0</m>. Consider the real-valued
    function <m>\varphi\colon \R \to \R</m>
   defined as    <me>\varphi(t) := \innprod{x + t y}{x + t y}</me>. 
   We shall use calclus and investigate the minimum value of <m>\varphi(t)</m> to prove the inequality. 
  <md>
    <mrow>\varphi(t) \amp= \amp\innprod{x + t y}{x + t y}</mrow>
    <mrow>\amp= \amp\innprod{x}{x} + 2 t \innprod{x}{y} + t^{2} \innprod{y}{y}.</mrow>
  </md>
Note that this is a differentiabke function of <m>t</m>. Differentiating <m>\varphi(t)</m> 
with respect to <m>t</m>, we get

  <me>
    \varphi'(t) = 2 \innprod{x}{y} + 2 t \innprod{y}{y}.
  </me>
  Setting <m>\varphi'(t) = 0</m>, we get the critical point at
  <me>
    t = -\frac{\innprod{x}{y}}{\innprod{y}{y}}.
  </me>
  Since <m>\varphi''(t) =\innprod{y}{y} \gt 0</m>, this critical point is a minimum. Thus
  <me>
    \varphi\left(-\frac{\innprod{x}{y}}{\innprod{y}{y}}\right) \geq 0.
  </me>
  Substituting for <m>t</m> in the expression for <m>\varphi(t)</m>, we get
  <md>
    <mrow>0 \amp\leq \innprod{x}{x} - \frac{\innprod{x}{y}^{2}}{\innprod{y}{y}}</mrow>
    <mrow>\Longleftrightarrow \innprod{x}{y}^{2} \amp\leq \innprod{x}{x} \innprod{y}{y}</mrow>
    <mrow>\Longleftrightarrow |\innprod{x}{y}| \amp\leq \norm{x} \norm{y}.</mrow>
  </md>
  This completes the 2nd proof of the Cauchy-Schwarz inequality.
  </p>
</proof>
    </theorem>

    <theorem xml:id="thm-Triangle-Inequality">
      <title>Triangle Inequality</title>
      <statement>
        <p>
          Let <m>x</m> and <m>y</m> be two vectors in an inner product space <m>V</m>.
          Then
          <md>
            <mrow>\norm{x+y}^2 \amp = \norm{x+y}^2=\norm{x}^2+\norm{y}^2+2\inner{x}{y}</mrow>
            <mrow>\amp \leq \norm{x}^2+\norm{y}^2+2|\inner{x}{y}|  \text{ \(a\leq |a|\) }</mrow>
            <mrow>\amp \leq  \norm{x}^2+\norm{y}^2+2\norm{x}\norm{y}  \text{ by the Cauchy-Schwarz inequality }</mrow>
            <mrow>\amp =  (\norm{x}+\norm{y})^2</mrow>
          </md>
        </p>
        <p>
          Hence for all <m>x,y</m>, we have 
          <md>
            <mrow xml:id="trangle_ineq" number="yes">\norm{x+y}\leq \norm{x}+\norm{y}</mrow>
          </md>
          is called the triangle inequality.
        </p>
      </statement>
    </theorem>

    <p>
      Let us play with the Cauchy-Schwarz inequality<nbsp/><xref ref="cauchy-swarch-ineq"/>.
      Suppose <m>x</m> and <m>y</m> are non zero vectors in <m>V</m>, Then
      <me>
        |\inner{x}{y}|\leq \norm{x}\norm{y}\Rightarrow \frac{|\inner{x}{y}|}{\norm{x}\norm{y}}\leq 1
      </me>
    </p>

    <p>
      Hence we have
      <me>
        -1\leq \frac{\inner{x}{y}}{\norm{x}\norm{y}}\leq 1
      </me>.
    </p>

    <p>
      Thus for any two non zero vectors, <m>x</m> and <m>y</m>,
      <m>\frac{\inner{x}{y}}{\norm{x}\norm{y}}</m> always lies between <m>-1</m> and 1.
      This allows us to define the angle between two non zero vectors.
      We assign this number to <m>\cos\theta</m> with
      <m>\theta\in[-\pi,\pi]</m> called the angle between <m>x</m> and <m>y</m>.
      Thus, if <m>\theta</m> is the angle between <m>x</m> and <m>y</m>,
      then we have
      <me>
        \cos \theta = \frac{\inner{x}{y}}{\norm{x}\norm{y}}
      </me>
    </p>

    <p>
      All the notions that we defined for dot product, namely,
      orthogonality,
      orthogonal projection, Gram-Schmidt orthogonalization process can we defined in a similar manner.
      All we need to do is,
      replace the dot product by the given inner product.
    </p>

<theorem xml:id="thm-norm-properties"> <title>Properties of Norm</title>
  <statement>
    <p>
      Let <m>(V,\langle ., .\rangle)</m> be an innepr product space. The norm defined as <xref ref="definition-norm"/> has the following properties:
    </p>
    <p>
      <ol>
        <li>
          <p>
            for all <m>x\in V</m>, <m>\norm{x}\geq 0</m> and <m>\norm{x}= 0</m> if and only if <m>x=0</m>.
          </p>
        </li>
        <li>
          <p>
            for all <m>\alpha \in \R</m> and <m>x\in V</m>, <m>\norm{\alpha x}=|\alpha|\norm{x}</m>.
          </p>
        </li>
        <li>
          <p>
            for all <m>x,y \in V</m>, <m>\norm{x+y}\leq \norm{x}+\norm{y}</m>.
          </p>
        </li>
      </ol>
    </p>
  </statement>
</theorem>

<definition xml:id="def-normed-lin-space">
  <statement>
    <p>
      Any vector space <m>V</m> over <m>\R</m> with a function <m>\norm{.} : V \to \R</m> which satisfies all the 
      properties mentioned in <xref ref="thm-norm-properties"/> is called a <alert>normed linear space.</alert>. Thus 
      any inner product space is also a normed linear space.
    </p>
  </statement>
</definition>

    <note>
      <statement>
        <p>
          The concepts such as orthogonality, orthogonal projection, orthogonal complement of 
any subset, orthogonal and orthonormal sets and Gram-Schmidt orthogonalization process etc that we 
defined and dealt with in the previuos chapter with respect to the dot product on <m>\R^n</m> can be 
defined on an inner product space. All we need to do is to replace the dot product by the corresponding 
inner product.  We encourage  readers to define each one of them. 

        </p>
      </statement>
  </note>

    <exercise xml:id="inp-7-1-10">
      <statement>
        <p>
          Let <m>(V, \inner{.}{.})</m> be a real inner product space.
          Define 
          <ol>
            <li>
              <p>
                orthogonality of two vectors <m>x</m>  and <m>y</m> in <m>V</m>,
              </p>
            </li>
            <li>
              <p>
                orthogonal complement of a subset <m>U</m> of <m>V</m>,
              </p>
            </li>
            <li>
              <p>
                Orthogonal projection of a vector <m>v</m> onto a non-zero 
          vector <m>u</m>,
              </p>
            </li>
            <li>
              <p>
                orthogonal set and orthonormal sets in <m>V</m> and 
              </p>
            </li>
            <li>
              <p>
                Gram-Schmidt orthogonalization process.
              </p>
            </li>
          </ol>   
        </p>
      </statement>
    </exercise>

    <p>
      As mentioned earlier all the notions related to orthogonality can be defined in a similar manner 
      on an inner product space and the results that we proved in the previous chapter hold true in 
      this more general setting. However, let us state some of the definitions and results here for 
      completeness. 
    </p>

<definition xml:id="def-inp-orthogonality">
  <statement>
    <p>
      Let <m>(V,\innprod{.}{.})</m> be an inner product space. 
      Two vectors <m>x,y\in V</m> are said to be <em>orthogonal</em> if <m>\innprod{x}{y}=0.</m>
    </p>
  </statement>
</definition>

<definition xml:id="def-inp-orthogonal-set">
  <statement>
    <p>
      Let <m>(V,\innprod{.}{.})</m> be an inner product space. 
      A set of vectors  <m>\{v_1,\ldots,v_k\}</m> is  said to be <em>orthogonal set</em>     if <m>\innprod{v_i}{v_j}=0</m> for <m>i\neq j</m>.
    </p>
    <p>
      In addition, if each vector in the set is of unit norm, i.e., <m>\norm{v_i}=1</m> for all <m>i=1,2,\ldots,k</m>,
      then the set is called an <em>orthonormal set.</em>
    </p>
  </statement>
</definition>

    <exercise xml:id="inp-7-1-11">
      <statement>
        <p>
          Let <m>x, y</m> be two vectors in an inner product space <m>V</m>.
          Then show that
       <ol>
        <li>
          <p>
            <m>x</m> and <m>y</m> are orthogonal if and only if
          <m>\norm{x+y}=\norm{x-y}</m>. (what does it mean geometrically?)
          </p>
        </li>
        <li>
          <p>
            <m>x+y</m> and <m>x-y</m> are orthogonal if and only if <m>\norm{x}=\norm{y}</m>.
          </p>
        </li>
       </ol>        
          
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-12">
      <statement>
        <p>
          Let <m>C([-\pi,\pi])</m> be the vectors space of set of continuous functions from <m>[-\pi,\pi]</m> to <m>\R</m>.
          Define the inner product on <m>C([-\pi,\pi])</m> as
          <me>
            \inner{f}{g}:=\int_{-\pi}^\pi f(t) g(t)`dt
          </me>.
        </p>
        <p>
          Show that under this inner product
          <m>\{1, \sin nx,\cos mx\}</m> is an orthogonal set.
        </p>
      </statement>
    </exercise>

    <exercise xml:id="inp-7-1-13">
      <title>Pythagoras Theorem</title>
      <statement>
        <p>
          Let <m>(V, \inner{.}{.})</m> be a real inner product space.
          Let <m>x_1, x_2, \ldots x_n</m> be <m>n</m> orthogonal vectors in <m>V</m>.
          Then
          <me>
            \norm{x_1+x_2+\cdots+x_n}^2=\norm{x_1}^2+\norm{x_2}^2+\cdots+\norm{x_n}^2
          </me>.
        </p>
        <p>
          This is called the Pythagoras theorem.
        </p>
      </statement>
    </exercise>

  <p>
    Is the converse of the Pythagoras theorem true?
    Consider <m>u=v=(1,1)</m> and <m>w=(0,-1)</m>. It is easy to check that that 
    <me>
     \norm{u+v+w}^2=\norm{u}^2+\norm{v}^2+\norm{w}^2,
    </me>
    <m>u,v, w</m> ar not orthogonal to each other.
  </p>
    <exercise xml:id="inp-7-1-14">
      <statement>
        <p>
          Let <m>\beta=\{u_1,\ldots, u_n\}</m> be an orthogonal basis of an inner product space <m>V</m>.
          Let <m>v\in V</m> and <m>\theta_1,\ldots, \theta_n</m> be the angle between <m>v</m> and <m>u_1,\ldots, u_n</m>,  respectively.
          Then
          <me>
            \cos\theta_1^2+\cdots+\cos\theta_n^2=1
          </me>.
        </p>
        <p>
          Here <m>\cos\theta_i</m> are called the direction cosines of <m>v</m> corresponding to <m>\beta</m>.
        </p>
      </statement>
 <hint>
        <p>
      If <m>v=0</m> the statement is trivial (all direction cosines are zero). Assume
    <m>v\neq 0</m>. Since <m>\beta</m> is an orthogonal basis we can expand <m>v</m> uniquely as
  <me>
  v = \sum_{i=1}^{n} \frac{\innprod{v}{u_{i}}}{\innprod{u_{i}}{u_{i}}}\, u_{i}.
  </me>
    Taking norms squared and using orthogonality of the <m>u_{i}</m> gives
  <me>
      \norm{v}^{2}
      = \sum_{i=1}^{n} \frac{\lvert\innprod{v}{u_{i}}\rvert^{2}}{\innprod{u_{i}}{u_{i}}}.
   </me>
    By definition of the angle <m>\theta_{i}</m> between <m>v</m> and <m>u_{i}</m> we have
  <me>
  \cos\theta_{i} = \dfrac{\innprod{v}{u_{i}}}{\norm{v}\,\norm{u_{i}}}.
  </me>
    Using the relation
  <me>
\dfrac{\lvert\innprod{v}{u_{i}}\rvert^{2}}{\innprod{u_{i}}{u_{i}}}
       = \norm{v}^{2}\,\dfrac{\lvert\innprod{v}{u_{i}}\rvert^{2}}{\norm{v}^{2}\,\norm{u_{i}}^{2}}
       = \norm{v}^{2}\,\cos^{2}\theta_{i},
  </me>
    substitute into the norm identity to obtain 
     <me>
    \norm{v}^{2} = \sum_{i=1}^{n} \norm{v}^{2}\,\cos^{2}\theta_{i} = \norm{v}^{2}
      \sum_{i=1}^{n} \cos^{2}\theta_{i}.
  </me>
    Dividing by <m>\norm{v}^{2}</m> (which is positive because <m>v\neq 0</m>) yields
  <me>
  \sum_{i=1}^{n}\cos^{2}\theta_{i}=1,
  </me>
    as required.
  </p>
    </hint>
    </exercise>
 <definition xml:id="def-inp-orthonormal-basis">
      <statement>
        <p>
  A basis <m>\{v_1, \ldots, v_n\}</m> of an inner product space <m>V</m> is said to be
<em>orthonormal basis</em> if we have <m>\innprod{v_i}{v_j}=\delta_{ij}</m> for 
<m> 1 \leq i,j \leq n</m>.
        </p>
      </statement>
    </definition>
   
    <p>
  Let us assume that <m>\{v_i\}</m> is an orthonormal basis of <m>V</m>.
  Write <m>v = \sum_i \alpha_i v_i</m>. Taking the inner product of both sides
  with the vector <m>v_j</m>, and using the orthonormal properties of the basis, we get

  <md>
  <mrow>  \innprod{v}{v_j} \amp = \innprod{\sum_i \alpha_i v_i}{v_j}</mrow>
   <mrow> \amp =\sum_i \innprod{\alpha_i v_i}{v_j}</mrow>    
    <mrow> \amp = \sum_i \alpha_i \innprod{v_i}{v_j} </mrow>
      <mrow> \amp= \sum_i \alpha_i \delta_{ij} =\alpha_j</mrow>
  </md>
  Thus <m>\alpha_j = \innprod{v}{v_j}</m>. In particular, we have
  <me>
    v = \sum_i \innprod{v}{v_i}v_i.
  </me>
</p>
 <p>   
Next we turn our attention to finding an orthogonal basis in an inner product space using the Gram-Schmidt process.
The process is exactly same as before, we just need to replace the dot product by the given inner product.
Let <m>(V, \inner{.}{.})</m> be an inner product space.
Let <m>\{v_1, v_2, \ldots, v_n\}</m> be a basis of <m>V</m>.
We construct an orthogonal basis <m>\{u_1, u_2, \ldots, u_n\}</m> as follows:
<ol>
  <li>
    <p>
      Set <m>u_1 = v_1</m>.
    </p>
  </li>
  <li>
    <p>
      For <m>2 \leq k \leq n</m>, set
      <me>
        u_k = v_k - \sum_{j=1}^{k-1} \frac{\inner{v_k}{u_j}}{\norm{u_j}^2} u_j.
      </me>
    </p>
  </li>
</ol>   
</p>
<p>
  Thus we have the following result.
</p>

<theorem xml:id="thm-inp-gram-schmidt">
    <title>Gram–Schmidt Orthogonalization Theorem</title>
    <p>
      Let <m>\{v_1, v_2, \ldots, v_n\}</m> be a linearly independent set in an inner product space <m>V</m>.
      Then there exists an orthogonal set <m>\{u_1, u_2, \ldots, u_n\}</m> such that
    </p>

    <me>
      \text{span}\{u_1, u_2, \ldots, u_k\}
      = \text{span}\{v_1, v_2, \ldots, v_k\}, \quad 1 \leq k \leq n.
    </me>

    <p>
      The vectors <m>u_1, u_2, \ldots, u_n</m> are defined recursively as follows:
    </p>
<md>
        <mrow>     u_1 \amp:= v_1, </mrow>
        <mrow> u_2 \amp:= v_2 - \frac{\innprod{v_2}{u_1}}{\innprod{u_1}{u_1}} u_1, </mrow>
        <mrow> u_3 \amp:= v_3 - \frac{\innprod{v_3}{u_1}}{\innprod{u_1}{u_1}} u_1 
                     - \frac{\innprod{v_3}{u_2}}{\innprod{u_2}{u_2}} u_2, </mrow>
        <mrow> \amp\ \vdots </mrow>
        <mrow> u_k \amp:= v_k - \sum_{j=1}^{k-1}
                     \frac{\innprod{v_k}{u_j}}{\innprod{u_j}{u_j}} u_j,
                     \quad 2 \leq k \leq n. </mrow>
        </md>

    <p>
      The resulting set <m>\{u_1, u_2, \ldots, u_n\}</m> is orthogonal.
      To obtain an <em>orthonormal</em> set, define
    </p>

    <me>
      e_i: = \frac{u_i}{\norm{u_i}}, \quad 1 \leq i \leq n.
    </me>

    <p>
      Then <m>\{e_1, e_2, \ldots, e_n\}</m> is an orthonormal basis of the subspace
      <m>\text{span}\{v_1, v_2, \ldots, v_n\}</m>.
    </p>

      </theorem>

        <proof>
    <p>
      The proof proceeds by induction on <m>k</m>.
      For <m>k = 1</m>, set <m>u_1 = v_1</m>, which is nonzero since <m>v_1</m>
      is linearly independent.
      Suppose <m>u_1, \ldots, u_{k-1}</m> have been constructed such that they are
      mutually orthogonal and
      <m>\operatorname{span}\{u_1, \ldots, u_{k-1}\}
      = \operatorname{span}\{v_1, \ldots, v_{k-1}\}</m>.
    </p>

    <p>
      Define
      <m>
        u_k = v_k - \sum_{j=1}^{k-1}
        \frac{\innprod{v_k}{u_j}}{\innprod{u_j}{u_j}} u_j.
      </m>
      Then for each <m>i &lt; k</m>,
    </p>

    <md>   
      <mrow> \innprod{u_k}{u_i}
       \amp = \innprod{v_k}{u_i}
        - \sum_{j=1}^{k-1}
        \frac{\innprod{v_k}{u_j}}{\innprod{u_j}{u_j}} \innprod{u_j}{u_i}</mrow>
      <mrow> \amp= \innprod{v_k}{u_i}
        - \frac{\innprod{v_k}{u_i}}{\innprod{u_i}{u_i}}\innprod{u_i}{u_i}
      = 0.</mrow>
     </md>

    <p>
      Hence, <m>u_k</m> is orthogonal to all previous <m>u_i</m>.
      This completes the inductive step, and the result follows.
    </p>
  </proof>
<remark>
  <statement>
    <p>
         The Gram–Schmidt process provides an explicit algorithm for transforming any
      linearly independent set into an orthonormal one.
      It is a fundamental tool in numerical linear algebra, functional analysis,
      and the construction of orthogonal polynomials.
    </p>
  </statement>
   </remark>

   <example>
    <p>
    Let <m>V=\mathcal{P}_2(\mathbb{R})</m> and define for
    <m>p,q \in V</m>
  <me>
\innprod{p}{q} := p(0)q(0) + p(1)q(1) + p(2)q(2).
  </me>
    Apply Gram–Schmidt to the linearly independent set
    <m>\{v_{1},v_{2},v_{3}\} = \{1,x,x^{2}\}</m>.
</p>

<p> 
<m>u_{1}=v_{1}=1</m>.

  <me>
  \innprod{u_{1}}{u_{1}} = 1^2 + 1^2 + 1^2 = 3.
  </me>
</p>

<p>
  <me>
      u_{2}
      = v_{2} - \dfrac{\innprod{v_{2}}{u_{1}}}{\innprod{u_{1}}{u_{1}}}\,u_{1}
      = x - \dfrac{x(0)+x(1)+x(2)}{3}\cdot 1= x - 1.
  </me>

  <me>
    \innprod{u_{2}}{u_{2}} = ( -1)^2 + 0^2 + 1^2 = 1+0+1 = 2.
  </me>
</p>


  <p>
  <me>
      u_{3}
      = v_{3}
        - \dfrac{\innprod{v_{3}}{u_{1}}}{\innprod{u_{1}}{u_{1}}}\,u_{1}
        - \dfrac{\innprod{v_{3}}{u_{2}}}{\innprod{u_{2}}{u_{2}}}\,u_{2}.
  </me>

  <me>
      \innprod{v_{3}}{u_{1}} = x^{2}(0)+x^{2}(1)+x^{2}(2) = 0 + 1 + 4 = 5.
  </me>

  <md>
     <mrow>  \innprod{v_{3}}{u_{2}} \amp =
     = \innprod{x^{2}}{x-1}</mrow>
      = \bigl(x^{2}(0)(x-1)(0)\bigr)+\bigl(x^{2}(1)(x-1)(1)\bigr)+\bigl(x^{2}(2)(x-1)(2)\bigr)
     <mrow> \amp  = 0\cdot(-1)+1\cdot 0 + 4\cdot 1 = 4. </mrow>
  </md>
Hence
  <me>
     u_{3}
      = x^{2} - \dfrac{5}{3}\cdot 1 - \dfrac{4}{2}\,(x-1)
      = x^{2} - \dfrac{5}{3} - 2x + 2
      = x^{2} - 2x + \dfrac{1}{3}.
  </me>

  <me>
    <m>\innprod{u_{3}}{u_{3}} = \dfrac{2}{3}.</m>
  </me>
    Thus we have produced an <em>orthogonal</em> set
    <m>\{u_{1},u_{2},u_{3}\}</m> where
  <me>
      u_{1} = 1,\qquad
      u_{2} = x - 1,\qquad
      u_{3} = x^{2} - 2x + \tfrac{1}{3}.
  </me>
  It is easy to see that 
      <me>\innprod{u_{3}}{u_{3}} = \dfrac{2}{3}.</me>
Since 
  <me>
      \norm{u_{1}} = \sqrt{3},\quad
      \norm{u_{2}} = \sqrt{2},\quad
      \norm{u_{3}} = \sqrt{\tfrac{2}{3}},
  </me>
we have 
  <me>
      e_{1} = \dfrac{1}{\sqrt{3}},\quad
      e_{2} = \dfrac{x-1}{\sqrt{2}},\quad
      e_{3} = \dfrac{x^{2}-2x+\tfrac{1}{3}}{\sqrt{2/3}}
            = \sqrt{\tfrac{3}{2}}\bigl(x^{2}-2x+\tfrac{1}{3}\bigr).
  </me>
    The set <m>\{e_{1},e_{2},e_{3}\}</m> is an orthonormal basis of
    <m>\operatorname{span}\{1,x,x^{2}\}=\mathcal{P}_2(\mathbb{R})</m> (with respect to the discrete inner product above).
  </p>
<p>
  Let us do this using SageMath as well by on <m>\mathcal{P}_4(\mathbb{R})</m>
</p>
  <sage>
    <input>
    def inpPol(f,g,n):
        a = sum([f(x=i)*g(x=i) for i in range(n+1)])
        return a

    f1(x)=1
    f2(x)=x
    f3(x)=x^2
    f4(x)=x^3
    f5(x)=x^4

    g1 = f1
    g2 = f2 - inpPol(f2,g1,4)/inpPol(g1,g1,4)*g1
    g3 = f3 -inpPol(f3,g1,4)/inpPol(g1,g1,4)*g1-inpPol(f3,g2,4)/inpPol(g2,g2,4)*g2
    g4 = f4- inpPol(f4,g1,4)/inpPol(g1,g1,4)*g1-\
            inpPol(f4,g2,4)/inpPol(g2,g2,4)*g2-inpPol(f4,g3,4)/inpPol(g3,g3,4)*g3
    g5 = f5-inpPol(f5,g1,4)/inpPol(g1,g1,4)*g1-inpPol(f5,g2,4)/inpPol(g2,g2,4)*g2-\
            inpPol(f5,g3,4)/inpPol(g3,g3,4)*g3-inpPol(f5,g4,4)/inpPol(g4,g4,4)*g4
    show(g1(x))
    show(g2(x))
    show(g3(x))
    show(g4(x))
    show(g5(x))
    </input>
    <output>
      
    </output>
  </sage>
   </example>

    <exercise xml:id="inp-7-1-15">
      <statement>
        <p>
          Let <m>\beta=\{u_1,\ldots,
          u_n\}</m> be an orthogonal basis of an inner product space <m>V</m>.
          Let <m>x</m> and <m>y</m> be two vectors such that
          <m>x=\sum x_i u_i</m> and <m>y=\sum y_i u_i</m>.
          Then
          <me>
            \inner{x}{y}=x_1y_1+x_2y_2+\cdots+x_ny_n
          </me>
          and
          <me>
            \norm{x}^2=x_1^2+x_2^2+\cdots +x_n^2
          </me>.
        </p>
      </statement>
    </exercise>

    <example xml:id="Legendre-poly">
        <title>Legendre Polynomials</title>
      <statement>
        <p>
          Consider <m>V ={\cal P}_3(\R)</m> with inner product <m>\inner{p}{q}:=\int_{-1}^1 p(x)q(x)\,dx</m>.
          Use the standard basis <m>\beta =\{v_1,v_2,v_3,v_4\} = \{1,x,x^2,x^3\}</m> to find an orthogonal basis of <m>{\cal P}_3(\R)</m>.
        </p>
        <p>
          First of all notice that <m>\beta</m> is not an orthogonal basis.
          For <m>\inner{v_1}{v_3}=\inner{1}{x^2} = \int_{-1}^1 x^2 dx = \frac23</m>,
          <m>\inner{v_2}{v_4}=\int_{-1}^1 x^4 dx = \frac25</m>.
          Also note that <m>\inner{v_1}{v_2}=\int_{-1}^1 xdx = 0</m>.
          <m>\inner{v_2}{v_3}=\int_{-1}^1 x^3dx = 0</m>.
          <m>\inner{v_1}{v_4}=\int_{-1}^1 x^3 dx = 0</m>. <m>\inner{v_3}{v_4}=\int_{-1}^1 x^5dx = 0</m>.
        </p>
        <p>
          Since <m>v_1</m> and <m>v_2</m> are already orthogonal,
          we can choose <m>u_1=v_1=1</m> and
          <m>u_2 = v_2=x</m> in the Gram-Schmidt process.
          Next
          <me>
            u_3 = v_3-\inner{v_3}{u_1}/\norm{u_1}^2u_1-\inner{v_3}{u_2}/\norm{u_2}^2u_2x^2
          </me>.
        </p>
        <p>
          We have
          <me>
            \inner{v_3}{u_1}= \int_{-1}^1 x^2 dx = \frac23, \inner{u_1}{u_1}=\int_{-1}^1 1dx = 2
          </me>.
        </p>
        <p>
          Hence
          <me>
            u_3 = x^2 - \frac{\frac23}{2} \times 1 = x^2-\frac13
          </me>.
        </p>
        <p>
          Next
          <md>
            <mrow>u_4 \amp =  u_4-\inner{v_4}{u_1}/\norm{u_1}^1u_1-\inner{v_4}{u_2}/\norm{u_2}^1u_2-\inner{v_4}{u_3}/\norm{u_3}^1u_3</mrow>
            <mrow>\amp =v_4-\inner{v_4}{u_2}/\norm{u_2}^1u_2-\inner{v_4}{u_3}/\norm{u_3}^1u_3</mrow>
            <mrow>\amp = x^3-\frac35 x</mrow>
          </md>.
        </p>
        <p>
          Hence an orthogonal basis is <m>\beta'=\{1,x,x^2-\frac{1}{3}, x^3-\frac35 x \}</m>.
          These are the first four <em>Legendre polynomials.</em>
        </p>
        <p>
          After normalizing the vectors,
          we get an orthonormal basis
          <me>
            \beta''=\left\{\frac{\sqrt{2}}{2}, \frac{\sqrt{6}}{2}x, \frac{3\sqrt{10}}{4}\left(x^2-\frac13\right), \frac{5\sqrt{14}}{4}\left(x^3-\frac35 x\right)\right\}
          </me>.
        </p>
        <figure xml:id="fig_legendrepoly" halign="center">
          <caption halign="center">Graph of Legendre polynomials</caption>
          <image width="50%" source="images/Legendre_poly.png"/>
        </figure>
      </statement>
    </example>

    <sage>
      <input>
var('x')
f1(x) = 1
f2(x) = x
f3(x) = x^2
f4(x) = x^3
f5(x) = x^4
B = [f1(x),f2(x),f3(x),f4(x),f5(x)]
def inp(f,g):
    return integral(f(x)*g(x),x,-1,1)
def inp_norm(f):
    return sqrt(integral(f(x)*f(x),x,-1,1))   
      </input>
      <output>
        
      </output>
    </sage>
<sage>
  <input>
g1(x) = f1(x)/inp_norm(f1);g1(x)
h(x) = f2(x)-inp(f2,g1)*g1(x)
g2(x) = h(x)/inp_norm(h);g2(x)
h(x) = f3(x)-inp(f3,g1)*g1(x)-inp(f3,g2)*g2(x)
g3(x) = h(x)/inp_norm(h);g3(x)
h(x) = f4(x)-inp(f4,g1)*g1(x)-inp(f4,g2)*g2(x)-inp(f4,g3)*g3(x)
g4(x) = h(x)/inp_norm(h);g4(x)
h(x) = f5(x)-inp(f5,g1)*g1(x)-inp(f5,g2)*g2(x)-inp(f5,g3)*g3(x)-inp(f5,g4)*g4(x)
g5(x) = h(x)/inp_norm(h);g5(x)
plot(g1(x),(x,-1,1))+plot(g2(x),(x,-1,1),color='red')+\
plot(g3(x),(x,-1,1),color='green')+plot(g4(x),(x,-1,1),color='pink')
  </input>
  <output>
    
  </output>
</sage>
    <exercise xml:id="inp-7-1-16">
      <statement>
        <p>
          Consider the standard basis <m>\beta=\{1,x,x^2,x^3\}</m> of
          <m>{\cal P}_3(\R)</m> with inner product <m>\inner{f}{g}:=\int_0^1 f(x)g(x)\,dx</m>.
          Find an orthonormal basis starting with <m>\beta</m> using the Gram-Schmidt orthogonalization process.
        </p>
        <p>
          <hint>
          
          <sage>
            <input>
def inp(f,g):
    return integral(f(x)*g(x),x,0,2*pi)

def projf(g,f):
    prgf=inp(g,f)/inp(f,f)*f
    return prgf

v1(x) = 1
v2(x) = x
v3(x) = x^2
v4(x) = x^3
f(x) = sin(x)
u1=v1
u2=v2-projf(v2,u1)
u3=v3-projf(v3,u1)-projf(v3,u2)
u4=v4-projf(v4,u1)-projf(v4,u2)-projf(v4,u3)
p=f-projf(f,u1)+projf(f,u2)+projf(f,u3)+projf(f,u4)
show(p(x))
            </input>
            <output>
              
            </output>
          </sage>    
        
          </hint>
        </p>
      </statement>
    </exercise>

    <example>
      <statement>
        <p>
          Let <m>A=\left(\begin{array}{rrr}2 \amp -1 \amp 0 \\-1 \amp 2 \amp -1 \\0 \amp -1 \amp 2 \end{array} \right)</m>.
          It is easy to check that <m>A</m> is a symmetric and positive definite matrix. (why?) Define an inner product on
          <m>\mathbb{R}^3</m> as <m>\inner{u}{v}:=v^TAu</m>.
        </p>
        <p>
          Use the the Gram-Schmidt orthogonalization process to find an orthonormal basis of from the standard basis vectors
          <m>\beta=\{e_1, e_2, e_3\}</m> with respect to the above inner product.
          <md>
            <mrow>u_1:  =\amp (1,0,0)</mrow>
            <mrow>u_2: =\amp  e_2-\frac{\inner{u_2\cdot e_2}{u_1}}{\norm{u_1}^2}u_1=(0,1,0)-\frac{-1}{2}(1,0,0)=(1/2,1,0)</mrow>
            <mrow>u_3: =\amp  e_3 -\frac{\inner{e_3}{u_1}}{\norm{u_1}^2}u_1-\frac{\inner{e_3}{u_1}}{\norm{u_2}^2}u_2</mrow>
            <mrow> = \amp  (0,0,1)-\frac{0}{2}(1,0,0)-\frac{-1}{3/2}(1/2,1,0)=(1/3, 2/3, 1)</mrow>
          </md>
        </p>
      </statement>

      <hint>
<sage>
  <input>
A=matrix([[2,-1,0],[-1,2,-1],[0,-1,2]])
A.is_positive_definite()
def inpA(u,v):
    return v.dot_product(A*u)
e1=vector([1,0,0])
e2=vector([0,1,0])
e3=vector([0,0,1])
u1=e1
u2=e2-inpA(e2,u1)/inpA(u1,u1)*u1
u3=e3-inpA(e3,u1)/inpA(u1,u1)*u1-inpA(e3,u2)/inpA(u2,u2)*u2
q1 = u1/sqrt(inpA(u1,u1))
q2 = u2/sqrt(inpA(u2,u2))
q3 = u3/sqrt(inpA(u3,u3))
print(q1,q2,q3) 
  </input>
</sage>

      </hint>

    </example>

    <example>
      <title>Lagrange Interpolating Polynomials</title>
      <statement>
        <p>
          Fix any <m>n+1</m> distinct real numbers,
          <m>c_0,c_1,\ldots, c_n</m> and define
          <me>
            \langle p,q \rangle := p(c_0)q(c_0)+p(c_1)q(c_1)+\cdots +p(c_n)q(c_n)=
            \sum_{j=0}^n p(c_j)q(c_j)
          </me>
          an inner product on <m>{\cal P}_n(\R)</m>.
        </p>
        <p>
          Recall the Lagrange Polynomial defined (Eqn.
          <xref ref="lagrange-eq1"></xref>).
          <me>
            \ell_i(x)=\prod_{j=0,j\neq i}^{n}\frac{x-c_j}{c_i-c_j}
          </me>.
        </p>
        <p>
          Then
          <me>
            \inner{\ell_i}{\ell_j}=\sum_{k=0}^n \ell_i(c_k)\ell_j(c_k)=\delta_{ij}.
          </me>
        </p>
        <p>
          Hence <m>\{\ell_i\}</m> is an orthonormal basis of <m>{\cal P}_n(\R)</m>.
        </p>
        <p>
          Let <m>p(x)\in {\cal P}_n(\R)</m> be any polynomial, then
          <me>
            \inner{p}{\ell_k}=p(c_0)\ell_k(c_0)+p(c_1)\ell_k(c_1)+\cdots +p(c_n)\ell_k(c_n)=p(c_k)
          </me>.
          Since <m>\{\ell_i\}</m> is an orthonormal basis of <m>{\cal P}_n(\R)</m>, we have
          <md>
            <mrow>p(x)  = \inner{p(x)}{\ell_0(x)}\ell_0(x)+\inner{p(x)}{\ell_1(x)}\ell_1(x)+\cdots +\inner{p(x)}{\ell_n(x)}\ell_n(x)</mrow>
            <mrow> =  p(c_0)\ell_0(x)+p(c_1)\ell_1(x)+p(c_2)\ell_2(x)+\cdots +p(c_n)\ell_n(x)</mrow>
          </md>
          which is the Lagrange interpolation expansion of <m>p(x)</m>.
        </p>
      </statement>
    </example>
<p>
  How will you define the othogonal projection of a vector onto another vector
  in an inner product space?
</p>

 <activity>
      <p>
        <em>Notion of orthogonal projection and reflection in an inner product space.</em>
      </p>
      <p>
        Note that the concepts of Gram–Schmidt orthogonalization, orthogonal projection, 
        and reflection can be naturally extended to an inner product space 
        <m>(V, \langle \cdot, \cdot \rangle)</m>. 
        Explore how these notions generalize in such spaces, and implement 
        solutions to related problems using Sage.
            </p>
    </activity>

<definition xml:id="def-inp-orthogonal-projection">
  <statement>
    <p>
    Let <m>(V,\innprod{.}{.})</m> be an inner product space. Suppose <m>u,v\in V</m> with 
    <m>u\neq 0</m>. Then the <em>orthogonal projection</em> of <m>v</m> onto <m>u</m> is defined as
    <me>
      \proj_u(v):= \frac{\inner{v}{u}}{\inner{u}{u}}u.
    </me>
    </p>
  </statement>
  </definition>
  <exercise>
    <statement>
      <p>
        Let <m>V=\mathcal{P}_2(\R)</m> with inner product
        <m>\inner{p}{q}=\int_0^1 p(x)q(x)dx</m>.
        Find the orthogonal projection of
        <m>p(x)=x^2</m> onto <m>u(x)=x+1</m>.
      </p>
    </statement>
  </exercise>

  
  <definition xml:id="def-inp-orthogonal-projection-onto-subspace">
    <title>Projection onto a subspace</title>
    <p>
      Let <m>V</m> be an inner product space and <m>W\leq V</m>,
      a finite dimensional subspace of <m>V</m>.
      Let <m>\{u_1,\ldots, u_k\}</m> be an orthonormal basis of <m>W</m>.
      Suppose <m>v\in V</m>.
      Similar to <xref ref="subspace-projection1">definition</xref>,
      we can define the orthogonal projection of <m>v</m> onto <m>W</m> as
      <me>
        \proj_W(v):=  \inner{v}{u_1}u_1+\inner{v}{u_2}u_2+\cdots+\inner{v}{u_k}u_k
      </me>.
    </p>
    </definition>

    <exercise xml:id="inp-7-1-17">
      <statement>
        <p>
          Find the orthogonal projection of vector
          <m>b=\begin{bmatrix}1\\2\\3\\4 \end{bmatrix}</m> onto the subspace spanned by three vectors <m>\left\{\begin{bmatrix}1\\-1\\0\\1 \end{bmatrix} , \begin{bmatrix}0\\1\\1\\-1 \end{bmatrix} , \begin{bmatrix}1\\1\\-1\\0 \end{bmatrix} \right\}</m>.
        </p>
      </statement>
    </exercise>
 
  <definition xml:id="def-inp-set-perp">
    <statement>
      <p>
     Let <m>(V,\innprod{.}{.})</m> be an inner product space over <m>\R</m> and 
     <m>U\subset V</m>. The the <em>orthogonal complement</em> of <m>U</m> in <m>V</m> is defined as
     <me>
       U^{\perp}=\{v\in V: \innprod{u}{v}=0 \text{ for all } u\in U\}.
     </me>   
      </p>
    </statement>
  </definition> 
  <exercise xml:id="ex1-inp-set-perp">
    <p>
   Let <m>(V,\innprod{.}{.})</m> be an inner product space over <m>\R</m> and 
     <m>U\subset V</m>. Show that <m>U^{\perp}</m> is a subspace of <m>V</m>.   
    </p>
  </exercise>
  <example>
    <statement>
      <p>
        Let <m>A</m> be a symmetric <m>n\times n</m> and positive definite matrix.
        Define an inner product on <m>\R^n</m> as <m>\innprod{u}{v}=v^TAu</m>. Let <m>U=\{e_1\}</m>. 
        Then 
        <me>
          U^{\perp}=\{v\in \R^n: \innprod{e_1}{v}=0\}=\{v\in \R^n: v^TAe_1=0\}.
        </me> 
        Since <m>e_1=(1,0,\ldots,0)^T</m>, <m>Ae_1</m> is the first column of <m>A</m>. This implies that
       <m>U^{\perp}</m> is the set of vector orthogonal to the first column of <m>A</m> 
       with respect to the standard inner product.
      Since the first column of <m>A</m> is nonzero, <m>\dim(U^{\perp})=n-1</m>.  
        It is easy to see that <m>U^{\perp}</m> is the set of vector orthogonal to the first column of <m>A</m> 
       with respect to the standard inner product.
      </p>
    </statement>
  </example>
  <exercise>
    <p>
      Let <m>{\cal P}_2(\R)</m> with discrete inner product 
      <me>\innprod{p}{q}=p(0)q(0)+p(1)q(1)+p(2)q(2).</me>
      Find <m>\{1\}^{\perp}</m> and <m>\{x\}^{\perp}</m>.    
    </p> 
  </exercise>

  <theorem xml:id="thm-inp-orthogonal-complement">
    <statement>
      <p>
        Let <m>(V,\innprod{.}{.})</m> be an inner product space over <m>\R</m> and 
        <m>W\leq V</m> be a finite dimensional subspace of <m>V</m>. Then
        <ol>
          <li>
            <p>
              <m>W^{\perp}</m> is a subspace of <m>V</m> and <m>V=W\oplus W^{\perp}</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>V</m> is finite dimensional, then <m>\dim(V)=\dim(W)+\dim(W^{\perp})</m>
            </p>
          </li>
          <li>
            <p>
            If <m>\dim{V}=n</m>, then <m>{W^{\perp}}^{\perp}=W</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>
  
  <proof>
    <p>
      (1) <m> W^{\perp}</m> is subspace follows from 
      <xref ref="ex1-inp-set-perp"/>. 
    </p>
    <p>
       Let <m>u\in W\cap  W^{\perp}</m>. Then <m>\innprod{u}{u}=0</m>.
      Hence the only vector in both <m>W</m> and <m>W^{\perp}</m> is the zero vector.
      Let <m>x\in V</m>. We need to show that there exist <m>w\in W</m> and <m>v\in W^{\perp}</m>
      such that <m>x=w+v</m>.
      Let <m>\{u_1,\ldots, u_k\}</m> be an orthonormal basis of <m>W</m>.
      Define
      <me>
        w = \innprod{x}{u_1}u_1+\innprod{x}{u_2}u_2+\cdots +\innprod{x}{u_k}u_k.
      </me>
      Now define <m>v=x-w</m>.
      For each <m>i=1,2,\ldots, k</m>, we have
      <md>
        <mrow>\innprod{v}{u_i} \amp = \innprod{x-w}{u_i} </mrow>
        <mrow>\amp = \innprod{x}{u_i}-\innprod{w}{u_i} </mrow>
        <mrow>\amp = \innprod{x}{u_i}-\inner{x}{u_i}\underbrace{\innprod{u_i}{u_i}}_{1} - \sum_{j\neq i}\inner{x}{u_j}\underbrace{\innprod{u_j}{u_i}}_{0} </mrow>
        <mrow>\amp = 0. </mrow>
      </md>
      Hence <m>v\in W^{\perp}</m> and the result follows.
    </p>
    <p>
      (2) Follows from (1).
    </p>
    <p>
      (3) Let <m>W^{\perp} =U</m>. Then by (2), we have
      <me>
        \dim(V)=\dim(W)+\dim(U).
      </me>
      Similarly,
      <me>
        \dim(V)=\dim(U^{\perp})+\dim(U).
      </me>
      Hence <m>\dim(W)=\dim(U^{\perp})</m>.
      It remains to show that <m>W\subseteq U^{\perp}</m> and <m>U^{\perp}\subseteq W</m>.
      Let <m>w\in W</m> and <m>u\in U</m>. Then <m>\innprod{w}{u}=0</m> since <m>U=W^{\perp}</m>.
      This shows that <m>W\subseteq U^{\perp}</m>.
      Now let <m>y\in U^{\perp}</m>.
      We need to show that <m>y\in W</m>.
      By (1), there exist <m>w\in W</m> and <m>u\in U</m> such that <m>y=w+u</m>.
    </p>
  </proof>
<p>
  If <m>W</m> is a finite dimensional subspace of an inner product space <m>V</m>. Then the subspace 
  <m>W^{\perp}</m> is called the <em>orthogonal complement</em> of <m>W</m> in <m>V</m>. 
  Since <m>V=W\oplus W^{\perp}</m>, every vector <m>x\in V</m> can be uniquely written as 
  <m>x=w+v</m> with <m>w\in W</m> and <m>v\in W^{\perp}</m>. The vector <m>w</m> is called the 
  <em>orthogonal projection</em> of <m>x</m> onto <m>W</m> and is denoted by <m>\proj_W(x)</m>. 

</p>
<p>
  Futhermore, if <m>\{u_1,\ldots, u_k\}</m> is an orthonormal basis of <m>W</m>, then
  <me>
    \proj_W(x)=\innprod{x}{u_1}u_1+\innprod{x}{u_2}u_2+\cdots +\innprod{x}{u_k}u_k.
  </me>
</p>
<p>
  The following theorem shows that the orthogonal projection is the best approximation of 
  a vector in <m>V</m> by a vector in <m>W</m>.
</p>
  <theorem xml:id="thm-inp-approximation">
    <title>Approximation Theorem</title>
    <statement>
      <p>
        Let <m>(V,\innprod{.}{.})</m> be an inner product space over <m>\R</m> and 
        <m>W\leq V</m> be a finite dimensional subspace of <m>V</m>. 
        Given <m>x\in V</m>, let <m>w=\proj_W(x)</m>.
        Then for all <m>u\in W</m>, <m>\norm{x-w}\leq \norm{x-u}</m>.
      </p>
    </statement>
  </theorem>
  
  <proof>
    <p>
      Let <m>u\in W</m>. Then <m>u-w\in W</m>.
      Since <m>x-w\in W^{\perp}</m>, we have <m>\innprod{x-w}{u-w}=0</m>.
      Hence by the Pythagorean theorem,
      <md>
        <mrow> \norm{x-u}^2\amp =\norm{(x-w)+(w-u)}^2</mrow>
        <mrow> \amp =\norm{x-w}^2+\norm{u-w}^2\geq \norm{x-w}^2.</mrow>
      </md>
      The result follows.
    </p>
  </proof>
<p>
  The Approximation Theorem shows that the orthogonal projection of a vector <m>x</m> onto a 
  subspace <m>W</m> is the best approximation of <m>x</m> by a vector in <m>W</m>. This result 
  has important applications in numerical analysis and scientific computing. 
</p>

  <example xml:id="eg-inp-fourier">
    <title>Fouries Coefficients</title>
      <statement>
      <p> 
        Let us explore one such application from Fourier analysis. Define an inner product on
  the space of continuous real-valued functions on <m>[-\pi, \pi]</m> as
  <me>
    \innprod{f}{g}=\frac{1}{\pi}\int_{-\pi}^{\pi} f(x)g(x) dx.
  </me>  
  Consider the subspace 
  <me>F_n=\text{span}\{1, \cos x, \sin x, \cos 2x, \sin 2x, \ldots, \cos nx, \sin nx\}.</me> 
  It is easy to check that the set <m>F_n</m> is orthgonal with respect to the above inner product.
  Also 
  <md>
    <mrow> \norm{1}^2\amp =\innprod{1}{1}=2\pi,</mrow>
    <mrow> \norm{\cos kx}^2\amp =\frac{1}{\pi}\int_{-\pi}^{\pi} \cos^2{kx}\,dx=\pi,</mrow>
    <mrow> \norm{\sin kx}^2\amp =\frac{1}{\pi}\int_{-\pi}^{\pi} \sin^2{kx}\,dx=\pi,</mrow>
  </md>
for <m>k=1,\ldots, n</m>.
  Hence an orthonormal basis of <m>F_n</m> is given by
  <me>
    \left\{\frac{1}{\sqrt{2\pi}}, \frac{\cos x}{\sqrt{\pi}}, \frac{\sin x}{\sqrt{\pi}}, \ldots, 
    \frac{\cos nx}{\sqrt{\pi}}, \frac{\sin nx}{\sqrt{\pi}}\right\}.
  </me>
  Given a continuous function <m>f\colon [-\pi, \pi]\to \R</m>, 
  we want to find the best approximation of <m>f</m>. Define 
  <md>
    <mrow>a_0 \amp =\frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)\,dx</mrow>
    <mrow>a_k \amp =\frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)\cos kx \,dx</mrow>
    <mrow>b_k \amp =\frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)\sin kx \,dx</mrow>
  </md>
  Then by the Approximation Theorem, the orthogonal projection of <m>f</m> onto <m>F_n</m> is given by
  <me>
    \proj_{F_n}(f)=a_0+\sum_{k=1}^n \left(a_k \cos kx + b_k \sin kx\right).   
  </me>
  The coefficients <m>a_0, a_k, b_k</m> are called the <em>Fourier coefficients</em> of <m>f</m>. Thus 
  we have shown that the Fourier coefficients give the best approximation of a function 
  by a trigonometric polynomial of degree at most <m>n</m> 
  <me>
    f(x)\approx a_0+\sum_{k=1}^n \left(a_k \cos kx + b_k \sin kx\right)
  </me>
</p>
  </statement>
  </example>
  
  <subsection xml:id="subsec-linear-functionals">
  <title>Linear Functionals</title>
  <p>
    Let <m>(V,\innprod{.}{.})</m> be an inner product space over <m>\R</m>. Fix a vector <m>a\in V</m> and 
    define <m>f_a\colon V\to \R</m> as <m>f_a(x)=\innprod{x}{a}</m>. It follows from from properties of 
    inner product that <m>f_a</m> is a linear map. (why?) 
  </p>
<definition xml:id="def-linear-functional">
  <statement>
    <p>
      Let <m>\innprod{.}{.}</m> be an inner product space over <m>\R</m>. Any liear map 
      <m>f\colon V \to \R</m> is called a <em>linear function.</em>
    </p>
  </statement>
</definition>

<p>
  Recall that we have characterized all linear maps from <m>\R^n\to \R</m>. 
In particular, if <m>T\colon \R^n\to \R</m> 
is a linear map, them there exist a vector <m>a=(a_1,\ldots, a_n)</m> such that 
<me>
  T(x)=\sum a_ix_i=x \cdot a.
</me>
 The question is can we characterize linear functionals on an inner product space. 
</p>

<theorem xml:id="thm-riesz">
  <title>Riesz Representation Theorem</title> 
  <statement>
    <p>
      Let <m>\innprod{.}{.}</m> be an inner product space over <m>\R</m>. 
      Given a linear map <m>f\colon V \to \R</m>, there exists a unique <m>y
\in V</m> such that <m>f(x) = \innprod{x}{y}</m> for all <m>x \in V</m>.
    </p>
  </statement>
</theorem>
<proof>
  <p>
    Let <m>\{u_1,\ldots, u_n\}</m> be an orthonormal basis of <m>V</m>.
    Define 
    <me>
      y = f(u_1)u_1+f(u_2)u_2+\cdots +f(u_n)u_n.
    </me>
    For any <m>x\in V</m>, we can write <m>x=c_1u_1+c_2u_2+\cdots +c_nu_n</m>.
    Hence
    <md>
      <mrow>\innprod{x}{y} \amp = \innprod{c_1u_1+c_2u_2+\cdots +c_nu_n}{f(u_1)u_1+f(u_2)u_2+\cdots +f(u_n)u_n} </mrow>
      <mrow>\amp = c_1f(u_1)\underbrace{\innprod{u_1}{u_1}}_{1}+c_2f(u_2)\underbrace{\innprod{u_2}{u_2}}_{1}+\cdots +c_nf(u_n)\underbrace{\innprod{u_n}{u_n}}_{1} </mrow>
      <mrow>\amp = c_1f(u_1)+c_2f(u_2)+\cdots +c_nf(u_n). </mrow>
    </md>
    Since <m>f</m> is linear, we have
    <md>
      <mrow> f(x) \amp =f(c_1u_1+c_2u_2+\cdots +c_nu_n)</mrow>
      <mrow> \amp =c_1f(u_1)+c_2f(u_2)+\cdots +c_nf(u_n).</mrow>
    </md>
    Hence <m>f(x)=\innprod{x}{y}</m>.
  </p>
  <p>
    To show uniqueness, suppose there exists <m>z\in V</m> such that 
    <m>f(x)=\innprod{x}{z}</m> for all <m>x\in V</m>. Then for each <m>i=1,2,\ldots, n</m>, we have
    <me>
      f(u_i)=\innprod{u_i}{y}=\innprod{u_i}{z}.
    </me>
    Hence
    <me>
      \innprod{u_i}{y-z}=0
    </me>
    for all <m>i=1,2,\ldots, n</m>.
    Since <m>\{u_1,\ldots, u_n\}</m> is a basis of <m>V</m>, we have <m>y-z=0</m> or <m>y=z</m>.  
  </p>
</proof>

<p>
  Can we characterize linear maps from a finite dimesional inner product space <m>V</m> 
  to a finite dimensional inner product space <m>W</m>?
</p>

<theorem xml:id="thm-inp-linear-map-characterization">
  <statement>
    <p>
      Let <m>(V,\innprod{.}{.})</m> and <m>(W,\innprod{.}{.})</m> be finite dimenional inner 
      product spaces over <m>\R</m>. Let <m>T\colon V\to W</m> be a linear map. 
      Let <m>\alpha=\{u_1,\ldots, u_n\}</m> be an orthonormal basis of <m>V</m> and 
      <m>\beta =\{v_1,\ldots, v_m\}</m> an orthonormal basis of  <m>W</m>. Then the matrix of <m>T</m>
      with respect to these bases is given by  
      <me>
        [T]_\alpha^\beta=\begin{bmatrix}
          \innprod{T(u_1)}{v_1} \amp \innprod{T(u_2)}{v_1} \amp \cdots \amp \innprod{T(u_n)}{v_1} \\
          \innprod{T(u_1)}{v_2} \amp \innprod{T(u_2)}{v_2} \amp \cdots \amp \innprod{T(u_n)}{v_2} \\
          \vdots \amp \vdots \amp \ddots \amp \vdots \\
          \innprod{T(u_1)}{v_m} \amp \innprod{T(u_2)}{v_m} \amp \cdots \amp \innprod{T(u_n)}{v_m}
          \end{bmatrix}.
      </me>
    </p>
  </statement>
</theorem>
<proof>
  <p>
    Sine <m>\alpha=\{u_1,\ldots,u_n\}</m> is an orthonormal basis of <m>V</m>,
    <me>
      T(u_j)=\sum_i\innprod{T(u_i)}{v_i}v_i.
    </me>
    This shows that the <m>j</m>-th column of <m>[T]_\alpha^\beta</m> is
    <me>
      \begin{bmatrix}
        \innprod{T(u_j)}{v_1} \\
        \innprod{T(u_j)}{v_2} \\
        \vdots \\
        \innprod{T(u_j)}{v_m}
      \end{bmatrix}.
    </me>
    In paticular, the <m>(i,j)</m>-th entry of <m>[T]_\alpha^\beta</m> is
    <me>
      \innprod{T(u_j)}{v_i}.
    </me>     
  </p>
</proof>
<p>
  The above theorem shows that a linear map between two finite dimensional inner product spaces 
  is completely determined by the images of an orthonormal basis of the domain space.
</p>
<p>
  Now suppose <m>W=V</m> in <xref ref="thm-inp-linear-map-characterization"/> and <m>T\colon V\to V</m> be a linear map on <m>V</m>.
  Let <m>\alpha=\{u_1,\ldots, u_n\}</m> be an orthonormal basis of <m>V</m>.
  Then the matrix of <m>T</m> with respect to this basis is given by  
  <me>
     [T]_\alpha=\begin{bmatrix}
          \innprod{T(u_1)}{u_1} \amp \innprod{T(u_2)}{u_1} \amp \cdots \amp \innprod{T(u_n)}{u_1} \\
          \innprod{T(u_1)}{u_2} \amp \innprod{T(u_2)}{u_2} \amp \cdots \amp \innprod{T(u_n)}{u_2} \\
          \vdots \amp \vdots \amp \ddots \amp \vdots \\
          \innprod{T(u_1)}{u_n} \amp \innprod{T(u_2)}{u_n} \amp \cdots \amp \innprod{T(u_n)}{u_n}
          \end{bmatrix}.
  </me>    
</p>
<p>
  Using the properties of inner product, it is easy to see that <m>[T]_\alpha</m> is a symmetric matrix
if and only if 
<me>\innprod{T(u_i)}{u_j}=\innprod{T(u_j)}{u_i}=\innprod{u_i}{T(u_j)}</me> for all <m>i,j</m>.
This motivates the following definition.
</p>
<definition xml:id="def-inp-symmetric-LT">
  <statement>
    <p>
     A linear map <m>T\colon V\to V</m> on an inner product space <m>(V,\innprod{.}{.})</m> is called
     <em>symmetric</em> if
     <me>
       \innprod{T(u)}{v}=\innprod{u}{T(v)}
     </me>
     for all <m>u,v\in V</m>.  
    </p>
  </statement>
</definition>
<p>
  Recall that we defined a linear map <m>T\colon \R^n\to \R^n</m> assocciated with an <m>n\times n</m> real 
  matrix <m>A</m> as <m>T_A(x)=Ax</m>. Also the matrix of <m>T_A</m> with respect to the standard basis is 
  <m>A</m>.   This we can define an <m>n\times n</m> matrix <m>A</m> to be symmetric if the linear map
  <m>T_A\colon \R^n\to \R^n</m> is symmetric with respect to the standard inner product (dot product) on <m>\R^n</m>.
</p>
<p>
  It is easy to see that an <m>n\times n</m> real matrix <m>A</m> is symmetric in the usual sense
  (i.e., <m>A=A^T</m>) if and only if the linear map <m>T_A\colon \R^n\to \R^n</m> is 
  symmetric with respect to the standard inner product on <m>\R^n</m>.
</p>

<p>
  Next we revist the orthogonal linear transformation defined defined in the last chapter and 
  give an equivalent definition using inner products. We have the following equivalent definition.
</p>
<definition xml:id="def-inp-orthogonal-maps">
  <statement>
    <p>
      Let <m>(V,\innprod{.}{.})</m> be a finite dimensional inner product space over <m>\R</m>.
      A linear map <m>T\colon V\to V</m> is an <em>orthogonal linear transformation</em> if and only if
      <me>
        \innprod{T(u)}{T(v)}=\innprod{u}{v}
      </me>
      for all <m>u,v\in V</m>.  
    </p>
  </statement>
</definition>
<p>
  In particular orthogonal linear transformations are precisely those linear maps that 
  preserve the inner product. It is easy to see that a linear map 
  <m>T\colon V\to V</m> is an orthogonal  linear transformation if and only if
  <me>
    \norm{T(u)}=\norm{u}.
  </me>
We have the following equivalent characterization of orthogonal linear transformations.
</p>   

<theorem xml:id="thm-inp-orthogonal-maps">
  <statement>
    <p>
      Let <m>(V,\innprod{.}{.})</m> be a finite dimensional inner product space over <m>\R</m>.  
    and <m>T\colon V\to V</m> be a linear map. 
    Then the following are equivalent.
    <ol>
      <li>
        <p>
          <m>T</m> is an orthogonal linear transformation.        
    </p>
  </li>
  <li>
    <p>
      <m>\norm{T(x)}=\norm{x}</m> for all <m>x\in V</m>.
    </p>
  </li>
    <li>
      <p>
        <m>T</m> maps every orthonormal basis of <m>V</m> to an orthonormal basis of <m>V</m>. That is,
        if <m>\{u_1,\ldots, u_n\}</m> is an orthonormal basis of <m>V</m>, then 
        <m>\{T(u_1),\ldots, T(u_n)\}</m> is also an orthonormal basis of <m>V</m>.
      </p>
    </li>
  </ol>
</p>
  </statement>
</theorem>
<proof>
  <p>
    (1) <m>\Rightarrow</m> (2): Follows from the definition of orthogonal linear transformation.  
  </p>
  <p>
    (2) <m>\Rightarrow</m> (3): Let <m>\{u_1,\ldots, u_n\}</m> be an orthonormal basis of <m>V</m>.
    For each <m>i,j=1,2,\ldots, n</m>, we have
    <md>
      <mrow>\innprod{T(u_i)}{T(u_j)} \amp = \frac{1}{2}(\norm{T(u_i)+T(u_j)}^2-\norm{T(u_i)}^2-\norm{T(u_j)}^2) </mrow>
      <mrow>\amp = \frac{1}{2}(\norm{u_i+u_j}^2-\norm{u_i}^2-\norm{u_j}^2) </mrow>
      <mrow>\amp = \innprod{u_i}{u_j} </mrow>
    </md>
    This shows that <m>\{T(u_1),\ldots, T(u_n)\}</m> is an orthonormal set.
    Since <m>T</m> is a linear map, 
    <m>\{T(u_1),\ldots, T(u_n)\}</m> spans <m>\im(T)</m>. Also <m>\{T(u_1),\ldots, T(u_n)\}</m> is 
    linearly independent. Hence 
    <me>n=\dim(\im(T))\leq \dim(V)=n.</me> This implies 
    <m>\im(T)=V</m>.
    This shows that <m>\{T(u_1),\ldots, T(u_n)\}</m> is an orthonormal basis of <m>V</m>.
  </p>
  <p>
    (3) <m>\Rightarrow</m> (1): Let <m>u,v\in V</m>. 
    Since <m>\{u_1,\ldots, u_n\}</m> is an orthonormal basis of <m>V</m>, we can write
    <m>u=c_1u_1+c_2 u_2+\cdots +c_nu_n</m> and
    <m>v=d_1u_1+d_2 u_2+\cdots +d_nu_n</m>. Then it is easy to see that   
    <me>
      \innprod{u}{v}=\sum c_j d_i.
    </me>
    
    Hence
    <md>
      <mrow>\innprod{T(u)}{T(v)} \amp = \innprod{\sum c_iT(u_i)}{\sum d_jT(u_j)} </mrow>
      <mrow>\amp = \sum_i\sum_j c_id_j\innprod{T(u_i)}{T(u_j)} </mrow>
      <mrow>\amp = \sum c_id_i=\innprod{u}{v}.</mrow>
    </md>
  </p>      
</proof>

<p>
  Now let us see what happens to the matrix of an orthogonal linear transformation with 
  respect to an orthonormal basis. Let <m>T\colon V\to V</m> be an orthogonal linear 
  transformation on a finite dimensional inner product space <m>(V,\innprod{.}{.})</m>.
  Let <m>\alpha=\{u_1,\ldots, u_n\}</m> be an orthonormal basis of <m>V</m>.
  Then the matrix of <m>T</m> with respect to this basis is given by  
  <me>
     [T]_\alpha=\begin{bmatrix}
          \innprod{T(u_1)}{u_1} \amp \innprod{T(u_2)}{u_1} \amp \cdots \amp \innprod{T(u_n)}{u_1} \\
          \innprod{T(u_1)}{u_2} \amp \innprod{T(u_2)}{u_2} \amp \cdots \amp \innprod{T(u_n)}{u_2} \\
          \vdots \amp \vdots \amp \ddots \amp \vdots \\
          \innprod{T(u_1)}{u_n} \amp \innprod{T(u_2)}{u_n} \amp \cdots \amp \innprod{T(u_n)}{u_n}
          \end{bmatrix}=\begin{bmatrix}
          a_{11} \amp a_{21} \amp \cdots \amp a_{n1} \\
          a_{21} \amp a_{22} \amp \cdots \amp a_{21} \\
          \vdots \amp \vdots \amp \ddots \amp \vdots \\
          a_{n1} \amp a_{n2}\amp \cdots \amp a_{nn}
          \end{bmatrix} =A.
  </me>   
  Let us write the columns of <m>A</m> as <m>A=[c_1 \; c_2 \; \cdots \; c_n]</m>.
    Since <m>\{T(u_1),\ldots, T(u_n)\}</m> is an orthonormal basis of <m>V</m>, it follows that
 <md>
    <mrow> \delta_{ij} \amp=\innprod{T(u_i)}{T(u_j)} </mrow>
    <mrow>\amp=\innprod{\sum_k a_{ki}u_k}{\sum_s a_{sj}u_s} </mrow>
    <mrow>\amp=\sum_k\sum_s a_{ki}a_{sj}=c_i\cdot c_j </mrow>
  </md>
  the dot product of <m>c_i</m> and <m>c_j</m>.
  This shows that the columns of <m>A</m> form an orthonormal set in <m>\R^n</m>.
  This suggest the following definition  
  of an othogonal matrix.
</p>

<definition xml:id="def-inp-orthogonal-matrix">
  <statement>
    <p>
      A real <m>n\times n</m> matrix <m>A</m> is called an <em>orthogonal matrix</em> if
      its columns form an orthonormal set in <m>\R^n</m>. This is, if <m>A=[c_1 \; c_2 \; \cdots \; c_n]</m>, then
      <me>
        c_i \cdot c_j = \delta_{ij}
      </me>
      for all <m>i,j=1,2,\ldots, n</m>. Equivalently, <m>A</m> is an orthogonal matrix if <m>A^TA=I_n</m>. 
    </p>
  </statement>
</definition>
 

</subsection>
</section>