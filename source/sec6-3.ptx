<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec6-3" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Orthogonal Diagonalizations</title>

 <introduction>
  <p>
    Recall the concept of diagonalization of a square matrix. We have seen that an 
    <m>n\times n</m> matrix <m>A</m> is diagonalizable 
    if there is an eigenbasis of <m>\R^n</m>. In this section, we shall explore 
    if we can find an eigenbasis which is also an orthonormal. First of all we shall define 
    what is meaning of an orthogonal matrix. 
  </p>
 </introduction>
  
  
    <theorem xml:id="thm-orthogonal-matrix">
      <statement>
        <p>
          Let <m>P</m> be an <m>n\times n</m> matrix.
          Then the following are equivalent:
       <ol>
        <li>
           <p>
       <m>P</m> is non-singular and <m>P^{-1}=P^T</m>.
        </p>
      </li>
       <li>
         <p>
       The rows of <m>P</m> are orthonormal vectors in <m>\R^n</m>. 
        </p>
      </li>
      <li>
        <p>
       The columns of <m>P</m> are orthonormal vectors in <m>\R^n</m>.
        </p>  
        </li>
       </ol>
      </p>
      </statement>

      <proof>
         <p>
            Assume that <m>P^{-1}=P^T</m>. This implies <m>PP^T=I</m>. Let the columns 
            of <m>P</m> are <m>p_1, p_2,\ldots, p_n</m>. Then <m>\{p_1,\ldots, p_n\}</m> is 
            linearly independent. It is easy to see that the <m>ij</m>-th entry of <m>PP^T</m> is 
            <m>p_i\cdot p_j</m>. Hence we have <m>p_i\cdot p_j=\delta_{ij}=1</m> if <m>i=j</m> 
            and 0 otherwise. This proves rows of <m>P</m> and orthogonal and hence columns 
            of <m>P</m> are orthogonal. The converse is easy.
         </p>
      </proof>

    </theorem>

  

    <definition xml:id="def-orthogonal-matrix">
      <statement>
        <p>
          A square matrix <m>P</m> is called an orthogonal matrix if it satisfies any one
          (and hence all) the conditions of Theorem
          <xref ref="thm-orthogonal-matrix"></xref>.
        </p>
      </statement>
    </definition>
  


     <example>
      <statement>
        <p>
       <ol>
        <li>
          <p>
            The matrix <m>\begin{pmatrix}\cos \theta \amp -\sin\theta\\\sin\theta \amp \cos\theta \end{pmatrix}</m> 
            is an orthogonal matrix.
          </p>
        </li>
        <li>
          <p>
            <m>\left(\begin{array}{rrr} -\frac{1}{3} \, \sqrt{3} \amp \sqrt{\frac{2}{3}} \amp 0 \\ 
              \frac{1}{3} \, \sqrt{3} \amp \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp -\sqrt{\frac{1}{2}} \\ 
              \frac{1}{3} \, \sqrt{3} \amp \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp \sqrt{\frac{1}{2}} 
              \end{array} \right)</m> 
            is an orthogonal matrix.
        
          </p>
        </li>
       </ol>
         </p>
      </statement>
    </example>
    
  

  
    <definition xml:id="def-orthogonally-diagonalizable">
      <statement>
        <p>
          An <m>n\times n</m> matrix is called
          <em>orthogonally diagonalizable </em>
          if there exists an orthogonal matrix <m>P</m> such that
          <m>P^{-1}AP=P^TAP</m> is a diagonal matrix.
        </p>
      </statement>
    </definition>


  <p>
    It is easy to easy to see that if <m>P</m> and <m>Q</m> are 
    orthgogonal matrices then <m>PQ</m> is also orthogonal. (why?)
  </p>

      <definition xml:id="def-orthogonally-similar-matrics">
      <statement>
        <p>
          Two  <m>n\times n</m> matrices <m>A</m> and <m>B</m> are called
          <em>orthogonally similar </em>
          if there exists an orthogonal matrix <m>P</m> such that
          <m>B =P^{-1}AP=P^TAP</m> is a diagonal matrix.
        </p>
      </statement>
    </definition>

<p>
  Thus an orthogonally diagonally matrix is ortghogonally similar to a diagonal matrix.
</p>

<p>
  Suppose a matrix <m>A</m> is orthogonally diagonalizable. That is <m>P^TAP=D</m>, a digonal matrix. 
  This means <m>A=PDP^T</m>. Hence 
  <me>
    A^T=(PDP^T)^T=PD^TP^T=PDP^T=A.
  </me>
  Thus if <m>A</m> is orthogonally diagonalizable then <m>A</m> must be symmetric. 
</p>
 
<lemma xml:id="lemma-orthogonality-eigenvectors1">
      <statement>
        <p>
          Let <m>A</m> be a symmetric matrix and <m>\lambda_1</m> and
          <m>\lambda_2</m> are distinct eigenvalues of <m>A</m>.
          If <m>v_1</m> and <m>v_2</m> are eigenvectors corresponding to
          <m>\lambda_1</m> and <m>\lambda_2</m> respectively.
          Then <m>v_1</m> and <m>v_2</m> are orthogonal.
        </p>
      </statement>

      <proof>
        <p>
          We have 
          <me>
            (\lambda_1 v_1)\cdot v_2 = (Av_1) \cdot (v_2)=
            {(Av_1)}^T(v_2)=v_1^T A^Tv_2=v_1^T(\lambda_2 v_2).
          </me>
          This implies <m>(\lambda_1-\lambda_2)(v_1\cdot v_2)=0</m>. Since <m>\lambda_1\neq \lambda_2</m>, 
          we have <m>v_1\cdot v_2=0</m>.
        </p>
      </proof>
    </lemma>
  
<p>
  The following therem shows that every real symmetric matrix is orthogonally 
  diagonalizable. 
</p>

<theorem xml:id="thm-6-4-6">
  <statement>
    <p>
          Let <m>A</m> be an <m>n\times n</m> real matrix.
          Then the following are equivalent.
        </p>
        <ol>
          <li>
            <p>
             <m>A</m> has an orthonormal set of eigenvectors.
            </p>
          </li>
        <li>
          <p>
            <m>A</m> is orthogonally diagonalizable.
          </p>
        </li>
      <li>
        <p>
           <m>A</m> is symmetric.
        </p>
      </li>  
      </ol>
    </statement>

  <proof>
   <p>
    <term><m>(1\implies 2)</m></term>
   </p> 
      <p>
        Let <m>v_1,\ldots, v_n</m> be orthogonormal eigenvectors of <m>A</m> such that 
  <m>Av_i=\lambda_i v_i</m>. Then 
  <m>P=\begin{bmatrix} v_1\amp v_2\cdots v_n\end{bmatrix}</m> is orthogonal. Hence 
  <me>
    P^TAP={\rm diag}(\lambda_1,\ldots,\lambda_n)=D. 
  </me>
  Hence <m>A</m> is orthogonally diagonalizable. 
      </p>
 
       <p>
    <term><m>(2\implies 1)</m></term>
   </p> 
  <p>
    Suppose there exists an orthogonal matrix <m>P</m> susch that 
    <m>P^{-1}AP=D</m>. Then
    <m>AP=PD</m>. Let <m>D={\rm diag}\{\lambda_1,\ldots,\lambda_n\}</m> and 
    <m>v_1,\ldots, v_n</m> be columns of <m>P</m>, then <m>\{v_1,\ldots, v_n\}</m>
    is an orthonormal basis of <m>\R^n</m>. Also <m>AP=PD</m> implies <m>Av_i=\lambda_i v_1</m>. 
    Hence <m>\beta</m> is an orthonormal eigenbasis of <m>A</m>. 

  </p>

   <p>
    <term><m>(2\implies 3)</m></term>
   </p> 
      <p>
    If <m>A</m> is orthogonally diagonalizable with <m>P^TAP=D</m> then 
    <me>
      A^T={(PDP^T)}^T=PDP^T=A.
    </me>
    Hence <m>A</m> is symmetric. 
      </p>
    
       <p>
    <term><m>(3\implies 2)</m></term>
   </p> 
    <p>
      We prove this result using induction on <m>n</m>.  For <m> n=1</m>. 
 Let <m>A=[\alpha]</m>. Then  <m>\{1\}</m> 
  is an orthonormal basis of <m>\R</m> and it is also an eigenvector.
    </p>
<p>
  Assume that the result is true for <m>n-1</m>. That is if <m>A</m> is  an <m>(n-1)\times (n-1)</m> 
  real symmetric matrix then it is orthogonally diagonalizable. 
</p>

<p>
  Let us prove the result for <m>n</m>. Let <m>A</m> be an <m>n\times n</m> real symmetric matrix.  
  By the fundamental theorem of algebra,  we know that every real polynomial of has a root in 
  <m>\mathbb{C}</m>. Hence the characteristic polynomila of <m>A</m> has a complex characteristics root. By 
  <xref ref="thm-hermitain-eigenvalues"/>, all eigenvalues of <m>A</m> are real. Thus <m>A</m> has 
  a real eigenvalue, say, <m>\lambda</m>. Let <m>u</m> be a unit  eigenvector corresponding to 
  the eigenvalue <m>\lambda</m> and <m>W=\R u</m>. Then <m>W</m> is a one dimensional subspace of 
  <m>\R^n</m>. Hence <m>W^\perp</m> an <m>(n-1)</m>-dimensional subspace of <m>\R^n</m>. 
  Also <m>W</m> is <m>A-</m>invariannt. Hence by <xref ref="exer-6-3-12"/>, <m>W^\perp</m> is 
  <m>A</m>-invariant.   Also <m>\R^n=W\oplus W^\perp</m>.
</p>
<p>
Let <m>\beta = \{u,v_1,\ldots,v_{n-1}\}</m> be an extended orthonormnal basis of <m>A</m>.  
Let <m>P=[u~v_1~\cdots~v_{n-1}]</m>, the orthogonal matrix whose columns are vectors 
<m>u,v_1,\ldots,v_{n-1}</m>.  The 
the matrix <m>M</m> of <m>A</m> with respect to <m>\beta</m> is <m>P^TAP</m> which is 
of the form
<me>
  M=\left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp C
\end{array}\right]
</me>, where <m>C</m> is an <m>(n-1)\times (n-1)</m> real symmetric matrix. (why)? 
Hence by induction, there exists an <m>(n-1)\times (n-1)</m> orthogonal matrix <m>Q</m> 
such that <m>Q^TCQ=D</m>, a diagonal matrix. Hence 
<me>
  M=\left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q
\end{array}\right] \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp D
\end{array}\right] \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q^T
\end{array}\right]
</me>
Thus we have 
<me>
  P^TAP = M = \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q
\end{array}\right] \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp D
\end{array}\right] \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q^T
\end{array}\right].
</me>
This implies 
<me>
  A = P\left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q
\end{array}\right] \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp D
\end{array}\right] \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q^T
\end{array}\right]P^T.
</me>
Define  <m>P_1=P\left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp Q
\end{array}\right]</m> . The <m>P_1</m> is an orthogonal matrix and 
<me>
A=P_1   \left[
\begin{array}{c|c}
\lambda \amp 0 \\ \hline
0 \amp D
\end{array}\right] P_1^T.
</me>
</p>

</proof>

</theorem>
<p>
  The above theorem is called the spectral throem of real symmetric matrix.  
</p>
<theorem xml:id="thm-spectral-symmetric-matrix">
   <title>Spectal Theorem of Symmetric Matrix</title>
  <idx><h>Spectal Theorem of Symmetric Matrix</h></idx>
  <statement>
    <p> 
      A matrix with real entries is orthogonally diagonalizable if and only if it is symmetric.      
    </p>
  </statement>
</theorem>

    <example>
      <statement>
        <p>
          Consider a matrix <m>A=\left(\begin{array}{rrr} 5 \amp  -2 \amp  -4 \\ -2 \amp  8 \amp  -2 \\ -4 \amp  -2 \amp  5 \end{array} \right)</m>.
          Clearly <m>A</m> is symmetric and hence it is orthogonally diagonalizable.
          The characteristic polynomial of <m>A</m> is
          <me>
            \det{(xI-A)}=x^3 - 18x^2 + 81x=x(x-9)^2
          </me>.
        </p>
        <p>
          Hence <m>0, 9, 9</m> are eigenvalues of <m>A</m>.
          Its is easy to find that <m>v_1=(1, 1/2, 1)</m> is an eigenvector corresponding to the eigenvalue 0.
          <m>v_2=(1, 0, -1), v_2=(0, 1, -1/2)</m> are eigenvectors corresponding to eigenvalue 9.
          Hence <m>P:=\left(\begin{array}{rrr} 1 \amp  1 \amp  0 \\ \frac{1}{2} \amp  0 \amp  1 \\ 1 \amp  -1 \amp  -\frac{1}{2} \end{array} \right)</m>.
          Then
          <me>
            P^{-1}AP=\left(\begin{array}{rrr} 0 \amp  0 \amp  0 \\ 0 \amp  9 \amp  0 \\ 0 \amp  0 \amp  9 \end{array} \right)
          </me>
        </p>
      </statement>
    </example>
  


    <problem>
      <statement>
        <p>
          For the following matrices find an orthogonal matrix <m>P</m> such that
          <m>P^{-1}AP</m> is a diagonal matrix.
          <me>
            \begin{pmatrix}2 \amp  -1 \\-1 \amp  1 \end{pmatrix} , \begin{pmatrix}1 \amp  0 \amp  -1\\0 \amp  1 \amp  2\\-1 \amp  2 \amp  5 \end{pmatrix}
          </me>
        </p>
      </statement>
    </problem>

  <theorem xml:id="thm-6-4-10"> 
      <statement>
        <p>
          The following are equivalent for an <m>n\times n</m> matrix <m>A</m>.
        </p>
        <p>
          <ol>
            <li>
              <p>
                <m>A</m> is orthogonal.  
              </p>
            </li>
            <li>
              <p>
                <m>\norm{Ax}=\norm{x}</m> for all <m>x\in \R^n</m>. 
              </p>
            </li>
            <li>
              <p>
                <m>\norm{Ax-Ay}=\norm{x-y}</m> for all <m>x,y\in \R^n</m>.
              </p>
            </li>
            <li>
              <p>
                <m>Ax\cdot Ay = (Ay)^TAx=x\cdot y</m>. This is same as 
                saying <m>A</m> maps an orthogonal basis to an orthogonal basis. 
              </p>
            </li>
          </ol> 
        </p>
      </statement>
      <proof>
        <p>
          Left as an exercise.
        </p>
      </proof>
    </theorem>
   
    <p>
      <term>Distance Preserving maps in <m>\R^n.</m></term>
    </p>

    <p>
      Suppose <m>f\colon \R^n \to \R^n</m> is a map, that preserves the distance, that 
      is <m>\norm{f(x)-f(x)}=\norm{x-y}</m> for all <m>x,y\in \R^n</m>. We would like to 
      study such maps. Let us first look at a spacial case when <m>f</m> fixes the origin.
    </p>
    <lemma xml:id="lem-distance-preserving">
      <statement>
        <p>
          Let <m>f\colon \R^n\to \R^n</m> be a map such that 
          <ol>
            <li>
              <p>
                f(0)=0
              </p>
            </li>
            <li>
              <p>
                <m>\norm{f(x)-f(x)}=\norm{x-y}</m> for all <m>x,y\in \R^n</m>.
              </p>
            </li>
          </ol>
          Then <m>f</m> is an orthogonal map.
        </p>
      </statement>

      <proof>
        <p>
          From (1) and (2) we have 
          <me>
            \norm{f(x)}=\norm{f(x)=f(0)}=\norm{f(x-0)}}=\norm{f(x)},
          </me>
          for all <m>x\in \R^n</m>. Using this we have       
          <me>
            \norm{f(x)-f(y)}^2=\norm{x-y}^2.
          </me>
          Exanding both sides, we get
          <me>
            f(x)\cdot f(y) = x\cdot y
          </me>
          for all <m>x,y</m>. That is, <m>f</m> preserves the dot product. This 
          implies that <m>f</m> maps an orthonormal basis of <m>\R^n</m> to an 
          orthonormal basis of <m>\R^n</m>. In particulatr, <m>\{f(e_i)\}</m> is 
          an orthonormal basis of <m>\R^n</m>, where <m>\{e_i\}</m> is the standard 
          basis. Hence

          <me>
            f(x)=f(\sum x_i e_i)=\sum f(x)\cdot f(e_i) f(e_i)=\sum x\cdot e_i f(e_i)=\sum x_i f(e_i).
          </me>
          This shows that <m>f</m> is a linear map. (why?)
        </p>
      </proof>
    </lemma>
<p>
  Now using the abobve <xref ref="lem-distance-preserving"/>, we can identify all 
  distance preserving maps in <m>\R^n</m>, which is the content of the next theorem.
</p>

<theorem xml:id="thm-distance-preserving">
  <statement>
    <p>
      Let <m>f\colon \R^n\to \R^n</m> be such that <m>\norm{f(x)-f(y)}=\norm{x-y}</m> for 
      all <m>x,y\in \R^n</m>. Then there exists a unique vector <m>x_0\in \R^n</m> and 
      an orthogonal liear map  <m>A</m> such that <m>f(x)=Ax+x_0</m>. In particular <m>f</m> 
      is an affine linear transformation.
    </p>
  </statement>
  <proof>
    <p>
      Let <m>x_0:=f(0)</m> and <m>g(x)=f(x)-x_0</m>. Then it is easy to check that 
      <m>g(0)=0</m> and <m>\norm{g(x)-g(y)}=\norm{x-y}</m> for all <m>x,y</m>. Hence by    
      <xref ref="lem-distance-preserving"/>, <m>g</m> is linear. By <xref ref="thm-6-4-10"/>, 
      <m>g(x)=Ax</m> for some orthogonal linear transformation <m>A</m>. Hence 
      <m>f(x)=Ax+x_0.</m>
    </p>
  </proof>
</theorem>


   
    <definition xml:id="def-square-root">
    <statement>
      Let <m>A</m> be a symmetric positive definite matrix.  
      A <em>square root</em> of <m>A</m> is a matrix <m>B</m> such that
      <me>
        B^2 = A.
      </me>
      The unique <em>symmetric positive definite</em> square root of <m>A</m> 
      is denoted by <m>A^{1/2}</m>.
    </statement>
  </definition>

<p>
 <term> How to find square root of a symmtric positive definte matrix?</term>
</p>

<proposition xml:id="prop-spectral-square-root">
    <statement>
      If <m>A</m> is symmetric and positive definite, then it admits the unique symmetric positive definite square root
      <me>
        A^{1/2} = Q \,\mathrm{diag}(\sqrt{\lambda_1}, \dots, \sqrt{\lambda_n})\, Q^T,
      </me>
      where <m>A = Q \,\mathrm{diag}(\lambda_1,\dots,\lambda_n) Q^T</m> is the spectral decomposition of <m>A</m> with <m>\lambda_i &gt; 0</m>.
    </statement>
    <proof>
      <p>
        Since <m>A</m> is symmetric, it is orthogonally diagonalizable:  
        <me>
          A = Q DQ^T,
        </me>
        with <m>D= \mathrm{diag}(\lambda_1,\dots,\lambda_n)</m>, where all <m>\lambda_i&gt;0</m>.  
        Define <m>D^{1/2} = \mathrm{diag}(\sqrt{\lambda_1},\dots,\sqrt{\lambda_n})</m>.  
        Then
        <me>
          (Q D^{1/2} Q^T)^2 = Q D^{1/2} Q^T Q D^{1/2} Q^T 
          = Q DQ^T = A.
        </me>
        Thus <m>A^{1/2} = QD^{1/2}Q^T</m> is symmetric and positive definite.  
        Uniqueness follows from the strict positivity of eigenvalues.
      </p>
    </proof>
  </proposition>


<subsection xml:id="subsec-affine-LT-Application">
  <title>Applications of Affine Linear Map</title>
  <p>
    Let us look some applications of affine linear transformation to fractals.
  </p>
  <example xml:id="Koch-Curve">
    <title>Koch Curve</title>
    <p>
    The Koch curve is a classic fractal that can be described using the language of
    <term>affine linear transformations</term>. 
    </p>
    
    <p>
    The construction of the Koch curve begins with a single line segment 
    from <m>(0,0)</m> to <m>(1,0)</m> called <term>initiator</term>. 
  </p>
  <p>
    Next we remove the middle third of the line, 
    and replace it with two lines that each have the same length (1/3 or orgininal) 
    as the remaining lines on each side. This new form is called the <term>generator</term>, 
    because it specifies a rule that is used to generate a new form. Note that 
    length of each seqment is 1/3.  See <xref ref="koch_curve_generator"/>.
  </p>

          <sidebyside widths="40% 40%">
            <figure xml:id="koch_curve_initiator">
               <caption>Initiator</caption>
              <image source="images/Koch_Initiator.png">
              </image>
            </figure>
            <figure xml:id="koch_curve_generator">
                  <caption>Generator</caption>
                   <image source="images/Koch_Generator.png">
              </image>
            </figure>
          </sidebyside>
  
<p>
  Next we repeat the above steps to each of four segments in the generator. 
  Then we get the cureve as in <xref ref="koch_curve_level2"/>.  Length of each 
  segment in this case is <m>16/9</m>.  If we apply the generator once again, we ge 
  the curve as in <xref ref="koch_curve_level3"/>. Length of each 
  segment in this case is <m>64/27</m>. 

        <sidebyside widths="40% 40%">
            <figure xml:id="koch_curve_level2">
               <caption>After 2 iterations</caption>
              <image source="images/Koch_Level2.png">
              </image>
            </figure>
            <figure xml:id="koch_curve_level3">
                  <caption>After 3 iterators</caption>
                   <image source="images/Koch_Level3.png">
              </image>
            </figure>
          </sidebyside>
  </p>
  <p>
    If we keep apply this process, we get what is called the Koch Curve 
    (named after the mathematician Helge von Koch in 1904). 
    After applying the 7 iterators  we get the curve as in <xref ref="koch_curve_level7"/>

    <figure xml:id="koch_curve_level7">
                  <caption>Koch Curve with 7 iterations.</caption>
                   <image source="images/Koch_Level7.png" width="50%">
              </image>
            </figure>
  </p>

  <p>
    Now let us construct the Koch curve as an application of affine linear map.
    The construction begins with a single line segment from <m>(0,0)</m> to <m>(1,0)</m>.
    At each step, this segment is replaced by four smaller segments:
  </p>

  <ol>
    <li>The first third (straight, scaled by <m>1/3</m>).</li>
    <li>The second third (scaled by <m>1/3</m> and rotated by <m>+60^\circ</m>).</li>
    <li>The third third (scaled by <m>1/3</m> and rotated by <m>-60^\circ</m>).</li>
    <li>The last third (straight, shifted).</li>
  </ol>
  <p>
    Each of these pieces is obtained from the original segment by applying one of four affine 
    linear maps.
    <md>
      <mrow>  T_1(z)  \amp= \tfrac{1}{3}z, </mrow>
      <mrow>  T_2(z)  \amp= \tfrac{1}{3}z + \tfrac{1}{3}, </mrow>
      <mrow>  T_3(z)  \amp= \tfrac{1}{3} e^{i\pi/3} z + \tfrac{1}{3}, </mrow>
      <mrow>  T_4(z)  \amp= \tfrac{1}{3}z + \tfrac{2}{3}. </mrow>
    </md>
  </p>
<p>
    Written in real coordinates, these are of the form <m>T_j(x) = A_j x + b_j</m> with
    <ul>
      <li><m>A_1 = \tfrac{1}{3}I, \quad b_1 = (0,0)</m>,</li>
      <li><m>A_2 = \tfrac{1}{3}I, \quad b_2 = (1/3,0)</m>,</li>
      <li><m>A_3 = \tfrac{1}{3}R_{60}, \quad b_3 = (1/3,0)</m>,</li>
      <li><m>A_4 = \tfrac{1}{3}I, \quad b_4 = (2/3,0)</m>,</li>
    </ul>
    where <m>R_{60}</m> is the <m>2\times 2</m> rotation matrix for <m>60^\circ</m>.
  </p>
<p>
  We deomontrate this in Sage.
</p>
<sage>
  <input>
# Define rotation matrix for 60 degrees
theta = pi/3
R60  = matrix([[cos(theta), -sin(theta)], [sin(theta), cos(theta)]])
Rm60 = matrix([[cos(-theta), -sin(-theta)], [sin(-theta), cos(-theta)]])

def koch_subdivide(segment):
    """Subdivide a line segment into 4 segments of Koch curve"""
    P, Q = segment
    v = Q - P
    P1 = P + v/3
    P3 = P + 2*v/3
    P2 = P1 + R60*(v/3)  # peak point
    return [(P, P1), (P1, P2), (P2, P3), (P3, Q)]

def koch_curve(iterations):
    """Generate Koch curve segments after given iterations"""
    segments = [(vector([0,0]), vector([1,0]))]  # initial line
    for _ in range(iterations):
        new_segments = []
        for seg in segments:
            new_segments.extend(koch_subdivide(seg))
        segments = new_segments
    return segments

def draw_koch(iterations, color="blue"):
    segs = koch_curve(iterations)
    G = Graphics()
    for P, Q in segs:
        G += line([P, Q], color=color,thickness=0.6)
    return G

# Generating Koch Curve with 4 iterations
## Do not use more than 7 generation, it may take a lot of time.
show(draw_koch(4),figsize=4, axes=False,aspect_ratio=1)

  </input>
  <output>
    
  </output>
</sage>
</example>

<example xml:id="Sierpinski_Triangle_Affile">
    <title>Sierpiński Triangle</title>
    <statement>
      <p>
         The <term>Sierpinski triangle</term> (named after the Polish 
         mathematician Waclaw Sierpinski),
         also called the Sierpinski gasket,
        is a self-similar fractal subset <m>S</m> of the plane.
        It can be obtained by an iterative geometric construction
        starting from a filled equilateral triangle and applying
        iterated function system (IFS) consisting of affine maps.
      </p>

      <p>
      The construction begins with a filled equilateral triangle (stage 0).
      At each stage we subdivide and remove parts according to the following rules:
    </p>
    <ul>
      <li>
        Stage 0: Start with a solid equilateral triangle of side length 1. 
        See <xref ref="Fig_Sierpinski_Level0"/>.
      </li>
      <li>
        Stage 1: Subdivide the triangle into four smaller equilateral triangles of side length 1/2
        and remove the central one. See <xref ref="Fig_Sierpinski_Level1"/>.
      </li>
      <li>
        Stage <m>n+1</m>: For each filled triangle from stage <m>n</m>,
        repeat the same process: divide into four, remove the central one.
        See <xref ref="Fig_Sierpinski_Level2"/> and  <xref ref="Fig_Sierpinski_Level3"/> 
        for two and three iterations.
      </li>
    </ul>
    <p>
      Continuing indefinitely, the limit of this process is the Sierpiński triangle. 
      See <xref ref="Fig_Sierpinski_Level8"/> after 8 iterations.
    </p>

       <sidebyside widths="40% 40%">
            <figure xml:id="Fig_Sierpinski_Level0">
               <caption>Original Triangle</caption>
              <image source="images/Sierpinski_Level0.png">
              </image>
            </figure>
            <figure xml:id="Fig_Sierpinski_Level1">
                  <caption>After One Iteration</caption>
                   <image source="images/Sierpinski_Level1.png">
              </image>
            </figure>
          </sidebyside>
 
          <sidebyside widths="40% 40%">
            <figure xml:id="Fig_Sierpinski_Level2">
               <caption>After Two Iterations</caption>
              <image source="images/Sierpinski_Level2.png">
              </image>
            </figure>
            <figure xml:id="Fig_Sierpinski_Level3">
                  <caption>After Three Iterations</caption>
                   <image source="images/Sierpinski_Level3.png">
              </image>
            </figure>
          </sidebyside>

 <figure xml:id="Fig_Sierpinski_Level8">
               <caption>Sierpinski Triangle after 8 iterations.</caption>
              <image source="images/Sierpinski_Level8.png" width="50%">
              </image>
            </figure>
           
    <p>
      The Sierpiński triangle arises from three specific affine maps:
      <md>
        <mrow>  T_1(x) \amp = \tfrac{1}{2} x, </mrow>
        <mrow>  T_2(x) \amp= \tfrac{1}{2} x + (1/2, 0), </mrow>
        <mrow>  T_3(x) \amp= \tfrac{1}{2} x + (1/4, \tfrac{\sqrt{3}}{4})</mrow>
      </md>
    </p>
    <p>
      Each of these maps scales the plane by a factor of <m>1/2</m> and then translates:
      <ul>
        <li><m>T_1</m> shrinks towards the origin.</li>
        <li><m>T_2</m> shrinks and shifts right to cover the bottom-right subtriangle.</li>
        <li><m>T_3</m> shrinks and shifts upward to cover the top subtriangle.</li>
      </ul>
    </p>

    <p>
      If <m>S</m> denotes the Sierpiński triangle, then it satisfies the fundamental
      iterated function system equation:
      <me>
        S = T_1(S) \cup T_2(S) \cup T_3(S).
      </me>
    </p>
<p>
  Now let us see how we can make the Sierspinki triangle in Sage.
</p>
<sage>
  <input>
## Sierpinki Triangle as affine map
# Define affine maps
A  = matrix([[1/2, 0], [0, 1/2]])
b1 = vector([0, 0])
b2 = vector([1/2, 0])
b3 = vector([1/4, sqrt(3)/4])

def apply_affine_to_triangle(A, b, tri):
    return [(A*p + b) for p in tri]

def subdivide(triangles):
    """Apply the 3 affine maps to a list of triangles"""
    out = []
    for T in triangles:
        out.append(apply_affine_to_triangle(A, b1, T))
        out.append(apply_affine_to_triangle(A, b2, T))
        out.append(apply_affine_to_triangle(A, b3, T))
    return out

def sierpinski(triangle, layers):
    """Generate triangles after given number of layers"""
    tris = [triangle]
    for _ in range(layers):
        tris = subdivide(tris)
    return tris

# Initial triangle
T0 = [vector([0,0]), 
      vector([1,0]), 
      vector([1/2, sqrt(3)/2])]

# Example: 4 layers
layers = 4
T_final = sierpinski(T0, layers)

# Draw all triangles
G = Graphics()
for tri in T_final:
    G += polygon(tri, axes=False, color="blue")
show(G, figsize=4)
  </input>
  <output>
    
  </output>
</sage>
    </statement>  
  
</example>


<example xml:id="Sierpinski_Carpet_Affile">
    <title>Sierpinski Carpet</title>
    <statement>
  <p>
    The <term>Sierpinski carpet</term> is the planar fractal obtained by repeatedly
        removing the open central square from a subdivided unit square. Equivalently,
        it is the unique nonempty set <m>C</m> satisfying an iterated-function
        system (IFS) of eight contractive affine maps.
  </p>
  <sage>
    <input>
# 8 affine transformations for the carpet (scale by 1/3 and shift)
A = matrix([[1/3, 0], [0, 1/3]])
translations = [
    vector([0,0]), vector([1/3,0]), vector([2/3,0]),
    vector([0,1/3]),                 vector([2/3,1/3]),
    vector([0,2/3]), vector([1/3,2/3]), vector([2/3,2/3])
]

def apply_affine_to_square(A, b, sq):
    """Apply affine transformation to a square"""
    return [(A*p + b) for p in sq]

def subdivide_squares(squares):
    """Replace each square by 8 smaller squares"""
    out = []
    for S in squares:
        for b in translations:
            out.append(apply_affine_to_square(A, b, S))
    return out

def sierpinski_carpet(square, layers):
    """Generate Sierpinski carpet squares after given layers"""
    sqs = [square]
    for _ in range(layers):
        sqs = subdivide_squares(sqs)
    return sqs

# Initial square
S0 = [vector([0,0]), 
      vector([1,0]), 
      vector([1,1]), 
      vector([0,1])]

# Example: 3 layers
layers = 3
S_final = sierpinski_carpet(S0, layers)

# Draw
G = Graphics()
for sq in S_final:
    G += polygon(sq, axes=False, color="blue")
show(G, figsize=4)

    </input>
    <output>
      
    </output>
  </sage>
  </statement>
</example>


<example xml:id="Sierpinski_Pyramid_Affile">
    <title>Sierpinski Pyramid</title>
    <statement>
  <p>
    The <term>Sierpinski pyramid</term> (also called the <term>Sierpinski tetra-pyramid</term> when based on a triangle, or 
    <term>Sierpiński square pyramid</term> when based on a square) is a three-dimensional fractal obtained by repeatedly 
    subdividing a pyramid into smaller self-similar pyramids. It provides a natural extension of the ideas behind the 
    Sierpiński triangle and Sierpiński carpet to three dimensions. 
  </p>

  <p>
    The construction can be described as an application of affine linear maps. 
    Starting from an initial pyramid <m>P_0</m>, we apply scaling by a factor of <m>\tfrac{1}{2}</m> 
    followed by translations to position the smaller pyramids. In the square-based case, 
    four pyramids are placed at the corners of the base, and one is placed on the top near the apex. 
    This gives a total of five affine maps:
    <me>
      P = T_1(P) \cup T_2(P) \cup T_3(P) \cup T_4(P) \cup T_5(P),
    </me>
    where each <m>T_j</m> is of the form <m>T_j(x) = A x + b_j</m>, 
    with <m>A</m> being the scaling matrix and <m>b_j</m> the translation vector.
  </p>

  
  <sage>
    <input>
# Define base pyramid vertices
base = [(0,0,0), (1,0,0), (1,1,0), (0,1,0)]
apex = (0.5, 0.5, 1)

# Initial pyramid
P0 = base + [apex]

# Function to create a pyramid polygon3d
def pyramid(points, color='lightblue', opacity=0.8):
    base = points[:-1]
    apex = points[-1]
    faces = []
    # base face
    faces.append(base)
    # side faces
    for i in range(len(base)):
        faces.append([base[i], base[(i+1)%len(base)], apex])
    return sum([polygon3d(f, color=color, opacity=opacity) for f in faces])

# Apply affine transformation to pyramid
def apply_affine_to_pyramid(A, b, pyramid_vertices):
    return [(A*vector(p) + b) for p in pyramid_vertices]

# Subdivide pyramid into 5 smaller pyramids (4 base corners + top)
def subdivide_pyramid(pyramid_vertices):
    A = matrix([[1/2,0,0],[0,1/2,0],[0,0,1/2]])
    out = []
    # 4 base shifts
    shifts = [vector([0,0,0]), vector([0.5,0,0]), vector([0,0.5,0]), vector([0.5,0.5,0])]
    for s in shifts:
        out.append(apply_affine_to_pyramid(A, s, pyramid_vertices))
    # top shift
    out.append(apply_affine_to_pyramid(A, vector([0.25,0.25,0.5]), pyramid_vertices))
    return out

# Generate Sierpinski Pyramid of given depth
def sierpinski_pyramid(depth=2):
    pyramids = [P0]
    for _ in range(depth):
        new_pyramids = []
        for P in pyramids:
            new_pyramids.extend(subdivide_pyramid(P))
        pyramids = new_pyramids
    G = sum([pyramid(P, color='lightblue', opacity=0.6) for P in pyramids])
    return G

# Example: depth = 4
show(sierpinski_pyramid(4), viewer='threejs', frame=False)

    </input>
    <output>
      
    </output>
  </sage>
</statement>

</example>
  
</subsection>
    <subsection xml:id="subsec-6-3-quadratic-form">
    <title>Quadratic Forms and Conic Sections</title>
      <p>
        In this subsection, we give an application of orthogonal diagonalizability  to conic sections.
      </p>
    
      <p>
            A general second-degree equation in two variables is given by
            <me>
                Q(x,y) = ax^2 + 2bxy + cy^2 + dx + ey + f = 0,
            </me>
            where <m>a,b,c,d,e,f \in \mathbb{R}</m>.
        </p>

          <p>
            This equation can be written compactly in matrix notation as
            <me>
                Q(x,y) =
                \begin{bmatrix}x \amp y\end{bmatrix} 
                \begin{bmatrix} a \amp b \\ b \amp c \end{bmatrix}
                \begin{bmatrix}x \\ y\end{bmatrix}+ \begin{bmatrix} d \amp e \end{bmatrix} 
                \begin{bmatrix} x \\ y \end{bmatrix} + f = 0.
            </me>
        </p>
        <p>
            Here,
            <me>
               A = \begin{bmatrix} a \amp b \\ b  \amp c \end{bmatrix}
            </me>
            is the symmetric matrix associated with the quadratic part <m>ax^2+2bxy+cy^2</m>.
        </p>

         <p>
            Since <m>A</m> is symmetric, it is orthogonally diagonalizable.  
            That is, there exists an orthogonal matrix <m>P</m> such that
            <me>
                P^TAP = D = \begin{pmatrix} \lambda_1 \amp 0\\ 0 \amp \lambda_2\end{pmatrix},
            </me>
            where <m>\lambda_1,\lambda_2</m> are eigenvalues of <m>A</m> and <m>P</m> is 
            the column matrix of orthogonal eigenbasis.
        </p>

<p> 
        With the change of variables
        <me>
        \begin{bmatrix} x \\ y \end{bmatrix} = P \begin{bmatrix} u \\ v \end{bmatrix},
        </me> the quadratic form simplifies to
        <me>Q(u,v) = \lambda_1 u^2 + \lambda_2 v^2 + \alpha u+\beta v + f = 0.</me>  
        Note that here
   <me>
    \begin{bmatrix}\alpha\\\beta\end{bmatrix} =\begin{bmatrix}d\amp e\end{bmatrix}P. 
   </me>

Thus, the cross term <m>2bxy</m> is eliminated by using the 
orthogonal linear trasformation <m>\begin{bmatrix} u \\ v \end{bmatrix}=P\begin{bmatrix} x \\ y \end{bmatrix} </m> 
and the conic aligns with its principal axes, that 
is along the eigenvectors directions. 
</p>

<p>
   Now we have various cases. 
   If we assume that <m>\lambda_1</m> and <m>\lambda_2</m> are positive,  then 
   we can complete square and we get
   <me>
    Q(u,v) = \lambda_1\left(u+\frac{\alpha}{2\lambda_1}\right)^2+
    \lambda_2\left(v+\frac{\beta}{2\lambda_2}\right)^2-g,
   </me>
    for some real number <m>g</m>.  What is <m>g</m>? It is <m>\left(\frac{\alpha^2}{4\lambda_1}
        + \frac{\beta^2}{4\lambda_2} - f\right)</m>.
    </p>

    <p> 
The origin of this quadratic in <m>uv</m>-coordinates is 
<me> \begin{bmatrix} u_0\\v_0\end{bmatrix}=
  \begin{bmatrix}-\frac{\alpha}{2\lambda_1}\\-\frac{\beta}{2\lambda_2}\end{bmatrix}.
</me>
Hence the orgin in terms of <m>xy</m>-coordinates is 
<me> \begin{bmatrix} x_0\\y_0\end{bmatrix}=
  P\begin{bmatrix}-\frac{\alpha}{2\lambda_1}\\-\frac{\beta}{2\lambda_2}\end{bmatrix}.
</me>
Thus we have converted the original quadratic <m>Q(x,y)</m> to 
<me>
Q(\tilde{x},\tilde{y})=\lambda_1\tilde{x}^2+\lambda_2\tilde{y}^2-g =
\frac{\tilde{x}^2}{\left(\frac{g}{\lambda_1}\right)^2}+\frac{\tilde{y}^2}{\left(\frac{g}{\lambda_1}\right)^2}-1.
</me>
This is an ellipse.

Here,  we have
<me>
  \begin{pmatrix}
  \tilde{x}\\ \tilde{y}
  \end{pmatrix} = \begin{pmatrix} u+\frac{\alpha}{2\lambda_1}\\v+\frac{\beta}{2\lambda_2}\end{pmatrix}
  =\begin{pmatrix} u\\v\end{pmatrix}+\begin{pmatrix} \frac{\alpha}{2\lambda_1}\\\frac{\beta}{2\lambda_2}\end{pmatrix}=
  P^{-1}\begin{pmatrix}x\\y
  \end{pmatrix}+\begin{pmatrix} \frac{\alpha}{2\lambda_1}\\\frac{\beta}{2\lambda_2}\end{pmatrix}.
</me> 

The transformation, <m>P^{-1}\begin{pmatrix}x\\y
  \end{pmatrix}+\begin{pmatrix} \frac{\alpha}{2\lambda_1}\\\frac{\beta}{2\lambda_2}\end{pmatrix} </m> is called an <em>affine linear transformation.</em> Here <m>P^{-1}</m> is a orthogonl linear map.

 Thus an <em>affine linear transformation</em> on <m>\R^n</m> is a map of the form <m>T(x)=Px+v</m> ,where <m>P</m> is an 
orthogonal transformation and <m>v</m> is a called a translation vector. Such maps are also called 
<em>isometries.</em>
</p>

<p>
  In case, <m>\lambda_1</m> and <m>\lambda_2</m> both are negative then, we can multiply the 
  whole equation by <m>-1</m> and we get the a similar expression except, the right hand changes 
  its sign.
</p>
  
<p>
  In case one of the <m>\lambda's</m>, say <m>\lambda_2&lt;0</m>, then the conic tranforms to 
  <me>
    Q(\tilde{x},\tilde{y})=\lambda_1\tilde{x}^2-\lambda_2\tilde{y}^2-g, 
  </me>
  which is a hyperbola.
</p>


<p>
  In case one of the <m>\lambda's</m>, say <m>\lambda_2=0</m>, then the conic tranforms to 
  <me>
    Q(\tilde{x},\tilde{y})=\lambda_1\tilde{x}^2+\beta \tilde{y} -g, 
  </me>
  which is parabola. Here <m>\tilde{y}=v</m> and <m>g=\alpha^2/(4\lambda_1)-f</m>.
</p>

<p><alert>Classification of Conics in two variables</alert></p>


        <p>Based on the above discussions, the classification of the above conic section 
          depends on the eigenvalues of <m>A</m>.</p>
<p>
        <ul>
            <li>
                <p><em>Ellipse:</em> If both eigenvalues <m>\lambda_1, \lambda_2</m> 
                  have the same sign, then the quadratic is an ellipe of the form 
                  <m>x^2/a^2+y^2/b^2=1</m>.
                </p>
            </li>
            <li>
                <p><em>Circle:</em> When <m>\lambda_1 = \lambda_2</m>, then the quadratic is circle.</p>
            </li>
            <li>
                <p><em>Hyperbola:</em> If eigenvalues have opposite signs, then the quadratic is a hyperbola of the form 
                <me>x^2/a^2 -y^2/b^2= 1.</me> </p>
            </li>
            <li>
                <p><em>Parabola:</em> If one eigenvalue is zero, then it is a parabola.</p>
            </li>
        </ul>
      </p>


      <example xml:id="eg-conic-ellipe1">
        <statement>
          <p>
         Condiser the quadratic <m>(Q(x,y)=7 \, x^{2} - 6 \, x y + 7 \, y^{2} - 2 \, x + 3 \, y - 24</m>. Let us convert this 
         quadrtic into a conic section in canonial form.    
          </p>
        </statement>
       <solution>
        <p>
          The associated symmetric matrix of this quadratic <m>A</m> is given by 
          <me>
            A = \begin{pmatrix} 7 \amp -3 \\-3 \amp 7\end{pmatrix}.
          </me>
          It is easy to check the the eigenvalues of <m>A</m> are <m>\lambda_1=10</m> and <m>\lambda_2=4</m> with 
          the corresponding eigenvectors <m>v_1 = \left(\frac{1}{2} \, \sqrt{2},\,-\frac{1}{2} \, \sqrt{2}\right)</m> and 
          <m>v_2=\left(\frac{1}{2} \, \sqrt{2},\,\frac{1}{2} \, \sqrt{2}\right)</m>. Hence we have 
          <me>
            D= \left(\begin{array}{rr}
10 \amp 0 \\
0 \amp 4
\end{array}\right), P = \left(\begin{array}{rr}
\frac{1}{\sqrt{2}}  \amp \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} \amp \frac{1}{\sqrt{2}}
\end{array}\right).
          </me>      
  The new coordinates in terms of <m>uv</m> is 
  <me>
    \begin{bmatrix} x \\ y \end{bmatrix} = P \begin{bmatrix} u \\ v \end{bmatrix}=\left(\begin{array}{rr}
\frac{1}{\sqrt{2}} u + \frac{1}{\sqrt{2}}  v \\ -\frac{1}{\sqrt{2}}  u + \frac{1}{\sqrt{2}v} 
\end{array}\right).
  </me>
   Now substituting <m>x=\frac{1}{\sqrt{2}} u + \frac{1}{\sqrt{2}}  v</m> and <m>y=-\frac{1}{\sqrt{2}} u + \frac{1}{\sqrt{2}} v </m> in the given quadratic, we get
   <me>
    Q(u,v)=10 \, u^{2} + 4 \, v^{2} - 7 \, \sqrt{2} u + \sqrt{2} v - 56.
   </me>
   After completing the squares, we get
   <me>
    Q(u,v)=10 \left(u- \frac{7\sqrt{2}}{20}\right)^2+ 4 \left(v+ \frac{\sqrt{2}}{8}\right)^2 - 2343/40.
   </me>
   This can be written as an equation of ellipes. Note that here the translation vector is given by
   <me>
    \begin{pmatrix}x_0\\y_0\end{pmatrix}=P\begin{pmatrix} \frac{7\sqrt{2}}{20}\\\frac{-\sqrt{2}}{8}\end{pmatrix}=
    \begin{pmatrix}\frac{9}{40}\\ -\frac{19}{40}\end{pmatrix}.
   </me>
   
        </p>
        <p>
          Let us explore this in Sage. Here we plot the orginal quadratic curve along with 
          the transformed coordinates.  

          <sage>
            <input>
          var('x,y')
var('u,v')
var('s,t')
var('z')
X = vector([x,y])
Y = vector([u,v])
a,b,c,d,e,f = 7,-3,7,-6,8,-56
Q(x,y) = a*x^2+2*b*x*y+c*y^2+d*x+e*y+f
print('The curve is given by')
show(Q(x,y))
curve = implicit_plot(Q(x,y),(x,-4,4),(y,-4,4),gridlines=True,aspect_ratio=1)
xaxis=line([(-4,0),(4,0)],color='black')
yaxis=line([(0,-4),(0,4)],color='black')
show(curve+xaxis+yaxis)
## We define the associated matrix A
A= matrix([[a,b],[b,c]])
B= vector([d,e])
print('The quadratic in terms of matrices A and B')
show(expand(X.dot_product(A*X))+B.dot_product(X)+f)

## Orthogonal Diagonalization of the matrix A
print(f'The eigenvalues of A are {A.eigenvalues()}')
D,P = A.eigenmatrix_right();
lam1 = D[0,0]
lam2 = D[1,1]

## We normalize the eigenvectors and redefine P
v1 = P.columns()[0]/norm(P.columns()[0])
v2 = P.columns()[1]/norm(P.columns()[1])
P = column_matrix([v1,v2])
print('To check if PDP^{-1}=A')
print(P*D*P.inverse())

## The qudratic in terms of new coordinate system uv
R(u,v)=expand(Q.substitute(x=(P*Y)[0],y=(P*Y)[1])(u,v));
print(R(u,v))

## The translation vector
alpha = (B*P)[0]
beta = (B*P)[1]
x0=-alpha/(2*lam1)
y0=-beta/(2*lam2)
g = (alpha)^2/(4*lam1)+(beta)^2/(4*lam2)-f
show(g)
print('The quadatic in canonical form is')
S(s,t)=expand(R.subs(u=s-alpha/(2*lam1),v=t-beta/(2*lam2))(s,t))
show(S(s,t))
## Plotting the ellipes withe major-minor arix
origin = P*vector([x0,y0])
neworigin = point([(origin[0],origin[1])],color='green')
newxaxis = parametric_plot(origin+z*v1, (z,-5,5),color='green')
newyaxis = parametric_plot(origin+z*v2, (z,-5,5),color='green')
show(curve+neworigin+newxaxis+newyaxis+xaxis+yaxis,aspect_ratio=1)
            </input>
            <output>
              
            </output>
          </sage>
        </p>
       </solution>
      </example>
    <example>
      <p>
      Consider the quadratic equation  <m>3x^{2}+4xy + 2 y^{2} - 8 x + 6 y-3=0</m>. We wish the classify this as a conic section.
        </p>
       
       <p>   
        Let us first plot the graph of this curve in Sage.
      </p>
  <sage>
  <input>
    var('x,y')
    curve = implicit_plot(3*x^2 + 4*x*y + 2*y^2 - 8*x + 6*y - 3==0,(x,-5,20),(y,-20,10),gridlines=True,aspect_ratio=1)
    xaxis=line([(-5,0),(20,0)],color='black')
    yaxis=line([(0,-20),(0,10)],color='black')
    show(curve+xaxis+yaxis)

  </input>
  <output>
    
  </output>
</sage>

<p>
  The symmetric matrix associated with quadratic tem is given by 
            <me>A = \begin{pmatrix}3 \amp 2 \\ 2 \amp 2 \end{pmatrix}</me>.

           It is easy to check that  he eigenvalues are <m>\lambda_1=0.4384471871911698, \lambda_2=4.561552812808830</m>.  Since 
           both the eigenvalues are positive, this quadratic is an ellipse. This is what the graph shows. 
        </p>

 <p>
  Now we give all the steps in Sage to plot the curve along with the now coordinate system.   
 </p>       

 <sage>
  <input>
var('x,y')
var('u,v')
var('s,t')
var('z')
X = vector([x,y])
Y = vector([u,v])
a,b,c,d,e,f = 3,2,2,-8,6,-3
Q(x,y) = a*x^2+2*b*x*y+c*y^2+d*x+e*y+f
print('The curve is given by')
show(Q(x,y))
curve = implicit_plot(Q(x,y),(x,-5,20),(y,-20,10),gridlines=True,aspect_ratio=1)
xaxis=line([(-5,0),(20,0)],color='black')
yaxis=line([(0,-20),(0,10)],color='black')
show(curve+xaxis+yaxis)
## We define the associated matrix A
A= matrix([[a,b],[b,c]])
B= vector([d,e])
print('The quadratic in terms of matrices A and B')
show(expand(X.dot_product(A*X))+B.dot_product(X)+f)

## Orthogonal Diagonalization of the matrix A
print(f'The eigenvalues of A are {A.eigenvalues()}')
D,P = A.eigenmatrix_right();
lam1 = D[0,0]
lam2 = D[1,1]

## We normalize the eigenvectors and redefine P
v1 = P.columns()[0]/norm(P.columns()[0])
v2 = P.columns()[1]/norm(P.columns()[1])
P = column_matrix([v1,v2])
print('To check if PDP^{-1}=A')
print(P*D*P.inverse())

## The qudratic in terms of new coordinate system uv
R(u,v)=expand(Q.substitute(x=(P*Y)[0],y=(P*Y)[1])(u,v));
print(R(u,v))

## The translation vector
alpha = (B*P)[0]
beta = (B*P)[1]
x0=-alpha/(2*lam1)
y0=-beta/(2*lam2)
g = (alpha)^2/(4*lam1)+(beta)^2/(4*lam2)-f
show(g)
print('The quadatic in canonical form is')
S(s,t)=expand(R.subs(u=s-alpha/(2*lam1),v=t-beta/(2*lam2))(s,t))
show(S(s,t))
## Plotting the ellipes withe major-minor arix
origin = P*vector([x0,y0])
neworigin = point([(origin[0],origin[1])],color='green')
newxaxis = parametric_plot(origin+z*v1, (z,-15,15),color='green')
newyaxis = parametric_plot(origin+z*v2, (z,-15,15),color='green')
show(curve+neworigin+newxaxis+newyaxis+xaxis+yaxis,aspect_ratio=1)
  </input>

    <output>
    
  </output>
 </sage>
</example>

<example>
  <p>
   Consider the quadratic eqation given by <m>-x^2+4xy-y^2-30x,+y+20=0</m>. Use Sage to classify this and plot the 
   curve along with the transformed coordinates system. 
  </p>
  <sage>
    <input>
var('x,y')
var('u,v')
var('s,t')
var('z')
X = vector([x,y])
Y = vector([u,v])
a,b,c,d,e,f = -1,4,-1,-30,1,20
Q(x,y) = a*x^2+2*b*x*y+c*y^2+d*x+e*y+f
print('The curve is given by')
show(Q(x,y))
curve = implicit_plot(Q(x,y),(x,-5,10),(y,-5,10),gridlines=True,aspect_ratio=1)
xaxis=line([(-4,0),(4,0)],color='black')
yaxis=line([(0,-4),(0,4)],color='black')
show(curve+xaxis+yaxis)
## We define the associated matrix A
A= matrix([[a,b],[b,c]])
B= vector([d,e])
print('The quadratic in terms of matrices A and B')
show(expand(X.dot_product(A*X))+B.dot_product(X)+f)

## Orthogonal Diagonalization of the matrix A
print(f'The eigenvalues of A are {A.eigenvalues()}')
D,P = A.eigenmatrix_right();
lam1 = D[0,0]
lam2 = D[1,1]

## We normalize the eigenvectors and redefine P
v1 = P.columns()[0]/norm(P.columns()[0])
v2 = P.columns()[1]/norm(P.columns()[1])
P = column_matrix([v1,v2])
print('To check if PDP^{-1}=A')
print(P*D*P.inverse())

## The qudratic in terms of new coordinate system uv
R(u,v)=expand(Q.substitute(x=(P*Y)[0],y=(P*Y)[1])(u,v));
print(R(u,v))

## The translation vector
alpha = (B*P)[0]
beta = (B*P)[1]
x0=-alpha/(2*lam1)
y0=-beta/(2*lam2)
g = (alpha)^2/(4*lam1)+(beta)^2/(4*lam2)-f
show(g)

S(s,t)=expand(R.subs(u=s-alpha/(2*lam1),v=t-beta/(2*lam2))(s,t))
show(S(s,t))
## Plotting the ellipes withe major-minor arix
origin = P*vector([x0,y0])
neworigin = point([(origin[0],origin[1])],color='green')
newxaxis = parametric_plot(origin+z*v1, (z,-5,5),color='green')
newyaxis = parametric_plot(origin+z*v2, (z,-5,5),color='green')
show(curve+neworigin+newxaxis+newyaxis+xaxis+yaxis,aspect_ratio=1)
    </input>
    <output>
      
    </output>
  </sage>
</example>

<example>
  <statement>
    <p>
  Consider the quadratic equation <m>Q(x,y) = 3 \, x^{2} - 6 \, x y + 3 \, y^{2} - 6 \, x + 8 \, y + 5</m> and classify this 
  to  a conic section.
  </p>
  
  </statement>

  <solution>
    <p>
   The matrix associated with the quadratic part of the above equation is 
   <m>A = \left(\begin{array}{rr}
3 \amp -3 \\
-3 \amp 3
\end{array}\right)</m>. It is easy to check that the eigenvalues of <m>A</m> are <m>\lambda_1=6, \lambda_2=0</m>. Since one of the eigenvalues is 0, this curve is a parabola.  Let us draw this curve along with the tranformed orgini and the two new coordinate directions in Sage.
   </p>
<sage>
  <input>
  var('x,y')
var('u,v')
var('s,t')
var('z')
X = vector([x,y])
Y = vector([u,v])
a,b,c,d,e,f = 3,-3,3,-6,8,5
Q(x,y) = a*x^2+2*b*x*y+c*y^2+d*x+e*y+f
print('The curve is given by')
show(Q(x,y))
curve = implicit_plot(Q(x,y),(x,-5,5),(y,-5,5),gridlines=True,aspect_ratio=1)
xaxis=line([(-5,0),(5,0)],color='black')
yaxis=line([(0,-5),(0,5)],color='black')
show(curve+xaxis+yaxis)
## We define the associated matrix A
A= matrix([[a,b],[b,c]])
B= vector([d,e])
print('The quadratic in terms of matrices A and B')
show(expand(X.dot_product(A*X))+B.dot_product(X)+f)
## Orthogonal Diagonalization of the matrix A
print(f'The eigenvalues of A are {A.eigenvalues()}')
D,P = A.eigenmatrix_right();
lam1 = D[0,0]
lam2 = D[1,1]

## We normalize the eigenvectors and redefine P
v1 = P.columns()[0]/norm(P.columns()[0])
v2 = P.columns()[1]/norm(P.columns()[1])
P = column_matrix([v1,v2])
print('To check if PDP^{-1}=A')
print(P*D*P.inverse())

## The qudratic in terms of new coordinate system uv
R(u,v)=expand(Q.substitute(x=(P*Y)[0],y=(P*Y)[1])(u,v));
print(R(u,v))

## The translation vector
alpha = (B*P)[0]
beta = (B*P)[1]
x0=-alpha/(2*lam1)
y0=-beta
g = alpha^2/(4*lam1)-f

## Plotting the parabola
origin = P*vector([x0,y0])
neworigin = point([(origin[0],origin[1])],color='green')
newxaxis = parametric_plot(origin+z*v1, (z,-5,5),color='green')
newyaxis = parametric_plot(origin+z*v2, (z,-5,5),color='green')
show(curve+neworigin+newxaxis+newyaxis+xaxis+yaxis,aspect_ratio=1)
  </input>
  <output>
    
  </output>
</sage>   
   
  </solution>
</example>

<activity>
  <p>
    For given quadratic equation <m>Q(x,y)=ax^2+2bxy+cy^2+dx+ey+f=0</m>, write down the corresponding canonical conics by describing
    the new orgin <m>(x_0,y_0)</m>, and the new coordinate vectors by codisering different cases in a tabular form. 
  </p>
</activity>

</subsection>


<subsection xml:id="subsec-conic-suraces-3variables">
  <title>Classification of Quadratic Surfaces in Three Variables</title>
  <introduction>
    <p>
      The classification of quadratic equation in three varibale can be done in a very similar manner as we have seen in case of two variable <xref ref="subsec-6-3-quadratic-form"/>
    </p>
  </introduction>

  <p>
  A general quadratic equation in three variables is
<me>Q(x,y,z) = ax^2 + by^2 + cz^2 + 2dxy + 2eyz + 2fzx + gx + hy + iz + j=0
</me>
where <m>a,b,c,d,e,f,g,h,i,j \in \mathbb{R}</m>.  
</p>
<p>
In matrix form,
<me>
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} +  \mathbf{b}^T \mathbf{x} + j,
\quad 
\mathbf{x} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}, \quad 
A = \begin{bmatrix} a \amp d \amp f \\ d \amp b \amp e \\ f \amp e \amp c \end{bmatrix}, \quad
\mathbf{b} = \begin{bmatrix} g \\ h \\ i \end{bmatrix}.
</me>

Since <m>A</m> is symmetric, there exists an orthogonal matrix <m>P</m> such that 
<me>
P^T A P = D= \operatorname{diag}(\lambda_1,\lambda_2,\lambda_3).
</me>
After an orthogonal change of variables <m>\mathbf{x} = P\mathbf{u}</m> and a translation 
to eliminate linear terms, the quadratic form reduces to the <em>canonical form</em>.
<me>
Q(u,v,w) = \lambda_1 u^2 + \lambda_2 v^2 + \lambda_3 w^2 + j = 0.
</me>
  </p>

 <p><alert>Classification of Quadrics</alert></p>
  
 
<p>
Depending on the signs of <m>\lambda_1, \lambda_2, \lambda_3</m>, we obtain the following surfaces:
</p>

<ol>
  <li>
    <p>
      <em>Ellipsoid: </em> All eigenvalues positive.
      <me>
         \frac{u^2}{a^2} + \frac{v^2}{b^2} + \frac{w^2}{c^2} = 1, a,b,c &gt; 0</me>. 
      </p>
         <sage>
          <input>
        ## Ellipsoid
        var('x y z')
        a, b, c = 2, 1.5, 1  # semi-axes

        ellipsoid = implicit_plot3d(x^2/a^2 + y^2/b^2 + z^2/c^2 - 1, 
                                      (x,-2,2), (y,-2,2), (z,-2,2), 
                                      color='gray', opacity=0.4)
        ellipsoid.show(frame=False,axes=True)
        </input>
          <output>
            
          </output>
        </sage>
  </li>
  <li>
    <p>
      <em>Hyperboloid of One Sheet: </em> Two positive eigenvalues, one negative.
      <me>
        \frac{u^2}{a^2} + \frac{v^2}{b^2} - \frac{w^2}{c^2} = 1.
      </me>
    </p>
    <sage>
      <input>
    ##Hyperboloid of One Sheets
var('x,y,z')
a, b, c = 1, 1, 1
hyperboloid1 = implicit_plot3d(x^2/a^2 + y^2/b^2 - z^2/c^2 - 1, 
                               (x,-2,2), (y,-2,2), (z,-3,3),
                               color='orange', opacity=0.4)
hyperboloid1.show(frame=False,axes=True,title='Hyperboloid of One Sheets')
      </input>
      <output>
        
      </output>
    </sage>
  </li>
  <li>
    <p>
       <em>Hyperboloid of Two Sheets:</em> One positive eigenvalue, two negative.
  <me>
  -\frac{u^2}{a^2} - \frac{v^2}{b^2} + \frac{w^2}{c^2} = 1.
  </me>
    </p>

    <sage>
      <input>
##Hyperboloid of Two Sheets
var('x,y,z')
a,b,c =1,1,1
hyperboloid2 = implicit_plot3d(-x^2/a^2 -y^2/b^2 + z^2/c^2 - 1, 
                               (x,-2,2), (y,-2,2), (z,-3,3),
                               color='green', opacity=0.6)
hyperboloid2.show(frame=False,axes=True)
      </input>
      <output>
        
      </output>
    </sage>
  </li>
  <li>
    <p>
      <em>Elliptic Cone: </em> Two positibve and one negative eigenvalues with no constant term.
  <me>
  \frac{u^2}{a^2} + \frac{v^2}{b^2} - \frac{w^2}{c^2} = 0.
  </me>
    </p>
    <sage>
      <input>
## Elliptic Cone
var('x,y,z')
a,b,c =1,1,1
elliptic_cone = implicit_plot3d(x^2/a^2 +y^2/b^2 - z^2/c^2, 
                               (x,-2,2), (y,-2,2), (z,-3,3),
                               color='green', opacity=0.6)
elliptic_cone.show(frame=False,axes=True)
      </input>
      <output>
        
      </output>
    </sage>
  </li>
  <li>
    <p>
  <em>Elliptic Paraboloid: (Bowl-shaped surface)</em> Two positive eigenvalues, one zero.
  <me>
  \frac{u^2}{a^2} + \frac{v^2}{b^2} = \frac{w}{c}.
  </me>
    </p>
    <sage>
      <input>
 ## Elliptic Paraboloid
var('x,y,z')
a,b,c=3,1,5
bowl = implicit_plot3d(x^2/a^2 + y^2/b^2-z/c, 
                (x,-2,2), (y,-2,2),(z,-2,2),
                color='green', opacity=0.6)
bowl.show(frame=False,axes=True)    
      </input>
      <output>
        
      </output>
    </sage>
  </li>
  <li>
    <p>
      <em>Hyperbolic Paraboloid: (Saddle Surface)</em> One positive eigenvalue, one negative, one zero.
  <me>
  \frac{u^2}{a^2} - \frac{v^2}{b^2} = \frac{w}{c}.
  </me>
    </p>
    <sage>
      <input>
    ## Hyperbolic Paraboloid (Saddle)
var('x,y,z')
a,b,c = 3,2,5
saddle = implicit_plot3d(x^2/a^2 - y^2/b^2-z/c, 
                (x,-2,2), (y,-2,2),(z,-2,2),
                color='green', opacity=0.6)
saddle.show(frame=False,axes=True)
      </input>
      <output>
        
      </output>
    </sage>
  </li>
  <li>
    <p>
   <em>Elliptic Cylinder:</em> Two positive eigenvalues, third zero.
  <me>
  \frac{u^2}{a^2} + \frac{v^2}{b^2} = 1.
  </me>
    </p>
    <sage>
      <input>
## Elliptic Cylinder
var('x,y,z')
a,b=2,1
ell_cyl = implicit_plot3d(x^2/a^2 + y^2/b^2-1, 
                (x,-4,4), (y,-2,2),(z,-2,2),
                color='green', opacity=0.6)
ell_cyl.show(frame=False,axes=True)
      </input>
      <output>
        
      </output>
    </sage>
  </li>
  <li>
    <p>
    <em>Hyperbolic Cylinder:  </em> One positive, one negative, third zero.
<me>
  \frac{u^2}{a^2} - \frac{v^2}{b^2} = 1.
</me>  
    </p>
  <sage>
    <input>
  ## Hyperbolic Cylinder
var('x,y,z')
a,b=2,1
hyp_cyl = implicit_plot3d(x^2/a^2 - y^2/b^2-1, 
                (x,-4,4), (y,-2,2),(z,-2,2),
                color='green', opacity=0.6)
hyp_cyl.show(frame=False,axes=True)    
    </input>
    <output>
      
    </output>
  </sage>
  </li>
  <li>
    <p>
      <em>Parabolic Cylinder:</em> Only one  nonzero eigenvalue.
  <me>
  \frac{u^2}{a^2} = \frac{v}{b}.
  </me>
    </p>
    <sage>
      <input>
     ## Parabolic Cylinder
var('x,y,z')
a,b=1,1
par_cyl = implicit_plot3d(x^2/a^2 - y/b-1, 
                (x,-2,2), (y,-2,2),(z,-2,2),
                color='green', opacity=0.6)
par_cyl.show(frame=False,axes=True)   
      </input>
      <output>
        
      </output>
    </sage>
  </li>
</ol>


</subsection>
  
</section>

 