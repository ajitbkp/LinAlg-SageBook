<!DOCTYPE html>
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*       on 2025-08-18T19:42:30+05:30       *-->
<!--*   A recent stable commit (2020-08-09):   *-->
<!--* 98f21740783f166a773df4dc83cab5293ab63a4a *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<html lang="en-US">
<head xmlns:og="http://ogp.me/ns#" xmlns:book="https://ogp.me/ns/book#">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Principal Component Analysis</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta property="og:type" content="book">
<meta property="book:title" content="Linear Algebra with SageMath">
<meta property="book:author" content="Ajit Kumar">
<script src="https://sagecell.sagemath.org/static/embedded_sagecell.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['\\(','\\)']],
    tags: "none",
    tagSide: "right",
    tagIndent: ".8em",
    packages: {'[+]': ['base', 'extpfeil', 'ams', 'amscd', 'newcommand', 'knowl']}
  },
  options: {
    ignoreHtmlClass: "tex2jax_ignore|ignore-math",
    processHtmlClass: "process-math",
    renderActions: {
        findScript: [10, function (doc) {
            document.querySelectorAll('script[type^="math/tex"]').forEach(function(node) {
                var display = !!node.type.match(/; *mode=display/);
                var math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                var text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = {node: text, delim: '', n: 0};
                math.end = {node: text, delim: '', n: 0};
                doc.math.push(math);
            });
        }, '']
    },
  },
  chtml: {
    scale: 0.88,
    mtextInheritFont: true
  },
  loader: {
    load: ['input/asciimath', '[tex]/extpfeil', '[tex]/amscd', '[tex]/newcommand', '[pretext]/mathjaxknowl3.js'],
    paths: {pretext: "https://pretextbook.org/js/lib"},
  },
};
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script><script>// Make *any* pre with class 'sagecell-sage' an executable Sage cell
// Their results will be linked, only within language type
sagecell.makeSagecell({inputLocation: 'pre.sagecell-sage',
                       linked: true,
                       languages: ['sage'],
                       evalButtonText: 'Evaluate (Sage)'});
</script><script src="https://pretextbook.org/js/lib/jquery.min.js"></script><script src="https://pretextbook.org/js/lib/jquery.sticky.js"></script><script src="https://pretextbook.org/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.13/pretext.js"></script><script>miniversion=0.674</script><script src="https://pretextbook.org/js/0.13/pretext_add_on.js?x=1"></script><script src="https://pretextbook.org/js/lib/knowl.js"></script><!--knowl.js code controls Sage Cells within knowls--><script>sagecellEvalName='Evaluate (Sage)';
</script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/pretext_add_on.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/banner_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/toc_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/knowls_default.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/style_oscarlevin.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/colors_blue_red.css" rel="stylesheet" type="text/css">
<link href="https://pretextbook.org/css/0.4/setcolors.css" rel="stylesheet" type="text/css">
<!--** eBookCongig is necessary to configure interactive       **-->
<!--** Runestone components to run locally in reader's browser **-->
<!--** No external communication:                              **-->
<!--**     log level is 0, Runestone Services are disabled     **-->
<script type="text/javascript">
eBookConfig = {};
eBookConfig.useRunestoneServices = false;
eBookConfig.host = 'http://127.0.0.1:8000';
eBookConfig.course = 'PTX Course: Title Here';
eBookConfig.basecourse = 'PTX Base Course';
eBookConfig.isLoggedIn = false;
eBookConfig.email = '';
eBookConfig.isInstructor = false;
eBookConfig.logLevel = 0;
eBookConfig.username = '';
eBookConfig.readings = null;
eBookConfig.activities = null;
eBookConfig.downloadsEnabled = false;
eBookConfig.allow_pairs = false;
eBookConfig.enableScratchAC = false;
eBookConfig.build_info = "";
eBookConfig.python3 = null;
eBookConfig.acDefaultLanguage = 'python';
eBookConfig.runestone_version = '5.0.1';
eBookConfig.jobehost = '';
eBookConfig.proxyuri_runs = '';
eBookConfig.proxyuri_files = '';
eBookConfig.enable_chatcodes =  false;
</script>
<!--*** Runestone Services ***-->
<script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runtime.b0f8547c48f16a9f.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/637.d54be67956c5c660.bundle.js"></script><script type="text/javascript" src="https://runestone.academy/cdn/runestone/6.2.1/runestone.0e9550fe42760516.bundle.js"></script><link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/637.fafafbd97df8a0d1.css">
<link rel="stylesheet" type="text/css" href="https://runestone.academy/cdn/runestone/6.2.1/runestone.e4d5592da655219f.css">
</head>
<body id="changeme" class="pretext-book ignore-math has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div id="latex-macros" class="hidden-content process-math" style="display:none"><span class="process-math">\(    \newcommand{\Loadedframemethod}{default}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\inner}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normx}[1]{\left\Vert#1\right\Vert}
\newcommand{\partd}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\innprod}[2]{\left\lt #1,#2\right&gt;}
\def\rank{{ rank\,}}
\newcommand{\ds}{\displaystyle}

\def\diag{{ diag\,}}
\def\proj{{ proj\,}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\definecolor{fillinmathshade}{gray}{0.9}
\newcommand{\fillinmath}[1]{\mathchoice{\colorbox{fillinmathshade}{$\displaystyle     \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\textstyle        \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptstyle      \phantom{\,#1\,}$}}{\colorbox{fillinmathshade}{$\scriptscriptstyle\phantom{\,#1\,}$}}}
\)</span></div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="linalg-pretext-book.html"><span class="title">Linear Algebra with SageMath:</span> <span class="subtitle">A Gentle Introduction</span></a></h1>
<p class="byline">Ajit Kumar</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="chap10-PCA.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="chap10-PCA.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="sec10-2.html" title="Next">Next</a></span></div>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="chap10-PCA.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="chap10-PCA.html" title="Up">Up</a><a class="next-button button toolbar-item" href="sec10-2.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><ul>
<li class="link frontmatter">
<a href="frontmatter.html" data-scroll="frontmatter" class="internal"><span class="title">Front Matter</span></a><ul>
<li><a href="colophon-1.html" data-scroll="colophon-1" class="internal">Colophon</a></li>
<li><a href="biography-1.html" data-scroll="biography-1" class="internal">Author Biography</a></li>
<li><a href="dedication-1.html" data-scroll="dedication-1" class="internal">Dedication</a></li>
<li><a href="preface.html" data-scroll="preface" class="internal">Preface</a></li>
</ul>
</li>
<li class="link">
<a href="chap1-lineq.html" data-scroll="chap1-lineq" class="internal"><span class="codenumber">1</span> <span class="title">System of Linear Equations</span></a><ul>
<li><a href="sec1-1-elementary-operations.html" data-scroll="sec1-1-elementary-operations" class="internal">Elementary Row Operations</a></li>
<li><a href="sec1-2-col-operations.html" data-scroll="sec1-2-col-operations" class="internal">Elementary Column Operations</a></li>
<li><a href="sec1-3-echelon-form.html" data-scroll="sec1-3-echelon-form" class="internal">Echelon Forms</a></li>
<li><a href="sec1-4-matrix-rank.html" data-scroll="sec1-4-matrix-rank" class="internal">Rank of Matrices</a></li>
<li><a href="sec1-5-hom-system.html" data-scroll="sec1-5-hom-system" class="internal">Homogeneous System of Linear Equations</a></li>
<li><a href="sec1-6-LU.html" data-scroll="sec1-6-LU" class="internal"><span class="process-math">\(LU\)</span>-Facotorization</a></li>
<li><a href="sec1-7-exer.html" data-scroll="sec1-7-exer" class="internal">Exercises</a></li>
</ul>
</li>
<li class="link">
<a href="chap2-Rn-Space.html" data-scroll="chap2-Rn-Space" class="internal"><span class="codenumber">2</span> <span class="title"><span class="process-math">\(\R^n\)</span> as a Vector Space</span></a><ul>
<li><a href="sec2-0-intro.html" data-scroll="sec2-0-intro" class="internal">Introduction</a></li>
<li><a href="sec2-1-LinSpan.html" data-scroll="sec2-1-LinSpan" class="internal">Linear Spans</a></li>
<li><a href="sec-2-2-LI.html" data-scroll="sec-2-2-LI" class="internal">Linear Dependence</a></li>
<li><a href="sec-2-3-basis-dimension.html" data-scroll="sec-2-3-basis-dimension" class="internal">Basis and Dimension</a></li>
<li><a href="sec2-4-Sage.html" data-scroll="sec2-4-Sage" class="internal">Sage Computations</a></li>
</ul>
</li>
<li class="link">
<a href="chap3-LT.html" data-scroll="chap3-LT" class="internal"><span class="codenumber">3</span> <span class="title">Linear Transformations</span></a><ul>
<li><a href="sec3-1-LT.html" data-scroll="sec3-1-LT" class="internal">Introduction</a></li>
<li><a href="sec3-2-LT.html" data-scroll="sec3-2-LT" class="internal">Linear maps from <span class="process-math">\(\R^n\)</span> to <span class="process-math">\(\R^m\)</span></a></li>
<li><a href="sec3-3-LT.html" data-scroll="sec3-3-LT" class="internal">Reflections and Projections</a></li>
<li><a href="sec-LT-Geom.html" data-scroll="sec-LT-Geom" class="internal">Geometry of Linear Transformations</a></li>
<li><a href="sec-sec3-5-LT-Sage.html" data-scroll="sec-sec3-5-LT-Sage" class="internal">Sage Computations</a></li>
</ul>
</li>
<li class="link">
<a href="chap4-Vector-Space.html" data-scroll="chap4-Vector-Space" class="internal"><span class="codenumber">4</span> <span class="title">Vector Spaces</span></a><ul>
<li><a href="sec4-1-VS.html" data-scroll="sec4-1-VS" class="internal">Introduction</a></li>
<li><a href="sec4-2-Abst-VS.html" data-scroll="sec4-2-Abst-VS" class="internal">Vector Subspaces</a></li>
<li><a href="sec4-3-linspan-VS.html" data-scroll="sec4-3-linspan-VS" class="internal">Linear Span</a></li>
<li><a href="sec4-3-LI-VS.html" data-scroll="sec4-3-LI-VS" class="internal">Linear dependence and independence</a></li>
<li><a href="sec4-5-basis-dim-VS.html" data-scroll="sec4-5-basis-dim-VS" class="internal">Basis and dimension</a></li>
<li><a href="sec-sec4-5-VS-Sage.html" data-scroll="sec-sec4-5-VS-Sage" class="internal">Sage Computations</a></li>
<li><a href="sec4-5-VS-Ex.html" data-scroll="sec4-5-VS-Ex" class="internal">Exercise Set</a></li>
</ul>
</li>
<li class="link">
<a href="chap5-Eigen.html" data-scroll="chap5-Eigen" class="internal"><span class="codenumber">5</span> <span class="title">Eigenvalues and Eigenvectors</span></a><ul>
<li><a href="sec5-1-eigen-intro.html" data-scroll="sec5-1-eigen-intro" class="internal">Eigenvalues and Eigenvectors</a></li>
<li><a href="sec5-2-diagonalization.html" data-scroll="sec5-2-diagonalization" class="internal">Diagonalization</a></li>
<li><a href="sec5-5-Eigen-Sage.html" data-scroll="sec5-5-Eigen-Sage" class="internal">Eigenvalues and Eigenvectors in Sage</a></li>
<li><a href="sec5-3-eigen-Appl.html" data-scroll="sec5-3-eigen-Appl" class="internal">Applications of Eigenvalues and Eigenvectors</a></li>
<li><a href="sec5-4-eigen-exer.html" data-scroll="sec5-4-eigen-exer" class="internal">Exercises on Eigenvalues and Eigenvectors</a></li>
</ul>
</li>
<li class="link">
<a href="chap5-orthogonality.html" data-scroll="chap5-orthogonality" class="internal"><span class="codenumber">6</span> <span class="title">Orthogonality</span></a><ul>
<li><a href="sec6-0-orthogonality.html" data-scroll="sec6-0-orthogonality" class="internal">Orthogonality</a></li>
<li><a href="sec6-1-GramSchmidt.html" data-scroll="sec6-1-GramSchmidt" class="internal">Gram-Schmidt Orthogonalization Process</a></li>
<li><a href="sec6-2.html" data-scroll="sec6-2" class="internal">Orthogonal Complements</a></li>
<li><a href="sec6-3.html" data-scroll="sec6-3" class="internal">Orthogonal Diagonalizations</a></li>
<li><a href="sec6-4.html" data-scroll="sec6-4" class="internal">QR-Factorization</a></li>
</ul>
</li>
<li class="link">
<a href="chap7-inner-product.html" data-scroll="chap7-inner-product" class="internal"><span class="codenumber">7</span> <span class="title">Inner Product</span></a><ul>
<li><a href="sec7-1-InnerProduct.html" data-scroll="sec7-1-InnerProduct" class="internal">Inner Product</a></li>
<li><a href="sec7-2-Exer.html" data-scroll="sec7-2-Exer" class="internal">Exercise Set</a></li>
</ul>
</li>
<li class="link">
<a href="chap8-Least-Square.html" data-scroll="chap8-Least-Square" class="internal"><span class="codenumber">8</span> <span class="title">Least Square Problems</span></a><ul><li><a href="sec8-1-LSTSQ.html" data-scroll="sec8-1-LSTSQ" class="internal">Least Square Problems</a></li></ul>
</li>
<li class="link">
<a href="chap9-SVD.html" data-scroll="chap9-SVD" class="internal"><span class="codenumber">9</span> <span class="title">Singular Value Decomposition</span></a><ul><li><a href="sec9-1-SVD.html" data-scroll="sec9-1-SVD" class="internal">Singular Value Decomposition</a></li></ul>
</li>
<li class="link">
<a href="chap10-PCA.html" data-scroll="chap10-PCA" class="internal"><span class="codenumber">10</span> <span class="title">Principal Component Analysis</span></a><ul>
<li><a href="sec10-1-PCA.html" data-scroll="sec10-1-PCA" class="active">Principal Component Analysis</a></li>
<li><a href="sec10-2.html" data-scroll="sec10-2" class="internal">Applications of PCA</a></li>
<li><a href="sec10-3.html" data-scroll="sec10-3" class="internal">Sage Practice Area</a></li>
</ul>
</li>
<li class="link backmatter"><a href="backmatter.html" data-scroll="backmatter" class="internal"><span class="title">Backmatter</span></a></li>
<li class="link">
<a href="appendix-sage-into.html" data-scroll="appendix-sage-into" class="internal"><span class="codenumber">1</span> <span class="title">Introduction to Sage</span></a><ul>
<li><a href="subsection-47.html" data-scroll="subsection-47" class="internal">Introduction to SageMath</a></li>
<li><a href="getting-started-sagemath.html" data-scroll="getting-started-sagemath" class="internal">Getting Started with Sage</a></li>
<li><a href="subsec-sage-programming.html" data-scroll="subsec-sage-programming" class="internal">Programming in Sage</a></li>
<li><a href="subsec-plotting-sage.html" data-scroll="subsec-plotting-sage" class="internal">Plotting in Sage</a></li>
</ul>
</li>
<li class="link"><a href="colophon-2.html" data-scroll="colophon-2" class="internal"><span class="title">Colophon</span></a></li>
</ul></nav><div class="extras"><nav><a class="pretext-link" href="https://pretextbook.org">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="pretext-content"><section class="section" id="sec10-1-PCA"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">10.1</span> <span class="title">Principal Component Analysis</span>
</h2>
<section class="introduction" id="introduction-26"><p id="p-1462">In this chapter, we shall explore the concept of pricipal component analysis (PCA) which is somewhat similar to SVD. We shall look at what is similarity between PCA and SVD along with some applications.</p></section><section class="subsection" id="subsec10-1-intro-pca"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">10.1.1</span> <span class="title">Introduction</span>
</h3>
<p id="p-1463">Large datasets with a large number of features/variables are very common and widespread. Interpreting such a large datasets is very complex task. In order to interpret such datasets one requires a method that reduces the dimension/features drastically, at the same time most of the information in the dateset is preserved. The principal component analysis (PCA) is one of the most widely used dimensionality reduction techniques. The main idea of PCA is to reduce the dimensionality in the datasets while preserving much of the variability as much as possible. It does so by creating a new set of uncorrelated variables that successfully maximize the variance. Finding such new variables also known as principal components reduces the problem to solving an eigenvalue-eigenvector problem.</p>
<p id="p-1464">Let us look at the set of points in the plane, (data with two features) in the <a href="" class="xref" data-knowl="./knowl/fig_pca1.html" title="Figure 10.1.1">figure 10.1.1</a>. In this case the data has maximum spread or variability along the <span class="process-math">\(y\)</span>-axis. Thus if we project, the points onto the <span class="process-math">\(y\)</span>-axis, the variability in the data can be captured. In particular, we can ignore the <span class="process-math">\(x\)</span>-coordinates. On the other hand if we look at the set of points in the <a href="" class="xref" data-knowl="./knowl/fig_pca2.html" title="Figure 10.1.2">figure 10.1.2</a>, maximum spread or variability lies along the <span class="process-math">\(x\)</span>-axis. Thus if we project, the points onto the <span class="process-math">\(x\)</span>-axis, the variability in the data can be captured. In particular, we can ignore the <span class="process-math">\(y\)</span>-coordinates. Thus in these two examples, we are able to reduce the dimension by 1.</p>
<figure class="figure figure-like" id="fig_pca1"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/PCA-Fig1.PNG" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.1<span class="period">.</span></span><span class="space"> </span>Variability along <span class="process-math">\(y\)</span>-axis</figcaption></figure><figure class="figure figure-like" id="fig_pca2"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/PCA-Fig2.PNG" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.2<span class="period">.</span></span><span class="space"> </span>Variability along <span class="process-math">\(x\)</span>-axis</figcaption></figure><p id="p-1465">Now suppose we have 12 points as show in the <a href="" class="xref" data-knowl="./knowl/fig_pca3.html" title="Figure 10.1.3">Figure 10.1.3</a> again in <span class="process-math">\(\R^2\text{,}\)</span> that is having two features/dimensions. The spread of this data seems to be not along <span class="process-math">\(x\)</span>-axis but roughly along the axis as shown in the <a href="" class="xref" data-knowl="./knowl/fig_pca4.html" title="Figure 10.1.4">Figure 10.1.4</a>, that is, along the vector <span class="process-math">\(u_1\text{.}\)</span> So if we project these points on the line along <span class="process-math">\(u_1\)</span> as shown in the <a href="" class="xref" data-knowl="./knowl/fig_pca4.html" title="Figure 10.1.4">Figure 10.1.4</a>, we will have maximum spread or variation of the data. Thus <span class="process-math">\(u_1\)</span> is the new axis along which the data has maximum variation.</p>
<figure class="figure figure-like" id="fig_pca3"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/PCA-Fig3.PNG" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.3<span class="period">.</span></span><span class="space"> </span>Data with two features.</figcaption></figure><figure class="figure figure-like" id="fig_pca4"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/PCA-Fig4.PNG" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.4<span class="period">.</span></span><span class="space"> </span>Variability along vector <span class="process-math">\(u_1\text{.}\)</span></figcaption></figure><p id="p-1466">Next if one carefully looks into the data points, one can see that the data also has some dispersion or variation along the line given by the direction <span class="process-math">\(u_2\)</span> as shown in the <a href="" class="xref" data-knowl="./knowl/fig_pca5.html" title="Figure 10.1.5">Figure 10.1.5</a> and which is not captured by the line along <span class="process-math">\(u_1\text{.}\)</span> In a way, we need to create another axis which is perpendicular to the 1st one.</p>
<p id="p-1467">Thus we have two perpendicular coordinate axes or a new coordinates system along which all the variations in the data can be captured. In this case, maximum variation along <span class="process-math">\(u_1\)</span> and second maximum along <span class="process-math">\(u_2\text{.}\)</span> Here <span class="process-math">\(u_1\)</span> is called the first principal direction and <span class="process-math">\(u_2\)</span> is called the second principal direction. Thus we can work with new coordinate axes and forget about the original <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span>-axes as show in the <a href="" class="xref" data-knowl="./knowl/fig_pca6.html" title="Figure 10.1.6">Figure 10.1.6</a>. We can even rotate the new coordinate system that coincides with original <span class="process-math">\(x\)</span> and <span class="process-math">\(y\)</span>-axes.</p>
<figure class="figure figure-like" id="fig_pca5"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/PCA-Fig5.PNG" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.5<span class="period">.</span></span><span class="space"> </span>1st and 2nd principal components <span class="process-math">\(u_1\)</span> and <span class="process-math">\(u_2\text{.}\)</span></figcaption></figure><figure class="figure figure-like" id="fig_pca6"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/PCA-Fig6.PNG" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.6<span class="period">.</span></span><span class="space"> </span>1st and 2nd principal components <span class="process-math">\(u_1\)</span> and <span class="process-math">\(u_2\text{.}\)</span></figcaption></figure><p id="p-1468">The above two examples, geometrically explains the essence of PCA. The idea is to project the original high dimensional data to a new coordinate system and choose only first we coordinates axes also called principal components. How many principal component to be taken depends upon how much variation we wish to capture.</p></section><section class="subsection" id="subsection-42"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">10.1.2</span> <span class="title">Mathematics behind PCA</span>
</h3>
<p id="p-1469">Let us assume that we have a data which has <span class="process-math">\(d\)</span> features and there are <span class="process-math">\(n\)</span> of them. This data can be represented by a <span class="process-math">\(n\times d\)</span> matrix, say <span class="process-math">\(X\text{.}\)</span> Thus</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
X = \begin{bmatrix}x_{11} \amp  x_{12} \amp  \cdots \amp  x_{1d}\\ x_{21} \amp  x_{22} \amp  \cdots \amp  x_{2d}\\ \vdots \amp  \vdots \amp  \ddots \amp  \vdots\\ x_{n1} \amp  x_{n2} \amp  \cdots \amp  x_{nd} \end{bmatrix}\text{.}
\end{equation*}
</div>
<p class="continuation">Thus each columns of <span class="process-math">\(X\)</span> represents a feature and there are <span class="process-math">\(n\)</span> samples for each feature.</p>
<p id="p-1470">Now we are looking for an unit vector <span class="process-math">\(u_1\)</span> and we wish to project the data onto <span class="process-math">\(u_1\)</span> such that the variance of the projected data is maximum.</p>
<p id="p-1471">Before we explain that in generality, let us look at what is meaning of projection of data in 2 dimension (that is in <span class="process-math">\(\R^2\)</span>) on an unit vector. Suppose <span class="process-math">\(u=(a,b)\)</span> is an unit vector ad <span class="process-math">\(p_1=(x_1,y_1)\)</span> be a point/vector in <span class="process-math">\(\R^2\text{.}\)</span> Then</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\proj_u(p_1)=\frac{p_1\cdot u}{\norm{u}^2}u=(x_1a+x_2b)u\text{.}
\end{equation*}
</div>
<p id="p-1472">The length of the projection is <span class="process-math">\(p_1x_2+p_2x_2\text{.}\)</span> If we have another point, say <span class="process-math">\(p_2 =(x_2,y_2)\text{,}\)</span> then the projection of both these points can be captured as</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{bmatrix}x_1 \amp  x_2 \\y_1 \amp  y_2 \end{bmatrix}  \begin{bmatrix}a\\b \end{bmatrix}  = Xu\text{.}
\end{equation*}
</div>
<p id="p-1473">Thus in general the projection of data <span class="process-math">\(X\)</span> which is <span class="process-math">\(n\times p\)</span> matrix onto a unit vector <span class="process-math">\(u_1=\begin{bmatrix}u_{11}\amp  u_{12}\amp \cdots \amp  u_{1d} \end{bmatrix} ^T\)</span> is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
X = \begin{bmatrix}x_{11} \amp  x_{12} \amp  \cdots \amp  x_{1d}\\ x_{21} \amp  x_{22} \amp  \cdots \amp  x_{2d}\\ \vdots \amp  \vdots \amp  \ddots \amp  \vdots\\ x_{n1} \amp  x_{n2} \amp  \cdots \amp  x_{nd} \end{bmatrix} \begin{bmatrix}u_{11}\\ u_{12}\\\vdots \\u_{1d} \end{bmatrix} =Xu_1\text{.}
\end{equation*}
</div>
<p id="p-1474">Next we deal with the second issue in PCA, namely, 'variance'. For this we take the centered data <span class="process-math">\(X_c =X-\overline{X}\text{,}\)</span> where <span class="process-math">\(\overline{X}=\frac{1}{d}\begin{bmatrix}x_{11}+x_{12}+\cdots+x_{1d}\\ x_{21}+x_{22}+\cdots+x_{2d}\\\vdots\\x_{1n}+x_{1n}+\cdots+x_{1n} \end{bmatrix}\text{.}\)</span> The covariance of <span class="process-math">\(X\text{,}\)</span> is given</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
S ={ Cov}(X)=\frac{1}{n-1}{X_c}^TX_c\text{.}
\end{equation*}
</div>
<p id="p-1475">Note that (i) <span class="process-math">\(S\)</span> is symmetric and (ii) Semi-positive definite, all eigenvalues of <span class="process-math">\(S\)</span> are non negative. Also <span class="process-math">\(S\)</span> is orthogonally diagonalizable. In particular, there exists an orthogonal matrix <span class="process-math">\(U = \begin{bmatrix}u_1\amp u_2\amp \cdots \amp  u_d \end{bmatrix}\)</span> such that <span class="process-math">\(U^TSU = { diag }(\lambda_1,\cdots,\lambda_p)\text{.}\)</span> What we wanted was to maximize the variance of projection of the data onto unit vector <span class="process-math">\(u\text{.}\)</span> That is, we want to find an unit vector <span class="process-math">\(u\)</span> such that the variance of <span class="process-math">\(X_cu\)</span> is maximum. In other words,</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-78">
\begin{align*}
\text{maximize }\amp \frac{1}{n-1}(X_cu)^T{X_cu}=\frac{1}{n-1}u^T(X_c^TX_c)u=u^TSu\\
\text{subject to } \amp  \norm{u}=1\text{.}
\end{align*}
</div>
<p id="p-1476">It turns out that the solution of this optimization problem is <span class="process-math">\(u\text{,}\)</span> which is the eigenvector of <span class="process-math">\(S\text{.}\)</span> Thus the variance of the projected data onto a unit vector is maximum if <span class="process-math">\(u\)</span> happens to be an eigenvector of the covariance matrix <span class="process-math">\(S\text{.}\)</span></p>
<p id="p-1477">Note that <span class="process-math">\(S\)</span> is of order <span class="process-math">\(d\times d\)</span> which has <span class="process-math">\(d\)</span> linearly independent eigenvectors. We arrange these eigenvector corresponding to the decreasing eigenvalues. That <span class="process-math">\(u_1\)</span> is the eigenvector corresponding to the largest eigenvector <span class="process-math">\(\lambda_1\)</span> and is called the first principal component. The eigenvector <span class="process-math">\(u_2\)</span> corresponding to the second highest eigenvalue <span class="process-math">\(\lambda_2\text{,}\)</span> is called the second principal component. Thus if we project data onto the second principal component that it will have second higher variance. Look at <a href="" class="xref" data-knowl="./knowl/fig_data_pca.html" title="Figure 10.1.7">Figure 10.1.7</a> in which the data is plotted along with the principal components. The <a href="" class="xref" data-knowl="./knowl/fig_datapac_proj.html" title="Figure 10.1.8">Figure 10.1.8</a>, the data projected on the 1st component of PCA is plotted along with the data.</p>
<figure class="figure figure-like" id="fig_data_pca"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/data_pca.png" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.7<span class="period">.</span></span><span class="space"> </span>Data set with principal components</figcaption></figure><figure class="figure figure-like" id="fig_datapac_proj"><div class="image-box" style="width: 45%; margin-left: 27.5%; margin-right: 27.5%;"><img src="external/images/datapca_proj.png" class="contained"></div>
<figcaption><span class="type">Figure</span><span class="space"> </span><span class="codenumber">10.1.8<span class="period">.</span></span><span class="space"> </span>Projection on 1st PCA components</figcaption></figure><p id="p-1478">Next question is how many principal components, we should choose. This depends upon what percentage of variance of the data we wish to capture. Suppose we want to capture 90% variations, the we choose the 1st <span class="process-math">\(k\)</span> components such that</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{\sum_{i=1}^k \lambda_i}{\sum_{j=1}^d\lambda_j}\geq 0.9\text{.}
\end{equation*}
</div>
<p id="p-1479">The projected data onto the 1st <span class="process-math">\(k\)</span> principal components is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\begin{bmatrix}x_{11} \amp  x_{12} \amp  \cdots \amp  x_{1d}\\ x_{21} \amp  x_{22} \amp  \cdots \amp  x_{2d}\\ \vdots \amp  \vdots \amp  \ddots \amp  \vdots\\ x_{n1} \amp  x_{n2} \amp  \cdots \amp  x_{nd} \end{bmatrix} \begin{bmatrix}u_1\amp u_2\amp \cdots \amp  u_k \end{bmatrix} =XV= \begin{bmatrix}z_{11} \amp  z_{12} \amp  \cdots \amp  z_{1k}\\ z_{21} \amp  z_{22} \amp  \cdots \amp  z_{2k}\\ \vdots \amp  \vdots \amp  \ddots \amp  \vdots\\ z_{n1} \amp  z_{n2} \amp  \cdots \amp  z_{nk} \end{bmatrix} =Z
\end{equation*}
</div>
<p id="p-1480">Here <span class="process-math">\(V\)</span> is called the loading matrix. The new data or transformed data <span class="process-math">\(Z=XV\text{.}\)</span> Once we know the transformed data then we can construct the original data by <span class="process-math">\(X=ZV^T\text{.}\)</span></p>
<article class="example example-like" id="example-196"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">10.1.9</span><span class="period">.</span>
</h4>
<p id="p-1481">Consider the following 2 dimensional data.</p>
<div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b2 r3 l3 t3 lines"><span class="process-math">\(x_1\)</span></td>
<td class="l m b2 r3 l0 t3 lines">2.5</td>
<td class="l m b2 r3 l0 t3 lines">0.5</td>
<td class="l m b2 r3 l0 t3 lines">2.2</td>
<td class="l m b2 r3 l0 t3 lines">1.9</td>
<td class="l m b2 r3 l0 t3 lines">3.0</td>
<td class="l m b2 r3 l0 t3 lines">2.3</td>
<td class="l m b2 r3 l0 t3 lines">2.0</td>
<td class="l m b2 r3 l0 t3 lines">1.0</td>
<td class="l m b2 r3 l0 t3 lines">1.5</td>
<td class="l m b2 r2 l0 t3 lines">1.1</td>
</tr>
<tr>
<td class="l m b2 r3 l3 t0 lines"><span class="process-math">\(x_2\)</span></td>
<td class="l m b2 r3 l0 t0 lines">2.0</td>
<td class="l m b2 r3 l0 t0 lines">0.7</td>
<td class="l m b2 r3 l0 t0 lines">2.9</td>
<td class="l m b2 r3 l0 t0 lines">2.2</td>
<td class="l m b2 r3 l0 t0 lines">2.8</td>
<td class="l m b2 r3 l0 t0 lines">2.7</td>
<td class="l m b2 r3 l0 t0 lines">1.6</td>
<td class="l m b2 r3 l0 t0 lines">1.1</td>
<td class="l m b2 r3 l0 t0 lines">1.6</td>
<td class="l m b2 r2 l0 t0 lines">0.9</td>
</tr>
</table></div>
<p id="p-1482">Find the first and the second principal components of this data set. Explain what percentage of variance os explained by the 1st principal component.</p>
<p id="p-1483">The <span class="process-math">\(\overline{x_1} =1.8\)</span> and <span class="process-math">\(\overline{x_2}=1.85\text{.}\)</span> The centered data set is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
X_c = X - \begin{bmatrix}\overline{x_1}\\\overline{x_2} \end{bmatrix} =\left(\begin{array}{rr} 0.7 \amp  0.15 \\ -1.3 \amp  -1.15\\ 0.4 \amp  1.05\\ 0.1 \amp  0.35 \\ 1.2 \amp  0.95 \\ 0.5\amp  0.85 \\ 0.2 \amp  -0.25\\ -0.8 \amp  -0.75 \\ -0.3 \amp  -0.25 \\ -0.7 \amp  -0.95 \end{array} \right)
\end{equation*}
</div>
<p id="p-1484">Next we construct the covariance matrix of <span class="process-math">\(X\text{,}\)</span> which is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
S = \frac{1}{10-1}X_c^TX_c=\left(\begin{array}{rr} 0.589 \amp  0.546 \\ 0.546 \amp  0.643 \end{array} \right)
\end{equation*}
</div>
<p id="p-1485">The eigenvalues of <span class="process-math">\(S\)</span> are eigenvalues <span class="process-math">\(\lambda_1 =1.1620\)</span> and <span class="process-math">\(\lambda_2=0.0696\text{.}\)</span> The corresponding eigenvectors are <span class="process-math">\(u_1 = \begin{pmatrix}0.6894\\0.7243 \end{pmatrix}\)</span> and <span class="process-math">\(u_2 = \begin{pmatrix}0.7243\\-0.689 \end{pmatrix}\text{.}\)</span></p>
<p id="p-1486">Hence the loading matrix <span class="process-math">\(V\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
V = \left(\begin{array}{rr} 0.6894 \amp  0.7243 \\ 0.7243 \amp  -0.6894 \end{array} \right)\text{.}
\end{equation*}
</div>
<p class="continuation">The projected data on the 1st two principal components is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
Z = X_cV = \left(\begin{array}{rr} 0.591 \amp  0.404 \\ -1.73 \amp  -0.149 \\ 1.04 \amp  -0.434 \\ 0.322 \amp  -0.169 \\ 1.52 \amp  0.214 \\ 0.960 \amp  -0.224 \\ -0.0432 \amp  0.317 \\ -1.09 \amp  -0.0624 \\ -0.388 \amp  -0.0449 \\ -1.17 \amp  0.148 \end{array} \right)
\end{equation*}
</div>
<p id="p-1487">We can recover the original data set by</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
ZV^T+\left(\begin{array}{rr} 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \\ 1.80 \amp  1.85 \end{array} \right)=X
\end{equation*}
</div>
<p id="p-1488">The variance explained by the 1st principal component is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{\lambda_1}{\lambda_1+\lambda_2}\approx 0.9435
\end{equation*}
</div>
<p id="p-1489">Thus approximately 94.35% variance is captured by the 1st principal component.</p></article><p id="p-1490">Let us solve the above problem in Sage.</p>
<pre class="ptx-sagecell sagecell-sage" id="sage-317"><script type="text/x-sage">xx = [2.5,0.5,2.2,1.9,3.0,2.3,2.0,1.0,1.5,1.1]
yy = [2.0,0.7,2.9,2.2,2.8,2.7,1.6,1.1,1.6,0.9]
X = column_matrix([xx,yy])
n = len(xx)
d = 2
</script></pre>
<p id="p-1491">Next we plot the set of points.</p>
<pre class="ptx-sagecell sagecell-sage" id="sage-318"><script type="text/x-sage">pts = points([(xx[i],yy[i]) for i in range(n)],size=10)
show(pts,axes=(0,0),xmin=-0.2,ymin=-0.2,figsize=4)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-319"><script type="text/x-sage">xxm = mean(xx)
yym = mean(yy)
print(xxm.n(digits=3),yym.n(digits=3))    
mean_matrix = column_matrix([[xxm for i in range(n)],[yym for i in range(n)]])
Xc = X-mean_matrix;
Xc.n(digits=2)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-320"><script type="text/x-sage">mean_pts = points(Xc,size=10,color='blue');mean_pts
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-321"><script type="text/x-sage">S = 1/(n-1)*(Xc.T)*Xc
print('The Covariance matrix is')
S = S.change_ring(CDF)
eigenvalues = S.eigenvalues()
print('The principal components are:')
print(eigenvalues)
print('The Eigenvectors of covariance matrix are')
evec=S.eigenvectors_right();
print(evec)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-322"><script type="text/x-sage">pc1 = evec[1][1][0]
pc2 = evec[0][1][0]
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-323"><script type="text/x-sage"> pca_plot = (2*pc1).plot(color='green')+pc2.plot(color='red')+mean_pts
show(pca_plot,aspect_ratio=1)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-324"><script type="text/x-sage">V = column_matrix([pc1,pc2])
Z = Xc*V; 
print(Z)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-325"><script type="text/x-sage"> X1 = Z*V.T;
X1.n(digits=3)
print(X1)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-326"><script type="text/x-sage">print(Z*V+mean_matrix)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-327"><script type="text/x-sage">## Variance explaines by the 1st PAC.
eigenvalues[1]/(sum(eigenvalues))*100
</script></pre>
<article class="example example-like" id="pca-eg2"><h4 class="heading">
<span class="type">Example</span><span class="space"> </span><span class="codenumber">10.1.10</span><span class="period">.</span>
</h4>
<p id="p-1492">Consider the following data in 3-dimension.</p>
<p id="p-1493"><div class="tabular-box natural-width"><table class="tabular">
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(x_1\)</span></td>
<td class="l m b0 r0 l0 t0 lines">24</td>
<td class="l m b0 r0 l0 t0 lines">8</td>
<td class="l m b0 r0 l0 t0 lines">21</td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">9</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
<td class="l m b0 r0 l0 t0 lines">8</td>
<td class="l m b0 r0 l0 t0 lines">10</td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">15</td>
<td class="l m b0 r0 l0 t0 lines">4</td>
<td class="l m b0 r0 l0 t0 lines">12</td>
<td class="l m b0 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(x_2\)</span></td>
<td class="l m b0 r0 l0 t0 lines">13</td>
<td class="l m b0 r0 l0 t0 lines">3</td>
<td class="l m b0 r0 l0 t0 lines">6</td>
<td class="l m b0 r0 l0 t0 lines">14</td>
<td class="l m b0 r0 l0 t0 lines">3</td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
<td class="l m b0 r0 l0 t0 lines">16</td>
<td class="l m b0 r0 l0 t0 lines">3</td>
<td class="l m b0 r0 l0 t0 lines">2</td>
<td class="l m b0 r0 l0 t0 lines">6</td>
<td class="l m b0 r0 l0 t0 lines">10</td>
<td class="l m b0 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="l m b1 r0 l0 t0 lines"><span class="process-math">\(x_3\)</span></td>
<td class="l m b1 r0 l0 t0 lines">38</td>
<td class="l m b1 r0 l0 t0 lines">17</td>
<td class="l m b1 r0 l0 t0 lines">40</td>
<td class="l m b1 r0 l0 t0 lines">-9</td>
<td class="l m b1 r0 l0 t0 lines">21</td>
<td class="l m b1 r0 l0 t0 lines">14</td>
<td class="l m b1 r0 l0 t0 lines">11</td>
<td class="l m b1 r0 l0 t0 lines">3</td>
<td class="l m b1 r0 l0 t0 lines">2</td>
<td class="l m b1 r0 l0 t0 lines">30</td>
<td class="l m b1 r0 l0 t0 lines">1</td>
<td class="l m b1 r0 l0 t0 lines">18</td>
<td class="l m b1 r0 l0 t0 lines"></td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(x_1\)</span></td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
<td class="l m b0 r0 l0 t0 lines">5</td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">21</td>
<td class="l m b0 r0 l0 t0 lines">8</td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">15</td>
<td class="l m b0 r0 l0 t0 lines">16</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
<td class="l m b0 r0 l0 t0 lines">14</td>
<td class="l m b0 r0 l0 t0 lines">3</td>
<td class="l m b0 r0 l0 t0 lines">5</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(x_2\)</span></td>
<td class="l m b0 r0 l0 t0 lines">9</td>
<td class="l m b0 r0 l0 t0 lines">3</td>
<td class="l m b0 r0 l0 t0 lines">1</td>
<td class="l m b0 r0 l0 t0 lines">12</td>
<td class="l m b0 r0 l0 t0 lines">9</td>
<td class="l m b0 r0 l0 t0 lines">8</td>
<td class="l m b0 r0 l0 t0 lines">18</td>
<td class="l m b0 r0 l0 t0 lines">8</td>
<td class="l m b0 r0 l0 t0 lines">10</td>
<td class="l m b0 r0 l0 t0 lines">0</td>
<td class="l m b0 r0 l0 t0 lines">2</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
<td class="l m b0 r0 l0 t0 lines">6</td>
</tr>
<tr>
<td class="l m b0 r0 l0 t0 lines"><span class="process-math">\(x_3\)</span></td>
<td class="l m b0 r0 l0 t0 lines">-4</td>
<td class="l m b0 r0 l0 t0 lines">19</td>
<td class="l m b0 r0 l0 t0 lines">13</td>
<td class="l m b0 r0 l0 t0 lines">-6</td>
<td class="l m b0 r0 l0 t0 lines">34</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
<td class="l m b0 r0 l0 t0 lines">-18</td>
<td class="l m b0 r0 l0 t0 lines">25</td>
<td class="l m b0 r0 l0 t0 lines">29</td>
<td class="l m b0 r0 l0 t0 lines">17</td>
<td class="l m b0 r0 l0 t0 lines">31</td>
<td class="l m b0 r0 l0 t0 lines">0</td>
<td class="l m b0 r0 l0 t0 lines">7</td>
</tr>
</table></div></p>
<p id="p-1494">The mean of each feature are <span class="process-math">\(\overline{x_1}=8.96, \overline{x_2}=7.081,\overline{x_2}=3.6\text{.}\)</span> We have</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
X= \left(\begin{array}{rrr} 24 \amp  13 \amp  38 \\ 8 \amp  3 \amp  17 \\\vdots \amp  \vdots \amp  \vdots \\ 5 \amp  6 \amp  7 \end{array} \right), X -\overline{X} = \left(\begin{array}{rrr} 15.04 \amp  5.92 \amp  24.4 \\ -0.96 \amp  -4.08 \amp  3.4 \\\vdots \amp  \vdots \amp \vdots\\ -3.96 \amp  -1.08 \amp  -6.6 \end{array} \right)
\end{equation*}
</div>
<p id="p-1495">The covariance matrix of <span class="process-math">\(X\)</span> is given by</p>
<div class="displaymath process-math" data-contains-math-knowls="" id="md-79">
\begin{align*}
{ Cov}(X)=S=\amp \frac{1}{n-1}(X -\overline{X})^T(X -\overline{X})\\
=\amp \left(\begin{array}{rrr} 45.7066 \amp  -0.2466\amp  94.8583 \\ -0.2466 \amp  24.0766 \amp  -29.175 \\ 94.8583\amp  -29.175 \amp  235.9166 \end{array} \right)
\end{align*}
</div>
<p id="p-1496">The eigenvalues of <span class="process-math">\(S\)</span> are <span class="process-math">\(\lambda_1=278.02366293, \lambda_2=26.95307696, \lambda_3=0.72326011\text{.}\)</span> The corresponding eigenvectors are</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
pc_1=\left(\begin{array}{r} 0.3759787 \\ -0.10612 \\ 0.920531 \end{array} \right), pc_2=\left(\begin{array}{r} 0.46959\\ 0.87822 \\ -0.09055 \end{array} \right), pc_3=\left(\begin{array}{r} 0.79882 \\ -0.4663\\ -0.38003 \end{array} \right)\text{.}
\end{equation*}
</div>
<p id="p-1497">The percentage of variance explained by the first principal component is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{\lambda_1}{\lambda_1+\lambda_2+\lambda_3}\approx 0.9094\approx  90.94%
\end{equation*}
</div>
<p class="continuation">The percentage of variance explained by the first two principal components is</p>
<div class="displaymath process-math" data-contains-math-knowls="">
\begin{equation*}
\frac{\lambda_1+\lambda_2}{\lambda_1+\lambda_2+\lambda_3}\approx 0.997=99.7%
\end{equation*}
</div></article><p id="p-1498">Let us solve the above problem in Sage.</p>
<pre class="ptx-sagecell sagecell-sage" id="sage-328"><script type="text/x-sage">import numpy as np
X = np.array([[24, 13, 38], [8, 3, 17], [21, 6, 40], [1, 14, -9], [9, 3, 21], [7, 1, 14],
              [8, 7, 11], [10, 16, 3], [1, 3, 2], [15, 2, 30], [4, 6, 1], [12, 10, 18], [1, 9, -4],
              [7, 3, 19], [5, 1, 13], [1, 12, -6], [21, 9, 34], [8, 8, 7], [1, 18, -18],
              [15, 8, 25], [16, 10, 29], [7, 0, 17], [14, 2, 31], [3, 7, 0], [5, 6, 7]])
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-329"><script type="text/x-sage">u = X[:,0] # First column of X
v = X[:,1] # Second column of X
w = X[:,2] # Third column of X
n = len(u)
u_bar = np.mean(u) # Mean of the first column
v_bar = np.mean(v) # Mean of the second column
w_bar = np.mean(w) # Mean of the third column
u1=u-u_bar 
v1 = v-v_bar
w1 = w-w_bar
X_bar = np.array([u1,v1,w1]).T # Adjusted data
X1 = X-np.mean(X,axis=0) # Adjusted data
S1 = 1/(n-1)*(X1.T).dot(X1)
print(f'The covaraince matrix is:\n {S1}')
eigens = np.linalg.eigvals(S1)
print(f'Eigenvalues of the covariance matrix are:\n{eigens}')
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-330"><script type="text/x-sage">eig_val,eig_vec= np.linalg.eig(S1)
print('Eigenvalues are:',eig_val)
print('Matrix of eigenvectors:',eig_vec)
print("Eigenvectors are:", eig_vec[:,0], 'and', eig_vec[:,1], eig_vec[:,2])
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-331"><script type="text/x-sage">S1 = matrix(S1).change_ring(CDF)
S1.eigenmatrix_right()
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-332"><script type="text/x-sage">evec=S1.eigenvectors_right();
print(evec)
pc1 = evec[0][1][0]
pc2 = evec[2][1][0]
pc3 = evec[1][1][0];pc3
print('The pricipal components are')
print(pc1,pc2,pc3)
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-333"><script type="text/x-sage"># Variance explained by the 1st PCA 
eig_val[0]/sum(eig_val)*100
</script></pre>
<pre class="ptx-sagecell sagecell-sage" id="sage-334"><script type="text/x-sage"># Variance explained by the 1st two PCAs
(eig_val[0]+eig_val[2])/sum(eig_val)*100
</script></pre>
<p id="p-1499">We can vizualize the PCA in the 3d. We use a simulated data of 200 points in 3d.</p>
<pre class="ptx-sagecell sagecell-sage" id="sage-335"><script type="text/x-sage">import numpy as np

# Seed for reproducibility
np.random.seed(42)

# Generate 3D data with correlation
# First principal axis is along (1, 2, 0.5)
n_points = 200
t = np.linspace(-5, 5, n_points)
x = t + np.random.normal(0, 0.5, n_points)
y = 2*t + np.random.normal(0, 0.5, n_points)
z = 0.5*t + np.random.normal(0, 0.5, n_points)

X = np.vstack((x, y, z)).T

# Mean-center
X_mean = np.mean(X, axis=0)
X_centered = X - X_mean

# Covariance matrix
cov_matrix = np.cov(X_centered, rowvar=False)

# Eigen decomposition
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# Sort by descending eigenvalue
idx = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[idx]
eigenvectors = eigenvectors[:, idx]

# Scale for plotting
scale = 3
pc_vectors = [scale*np.sqrt(ev)*vec for ev, vec in zip(eigenvalues, eigenvectors.T)]

# Plot points
points3d_list = [point3d(p, size=20, color='blue') for p in X_centered]

# Plot principal component arrows
origin = vector([0, 0, 0])
pca_arrows = [arrow3d(origin, vector(v), color='red', width=2) for v in pc_vectors]

# Show plot
p = sum(points3d_list) + sum(pca_arrows)
p.show(viewer='threejs', frame=True)
</script></pre></section></section></div></main>
</div>
</body>
</html>
