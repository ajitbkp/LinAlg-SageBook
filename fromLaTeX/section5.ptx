<section>
  <title>Eigenvalues and Eigenvectors</title>
  <introduction>
    <definition>
      <statement>
        <p>
          Let <m>T</m> be a linear transformation from <m>\mathbb{R}^n\to \R^n</m>.
          A real number (scalar) is called an <em>eigenvalue</em>
          of <m>T</m> if there exists a non zero vector <m>v\in \R^n</m>
          (called an<em> eigenvector</em>
          corresponding to eigenvalue <m>\lambda</m>)
          if <m>T(v)=\lambda v</m>.
          That is, if <m>T(v)</m> is parallel to <m>v</m>.
        </p>
        <p>
          Thus if <m>T(v)=\lambda v</m>,
          then <m>{\lambda I-T}(v)=0</m>,
          where <m>I</m> is identity transformation on <m>V</m>.
        </p>
        <p>
          If <m>A</m> is an <m>n\times n</m> real matrix matrix,
          then we know that <m>T_A(x)=Ax</m> is a linear transformation induced by <m>A</m>.
          We can define eigenvalue of <m>A</m> as eigenvalue of <m>T_A</m>.
          In particular, real number is called an <em>eigenvalue</em>
          of <m>A</m> if there exists a non zero vector <m>v\in \R^n</m>
          (called an<em> eigenvector</em>
          corresponding to eigenvalue <m>\lambda</m>)
          if <m>Av=\lambda v</m>.
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let <m>A=\left(\begin{array}{rr} 1 \amp 2 \\ -1 \amp 4 \end{array} \right)</m>.
          Consider a vector <m>u=\begin{pmatrix}1\\1 \end{pmatrix}</m>.
          Then <m>Au=\begin{pmatrix}3\\3 \end{pmatrix} =3\begin{pmatrix}1\\1 \end{pmatrix} =3u</m>.
          Hence <m>u</m> is an eigenvector and <m>\lambda = 3</m> is an eigenvalue.
        </p>
        <p>
          Consider <m>v=\begin{pmatrix}2\\1 \end{pmatrix}</m>.
          Then it is easy to check that <m>Av=\begin{pmatrix}4\\2 \end{pmatrix} =2\begin{pmatrix}2\\1 \end{pmatrix}</m>.
          Hence <m>v</m> is also an eigenvector and <m>\lambda = 2</m> is an eigenvalue.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Consider the matrix of rotation
          <m>R_\theta</m> in anti-clock wise by an angle <m>\theta\neq n\pi</m>.
          The it is easy to see that
          <m>R_\theta</m> does not have an eigenvector.
          Thus not all square matrices have eigenvector.
        </p>
      </statement>
    </example>
    <remark>
      <p>
        If <m>\lambda</m> is an eigenvalue of <m>A</m> with corresponding eigenvector <m>v\neq 0</m>.
        Then any scalar multiple of <m>v</m> is also an eigenvector corresponding to the same eigenvalue <m>\lambda</m>.
      </p>
    </remark>
    <p>
      Let us analyze the notion of eigenvalues and eigenvector.
      If <m>v</m> is a eigenvector corresponding to an eigenvalue <m>\lambda</m>.
      Then <m>Av=\lambda v</m>.
      This implies <m>(\lambda I-A)v=0</m>,
      where <m>I</m> is <m>n\times n</m> identity matrix.
      This means that the homogeneous system
      <m>(\lambda I-A)x=0</m> has a non zero solution, namely <m>v</m>.
      Hence <m>\det{(\lambda I-A)}=0</m>.
      Notice that <m>\det(A-\lambda I)</m> is a polynomial
      (called the characteristic polynomial of <m>A</m>)
      of degree <m>n</m> in <m>\lambda</m>.
      Thus if <m>Av=\lambda v</m>,
      then <m>\lambda</m> is a root of the the characteristic polynomial <m>\det(A-\lambda I)</m>.
      By fundamental theorem of algebra an
      <m>n\times n</m> real matrix can have at most <m>n</m> real eigenvalues.
      The equation <m>\det(A-\lambda I)=0</m> is called
      <em>characteristic equation</em> of <m>A</m>.
    </p>
    <p>
      We can write <m>\det(A-\lambda I)=0</m> as <m>\det(A-\lambda I)=\lambda^n-c_1\lambda^{n-1}+c_2\lambda^{n-2}-\cdots+(-1)^nc_n=0</m>.
      If <m>\lambda_1,\ldots, \lambda_n</m> are roots of the characteristics equation,
      then one can show that
      <me>
        \lambda_1+\cdots+\lambda_n=c_1={ trace(A)}; \lambda_1\lambda_2+\cdots +\lambda_{n-1}\lambda_n=c_2;\ldots  \lambda_1\lambda_2\cdots\lambda_n=c_n=\det(A)
      </me>.
    </p>
    <example xml:id="eigen_eg0">
      <statement>
        <p>
          Let <m>A = \begin{pmatrix}1 \amp 1 \amp 1\\ 1 \amp 1 \amp 1\\1 \amp 1 \amp 1 \end{pmatrix}</m>.
          What are eigenvalues and eigenvectors of <m>A</m>?
        </p>
        <p>
          Note that <m>Ae_1=Ae_2=Ae_3=e_1+e_2+e_3</m>.
          This means <m>A(e_1+e_2+e_3)=3(e_2+e_2+e_3)</m>.
          Hence <m>3</m> is an eigenvalue and
          <m>e_1+e_2+e_3=\begin{pmatrix}1 \\1\\1 \end{pmatrix}</m> is an eigenvectors w.r.t. eigenvalue 3.
        </p>
        <p>
          Also <m>A(e_1-e_2)=0</m>.
          Hence <m>0</m> is an eigenvalue and
          <m>e_1-e_2=\begin{pmatrix}1 \\-1\\0 \end{pmatrix}</m> is an eigenvectors w.r.t. eigenvalue 0.
          Also, <m>e_1-e_3</m> and <m>e_2-e_3</m> are also eigenvectors corresponding to the eigenvalue 0.
        </p>
        <p>
          Note that in this example,
          we are able to find eigenvalues and eigenvectors by inspection and without going through characteristic polynomials.
        </p>
        <p>
          What will be generalization of this example?
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let <m>A = \begin{pmatrix}1 \amp t \amp t\\ t \amp 1 \amp t\\t \amp t \amp 1 \end{pmatrix}</m>.
          What are eigenvalues and eigenvectors of <m>A</m>?
        </p>
        <p>
          The trace of <m>A</m> is 3.
          The <m>\det{(A)}=2t^3 - 3t^2 + 1=(2t + 1)(t - 1)^2</m>.
          Since sum of eigenvalues is 3 and the product of eigenvalues is <m>\det{(A)}</m>,
          it is easy to guess that <m>\lambda_1 =2t+1</m>,
          <m>\lambda_2=\lambda_3=1-t</m> are eigenvalues of <m>A</m>.
        </p>
      </statement>
    </example>
    <example xml:id="eigen_eg1">
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m>.
          Find eigenvalues and corresponding eigenvector of <m>A</m>.
        </p>
        <p>
          We have <m>\det(A-\lambda I)=\begin{vmatrix}1-\lambda\amp 2\amp -2\\1\amp 1-\lambda\amp 1\\1\amp 3\amp -1-\lambda\end{vmatrix}=\lambda^3-\lambda^2-4\lambda+4</m>.
          It is easy to see that characteristic polynomial
          <m>\det(A-\lambda I)</m> has roots <m>\lambda=1, \lambda=-2, \lambda=2</m>.
          Thus <m>A</m> has eigenvalues <m>1, -2, 2</m>.
        </p>
        <p>
          Let us find eigenvectors with respect to the eigenvalue <m>\lambda=1</m>.
          Let <m>v=\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}</m> be an eigenvector corresponding to <m>\lambda=1</m>.
          Then <m>Av=\lambda v=v</m>.
          That is,
          <me>
            \begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix} \begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix} =\begin{pmatrix}x_1\\x_2\\x_3 \end{pmatrix}
          </me>.
        </p>
        <p>
          This gives a system of linear equations
          <me>
            x_1+2x_2-2x_3=x_1; x_1+x_2+x_3=x_2; x_1+3x_2-x_3=x_3
          </me>
        </p>
        <p>
          Solving the above system, we get <m>x_1=-x_2, x_2=x_3</m>.
          Thus <m>v=\begin{pmatrix}\alpha\\-\alpha\\-\alpha \end{pmatrix}</m> for <m>\alpha\in \R</m> is an eigenvector.
          In particular,
          <m>v=\begin{pmatrix}1\\-1\\-1 \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=1</m>.
        </p>
        <p>
          Similarly show that <m>\begin{pmatrix}0\\1\\1 \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=2</m> and
          <m>\begin{pmatrix}8/7\\-5/7\\1 \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=-2</m>
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let <m>\begin{pmatrix}0\amp 1\\-1\amp 0 \end{pmatrix}</m>.
          Find eigenvalues and corresponding eigenvector of <m>A</m>.
        </p>
        <p>
          The characteristic equation of <m>A</m> is given by <m>\det(A-\lambda I)=\begin{vmatrix}-\lambda\amp 1\\-1\amp -\lambda\end{vmatrix}=\lambda^2+1</m>.
          Hence eigenvalues of <m>A</m> are <m>\lambda=\pm i</m>.
        </p>
        <p>
          Let us find eigenvectors with respect to the eigenvalue <m>\lambda=i</m>.
          Let <m>v=\begin{pmatrix}x_1\\x_2 \end{pmatrix}</m> be an eigenvector corresponding to <m>\lambda=i</m>.
          Then <m>Av=\lambda v=i v</m>.
          That is,
          <me>
            \begin{pmatrix}0\amp 1\\-1\amp 0 \end{pmatrix} \begin{pmatrix}x_1\\x_2 \end{pmatrix} =\begin{pmatrix}i x_1\\ ix_2 \end{pmatrix} \Longrightarrow \begin{pmatrix}x_2\\-x_1 \end{pmatrix} =\begin{pmatrix}i x_1\\ ix_2 \end{pmatrix}
          </me>.
        </p>
        <p>
          Now it is easy to see that
          <m>v=\begin{pmatrix}1\\ i \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=i</m>.
          Similarly one can show that
          <m>v=\begin{pmatrix}1\\ -i \end{pmatrix}</m> is an eigenvector of <m>A</m> corresponding to <m>\lambda=-i</m>.
        </p>
        <p>
          Note that in the above example,
          <m>A</m> is a real matrix but its eigenvalues and eigenvectors are complex.
        </p>
      </statement>
    </example>
    <definition>
      <statement>
        <p>
          Let <m>A</m> an <m>n\times n</m> real matrix and
          <m>\lambda\in \R</m> be an eigenvalue of <m>A</m>.
          Then
          <me>
            E_\lambda = \{x\in \R^n: Ax=\lambda x\}
          </me>
          the collection of all eigenvectors of <m>A</m> corresponding to <m>\lambda</m> is a subspace of <m>A</m>,
          called the <em>eigenspace of <m>A</m></em>.
          The dimension of <m>E_\lambda</m> is called the {geometric multiplicity of <m>A</m>.}
        </p>
        <p>
          Let <m>\det{x I -A}=(x-\lambda_1)^{m_1}(x-\lambda_21)^{m_2}\cdots (x-\lambda_k)^{m_k}</m>.
          Then <m>\lambda_i</m> are eigenvalue of <m>A</m> with multiplicity <m>m_i</m>,
          called the <em>algebraic multiplicity</em> of <m>\lambda_i</m>.
        </p>
      </statement>
    </definition>
    <remark>
      <p>
        Geometric multiplicity of an eigenvalue is always less than or equals to its algebraic multiplicity.
        That is, if <m>m</m> is the geometric multiplicity of <m>\lambda</m> then <m>m\leq \dim{(E_\lambda)}</m>.
      </p>
    </remark>
    <exercise>
      <statement>
        <p>
          The geometric multiplicity of an eigenvalue <m>\lambda</m> is the nullity of
          <m>A-\lambda I</m> which is the dimension of null space of <m>A-\lambda I</m>.
        </p>
      </statement>
    </exercise>
    <example xml:id="eigen-eg117">
      <statement>
        <p>
          Consider the matrix <m>A=\begin{pmatrix}-1\amp  1 \amp  0\\0 \amp  -1 \amp  1\\0 \amp  0 \amp  -1 \end{pmatrix}</m>.
          It is easy to check that <m>\det{(xI-A)}=(x+1)^3</m>.
          That is <m>A</m> has only one eigenvalue of
          <m>\lambda =-1</m> of geometric multiplicity 3.
          It is easy to see that <m>e_3=(0,0,1)</m> is an eigenvector corresponding to <m>\lambda=-1</m>.
          We have
          <me>
            A -\lambda I = \begin{pmatrix}0\amp  1 \amp  0\\0 \amp   \amp  1\\0 \amp  0 \amp  0 \end{pmatrix}
          </me>.
          It is easy to see that nullity of <m>A -\lambda I</m> is 1.
          Hence the geometric multiplicity of <m>\lambda</m> is 1 where as its algebraic multiplicity is 3.
        </p>
      </statement>
    </example>
  </introduction>
  <subsection>
    <title>Properties of Eigenvalues and Eigenvectors</title>
    <p>
      We list the following properties (without proof.)
      <ol>
        <li>
          <p>
            <m>A</m> and <m>A^T</m> have the same eigenvalues.
          </p>
        </li>
        <li>
          <p>
            If <m>\lambda</m> is an eigenvalue of <m>A</m>,
            then <m>\alpha \lambda</m> is an eigenvalue of <m>\alpha A</m>.
          </p>
        </li>
        <li>
          <p>
            If <m>\lambda</m> is an eigenvalue of <m>A</m>,
            then <m>\lambda^2</m> is an eigenvalue of <m>A^2</m>.
          </p>
        </li>
        <li>
          <p>
            If <m>\lambda</m> is an eigenvalue of a non singular matrix <m>A</m>,
            then <m>1/ \lambda</m> is an eigenvalue of <m>A^{-1}</m>.
          </p>
        </li>
        <li>
          <p>
            If <m>\lambda</m> is an eigenvalue of <m>A</m>,
            then <m>\lambda-k</m> is an eigenvalue of <m>A-kI</m> for any scalar <m>k</m>.
          </p>
        </li>
        <li>
          <p>
            If <m>\lambda</m> is an eigenvalue of <m>A</m> and
            <m>p(x)=c_0+c_cx+c_2x^2+\cdots c_kx^k</m> is a polynomial in <m>x</m>,
            then <m>p(\lambda)</m> is an eigenvalue of <m>p(A)=c_0I+c_cA+c_2A^2+\cdots c_kA^k</m>.
          </p>
        </li>
        <li>
          <p>
            Two matrices <m>A</m> and <m>B</m> are called <em>similar</em>
            if there exists a matrix <m>P</m> such that <m>B=P^{-1}AP</m>.
            Similar matrices have same eigenvalues.
          </p>
        </li>
        <li>
          <p>
            If <m>\lambda_1</m> and <m>\lambda_2</m> are distinct eigenvalues of <m>A</m> then eigenvectors <m>v_1</m> and <m>v_2</m> corresponding to <m>\lambda_1</m> and
            <m>\lambda_2</m> are linearly independent.
            Can you generalize this?
          </p>
        </li>
        <li>
          <p>
            The rank if a matrix a square matrix <m>A</m> is is the number of nonzero eigenvalues of <m>A</m>.
          </p>
        </li>
        <li>
          <p>
            If <m>T</m> is a linear transformation from <m>\R^n\to \R^m</m>.
            Fix a basis <m>\beta</m> of <m>\R^n</m>.
            Let <m>A=[T]_\beta</m> be the matrix of <m>T</m> with respect to <m>\beta</m>.
            Then <m>A</m> and <m>T</m> have the same eigenvalues.
            Furthermore,
            eigenvalues of <m>T</m> are independent of the basis.
          </p>
        </li>
      </ol>
    </p>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m> and <m>B=A^3-3A+I</m>.
          Let us find eigenvalues of <m>B</m>.
        </p>
        <p>
          It is easy to the characteristic polynomial of <m>A</m> is given by
          <m>\lambda^3-\lambda^2-4\lambda=4</m> and <m>\lambda=-2, 1, 2</m>.
          Then eigenvalues of <m>B</m> are given by
          <me>
            \{(-2)^3-3\times (-2)+1, 1^3-3\times 1+1, 2^3-3\times (2)+1\} =\{-1, -1, 3\}
          </me>
        </p>
      </statement>
    </example>
    <theorem>
      <statement>
        <ol>
          <li>
            <p>
              Eigenvalues of Hermitian (symmetric) matrix are real.
            </p>
          </li>
          <li>
            <p>
              Eigenvalues of skew-Hermitian (skew-symmetric) matrix are zero or purely imaginary.
            </p>
          </li>
        </ol>
      </statement>
    </theorem>
    <proof>
      <p>
        (a) Let <m>\lambda</m> be an eigenvalues of <m>A</m> and <m>v</m>,
        the corresponding eigenvector of <m>A</m>.
        Then by definition <m>Av=\lambda v</m>.
        Multiplying both sides by <m>\overline{v}^T</m>
        (the conjugate transpose of the vector <m>v</m>),
        we get
        <me>
          \overline{v}^TAv=\lambda\overline{v}^Tv \Longrightarrow \lambda=\dfrac{\overline{v}^TAv}{\overline{v}^Tv}
        </me>.
      </p>
      <p>
        It is easy to see that <m>\overline{v}^TAv</m> and
        <m>\overline{v}^Tv</m> are scalars and that <m>\overline{v}^Tv</m> is a real number.
        Hence the behavior of <m>\lambda</m> is determined by <m>\overline{v}^TAv</m>.
      </p>
      <p>
        If <m>A</m> is a herminitan matrix then <m>\overline{A}=A^T</m>,
        also <m>\overline{v}^TAv</m> is scalar,
        implies <m>(\overline{v}^TAv)^T=\overline{v}^TAv</m>.
        Hence
        <me>
          \overline{(\overline{v}^TAv)}=v^T\overline{A}\overline{v}=v^T{A^T}\overline{v}=(\overline{v}^TA{v})^T=v^T{A}\overline{v}
        </me>.
        This implies that <m>\overline{v}^TAv</m> is a real number and hence <m>\lambda</m> is a real number.
      </p>
      <p>
        Now if <m>A</m> is a skew-hermitian matrix,
        then it is easy to show that <m>\overline{(\overline{v}^TAv)}=-(\overline{v}^TAv)</m>.
        Hence <m>\overline{v}^TAv</m> is either purely imaginary or zero.
        Which show <m>\lambda</m> is either purely imaginary of zero.
      </p>
    </proof>
    <theorem>
      <title>Cayley-Hamilton Theorem</title>
      <statement>
        <p>
          Every square matrix satisfies its characteristic equation.
          That is, if <m>p(x)=0</m> is characteristic equation of <m>A</m>,
          then <m>p(A)=0</m>.
        </p>
      </statement>
    </theorem>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m>.
          From <xref ref="eigen_eg1">Example</xref>,
          the characteristic polynomial of <m>A</m> is given by <m>p(x)=\det{x I -A}=x^3-x^2-4x+4</m>.
          We have <m>A^2=\left(\begin{array}{rrr} 1 \amp  -2 \amp  2 \\ 3 \amp  6 \amp  -2 \\ 3 \amp  2 \amp  2 \end{array} \right)</m> and <m>A^3=\left(\begin{array}{rrr} 1 \amp  6 \amp  -6 \\ 7 \amp  6 \amp  2 \\ 7 \amp  14 \amp  -6 \end{array} \right)</m>.
          Hence
          <md>
            <mrow>p(A)\amp =\amp A^3-A^2-4A+4I</mrow>
            <mrow>\amp =\amp  \left(\begin{array}{rrr} 1 \amp  6 \amp  -6 \\ 7 \amp  6 \amp  2 \\ 7 \amp  14 \amp  -6 \end{array} \right)-\left(\begin{array}{rrr} 1 \amp  -2 \amp  2 \\ 3 \amp  6 \amp  -2 \\ 3 \amp  2 \amp  2 \end{array} \right)-4\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix} +4\begin{pmatrix}1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 1 \end{pmatrix}</mrow>
            <mrow>\amp =\amp  \begin{pmatrix}0\amp 0\amp 0\\0\amp 0\amp 0\\ 0\amp 0\amp 0 \end{pmatrix} </mrow>
          </md>.
        </p>
        <p>
          Hence <m>A</m> satisfies its characteristic equation.
        </p>
        <p>
          It is easy to check that <m>\det{(A)}=-4</m>,
          hence <m>A</m> is non singular.
          Since <m>A^3-A^2-4A+4I=0</m>,
          multiplying both sides by its inverse,
          we get <m>A^2-A+4I+4A^{-1}=0</m>.
          Hence
          <me>
            A^{-1}=-\frac{-1}{4}A^2+\frac{1}{4}A+I=\left(\begin{array}{rrr} 1 \amp  1 \amp  -1 \\ -\frac{1}{2} \amp  -\frac{1}{4} \amp  \frac{3}{4} \\ -\frac{1}{2} \amp  \frac{1}{4} \amp  \frac{1}{4} \end{array} \right)
          </me>.
        </p>
        <p>
          We can also find higher powers of a matrix,
          using the Cayley-Hamilton theorem.
          For example multiplying bu <m>A</m> to the equation,
          <m>A^3-A^2-4A+4I=0</m>, we get <m>A^4-A^3-4A^2+4A=0</m>,
          from this we have
          <me>
            A^4 = A^3+4A^2-4A=\left(\begin{array}{rrr} 1 \amp  -10 \amp  10 \\ 15 \amp  26 \amp  -10 \\ 15 \amp  10 \amp  6 \end{array} \right)
          </me>.
        </p>
        <p>
          Can you find <m>A^5</m>?
        </p>
      </statement>
    </example>
    <exercise>
      <statement>
        <p>
          (i) Consider the matrix <m>A = \begin{pmatrix}3 \amp -2 \\-4\amp 3 \end{pmatrix}</m>.
          Show that <m>A</m> satisfies its characteristics equation.
          Hence find <m>A^{-1}, A^3, A^4</m>.
        </p>
        <p>
          (i) Consider the matrix <m>A = \begin{pmatrix}1 \amp 0 \amp 0 \\ -4 \amp -3 \amp 4 \\ -2 \amp -2 \amp 3 \end{pmatrix}</m>.
          Show that <m>A</m> satisfies its characteristics equation.
          Hence find <m>A^{-1}, A^4, A^5</m>.
        </p>
      </statement>
    </exercise>
    <definition>
      <statement>
        <p>
          Let <m>A</m> an <m>n\times n</m> and <m>\lambda_i</m> for
          <m>1\leq i\leq n</m> be eigenvalues of <m>A</m> then the spectral radius of <m>A</m> is define as <m>\rho(A):=\displaystyle\max_{1\leq i\leq n}\{ |\lambda_i| \}</m>.
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let <m>A= \begin{pmatrix}0 \amp  -1\\1 \amp  0 \end{pmatrix}</m>.
          Then the characteristics polynomial of <m>A</m> is <m>\det{xI-A}=x^2+1</m>.
          Hence <m>x=\{i,-i\}</m> are roots of the characteristic polynomial.
          Hence <m>i</m> and <m>-i</m> are eigenvalues of <m>A</m>.
          Hence
          <me>
            \rho(A) = \max\{|i|,|-i|\}=1
          </me>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Consider the matrix <m>A=\left(\begin{array}{rrr} 4 \amp  -3 \amp  0 \\ 3 \amp  4 \amp  0 \\ 5 \amp  10 \amp  10 \end{array} \right)</m>.
          Then the characteristics polynomial of <m>A</m> is <m>\det{xI-A}=x^{3} - 18 x^{2} + 105 x - 250</m>.
          Which has roots, <m>10, 3-4i,3+4i</m>.
          Hence
          <me>
            \rho(A) = \max\{10,|3-4i|,|3+4i\}=10
          </me>.
        </p>
      </statement>
    </example>
    <exercise>
      <statement>
        <p>
          Find the spectral radius of
          <m>\left(\begin{array}{rr} 2 \amp -3 \\ 3 \amp 2 \end{array} \right), \left(\begin{array}{rrr} 2 \amp 3 \amp 1 \\ -3 \amp 2 \amp 2 \\ 0 \amp 0 \amp 2 \end{array} \right)</m> and <m>\left(\begin{array}{rrr} 2 \amp 3 \amp 1 \\ 3 \amp 2 \amp 2 \\ 0 \amp 0 \amp 1 \end{array} \right)</m>.
        </p>
      </statement>
    </exercise>
  </subsection>
  <subsection>
    <title>Positive definite matrix</title>
    <definition>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> symmetric matrix.
          Then <m>A</m> is said to be <em>positive definite</em>
          if <m>x^TAx>0</m> for all <m>x\in \R^n</m> and <m>x^TAx=0</m> if and only if <m>x=0</m>.
          <m>A</m> is called <em>negative definite</em>
          if <m>-A</m> is positive definite.
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}1\amp 2\\2\amp 5 \end{pmatrix}</m>.
          Let <m>x=\begin{pmatrix}x_1\\x_2 \end{pmatrix}</m>.
          Then
          <me>
            x^TAx=\begin{pmatrix}x_1\amp x_2 \end{pmatrix} \begin{pmatrix}1\amp 2\\2\amp 5 \end{pmatrix} \begin{pmatrix}x_1\\x_2 \end{pmatrix} =\begin{pmatrix}x_1\amp x_2 \end{pmatrix} \begin{pmatrix}x_1+2x_2\\2x_1+5x_2 \end{pmatrix} =x_1^2+4x_1x_2+5x_2^2=(x_1+2x_2)^2+x_2^2
          </me>.
        </p>
        <p>
          Clearly <m>x^TAx>0</m> for all non zero vector <m>x</m> and <m>x^TAx=0</m> if and only if <m>x=0</m>.
          Hence <m>A</m> is positive definite.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}0\amp 1\\1\amp 0 \end{pmatrix}</m>.
          Let <m>x=\begin{pmatrix}x_1\\x_2 \end{pmatrix}</m>.
          Then
          <me>
            x^TAx=\begin{pmatrix}x_1\amp x_2 \end{pmatrix} \begin{pmatrix}0\amp 1\\1\amp 0 \end{pmatrix} \begin{pmatrix}x_1\\x_2 \end{pmatrix} = \begin{pmatrix}x_1\amp x_2 \end{pmatrix} \begin{pmatrix}x_2\\x_1 \end{pmatrix} =2x_1x_2
          </me>
        </p>
        <p>
          Thus if <m>x=\begin{pmatrix}1\\-1 \end{pmatrix}</m> then <m>x^TAx=-2\lt 0</m>.
          Hence it is not a positive definite.
          Its easy to see that <m>A</m> is alos not negative definite.
        </p>
      </statement>
    </example>
    <p>
      <em>We have the following facts about positive definite matrices.</em>
      <ol>
        <li>
          <p>
            All eigenvalues of a positive definite matrix are real and positive.
          </p>
        </li>
        <li>
          <p>
            Eigenvalues of a negative definite matrix are real and negative.
          </p>
        </li>
        <li>
          <p>
            If <m>A</m> is a real symmetric matrix then <m>A</m> is positive definite if and only if all leading minor of <m>A</m> are positive.
          </p>
        </li>
      </ol>
    </p>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}2 \amp  1 \amp 1\\1\amp 2\amp 1\\1\amp 1\amp 2 \end{pmatrix}</m>.
          For any <m>x=\begin{pmatrix}x_1\\x_2\\x_2 \end{pmatrix} \in \R^2</m>, we have
          <me>
            x^TAx=x_1^2+x_2^2+x_3^2+(x_1+x_2+x_3)^2
          </me>
        </p>
        <p>
          Henc e <m>A</m> is positive definite.
        </p>
      </statement>
    </example>
    <exercise>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}2 \amp -1 \amp 0\\-1\amp 2\amp -1\\0\amp -1\amp 2 \end{pmatrix}</m>.
          Show that <m>A</m> is positive definite using the above results.
        </p>
      </statement>
    </exercise>
  </subsection>
  <subsection>
    <title>Diagonalization</title>
    <definition>
      <statement>
        <p>
          A matrix <m>A</m> is said to be <em>diagonalizable</em>
          if there exists a non singular matrix <m>P</m> such that
          <m>P^{-1}AP</m> is a diagonal matrix.
          That is, <m>A</m> is similar to a diagonal matrix.
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}1\amp 2\amp -2\\1\amp 1\amp 1\\1\amp 3\amp -1 \end{pmatrix}</m> as in <xref ref="eigen_eg1">Example</xref>.
          Define <m>P=\begin{pmatrix}1\amp 0\amp 8/7\\-1\amp 1\amp -5/7\\-1\amp 1\amp 1 \end{pmatrix}</m>,
          whose columns are eigenvectors of <m>A</m>.
        </p>
        <p>
          Check that <m>\det(P)=\dfrac{12}{7}</m> and <m>\text{ adj } (P)=\begin{pmatrix}12/7 \amp 8/7 \amp -8/7\\12/7 \amp 15/7\amp -3/7\\0 \amp -1 \amp 1 \end{pmatrix}</m>.
          Hence <m>P^{-1}=\begin{pmatrix}1\amp 2/3 \amp -2/3\\1\amp 5/4\amp -1/4\\0\amp -7/4\amp 7/4 \end{pmatrix}</m>
        </p>
        <p>
          Then it is easy to check that <m>P^{-1}AP=\begin{pmatrix}1\amp 0\amp 0\\0\amp 2\amp 0\\0\amp 0\amp -2 \end{pmatrix}</m>.
        </p>
        <p>
          In this case we can find any power of <m>A</m> quite easily.
          For example <m>A^n=P^{-1}\begin{pmatrix}1\amp 0\amp 0\\0\amp 2^n\amp 0\\0\amp 0\amp (-2)^n \end{pmatrix} P</m>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}1 \amp 1\\0 \amp 1 \end{pmatrix}</m>.
          Then 1 is a repeated eigenvalue of <m>A</m> with eigenvector <m>\begin{pmatrix}1\\ 0 \end{pmatrix}</m>.
          It is easuy to see that <m>A</m> is non diagonalizable.
        </p>
      </statement>
    </example>
    <exercise>
      <statement>
        <p>
          If <m>A=[a_{ij}]</m> has <m>n</m> distinct eigenvalues then <m>A</m> is diagonalizable.
          In this case,
          one can define <m>P</m> where columns of <m>P</m> are <m>n</m> eigenvectors of <m>A</m>.
        </p>
      </statement>
    </exercise>
    <theorem xml:id="thm_diag">
      <statement>
        <p>
          A square matrix <m>A</m> of order <m>n</m> is diagonalizable if and only if <m>A</m> has <m>n</m> linearly independent eigenvectors.
        </p>
      </statement>
    </theorem>
    <proof>
      <p>
        Let <m>A</m> be diagonalizable,
        and that there exists a non singular matrix <m>P</m> such that
        <men xml:id="thm_diag_eq1">
          P^{-1}AP=D=\diag(\lambda_1,\ldots,\lambda_n)
        </men>
      </p>
      <p>
        Let us write <m>P=[P_1,\ldots,P_n]</m> where <m>P_j</m> is the <m>j</m>-th column of <m>P</m>.
        Then Eq.
        <xref ref="thm_diag_eq1"></xref> implies
        <me>
          AP=P\diag(\lambda_1,\ldots,\lambda_n) \Longrightarrow [AP_1,AP_2,\ldots,AP_n]=[\lambda_1P_1,\lambda_2P_2,\ldots,\lambda_nP_n]
        </me>
      </p>
      <p>
        Equivalently <m>AP_i=\lambda_iP_i</m> for <m>1\leq i\leq n</m>.
        That is same as saying columns of <m>P</m> are eigenvectors of <m>A</m> with respect to eigenvalue <m>\lambda_i</m>.
        This implies <m>A</m> has <m>n</m> linearly eigenvectors,
        namely columns of <m>P</m>.
      </p>
      <p>
        Conversely, let <m>A</m> have <m>n</m> linearly independent eigenvectors
        <m>v_1, \ldots,
        v_n</m> and that <m>Av_j=\lambda_jv_j</m>.
        Define <m>P:=[v_1,v_2\ldots,v_n]</m> and <m>D:=\diag(\lambda_1,\ldots,\lambda_n)</m>.
        Then
        <me>
          AP=[Av_1,Av_2,\ldots,Av_n]=[\lambda_1v_1,\lambda_2v_2,\ldots,\lambda_nv_n]=PD
        </me>.
      </p>
      <p>
        Hence <m>P^{-1}AP=D</m>.
        Note that <m>P</m> has rank <m>n</m>,
        which implies <m>P</m> is invertible.
      </p>
    </proof>
    <corollary>
      <statement>
        <p>
          If <m>A</m> is a square matrix of order <m>n</m> has <m>n</m> distinct eigenvalues then <m>A</m> is diagonalizable.
        </p>
      </statement>
    </corollary>
    <example>
      <statement>
        <p>
          Let <m>A</m> be a <m>3\times 3</m> real matrix with eigenvalues
          <m>\lambda_1=2,\lambda_2=1,\lambda_3=-1</m> and corresponding eigenvectors <m>v_1=\left(1,\,\frac{1}{3},\,-\frac{1}{3}\right),
          v_2=\left(1,\,0,\,-\frac{1}{3}\right),
          v_3=\left(1,\,\frac{1}{2},\,-\frac{1}{2}\right)</m> respectively.
          Then we have <m>P=[v_1~v_2~v_3]=\left(\begin{array}{rrr} 1 \amp 1 \amp 1 \\ \frac{1}{3} \amp 0 \amp \frac{1}{2} \\ -\frac{1}{3} \amp -\frac{1}{3} \amp -\frac{1}{2} \end{array} \right)</m>.
          <m>P^{-1}AP=\left(\begin{array}{rrr} 2 \amp 0 \amp 0 \\ 0 \amp 1 \amp 0 \\ 0 \amp 0 \amp -1 \end{array} \right)</m>.
          Hence <m>A = P\left(\begin{array}{rrr} 2 \amp 0 \amp 0 \\ 0 \amp 1 \amp 0 \\ 0 \amp 0 \amp -1 \end{array} \right)P^{-1}</m>.
          It is easy to see that <m>P^{-1}=\left(\begin{array}{rrr} 3 \amp 3 \amp 9 \\ 0 \amp -3 \amp -3 \\ -2 \amp 0 \amp -6 \end{array} \right)</m>.
        </p>
        <p>
          Hence
          <me>
            A =\left(\begin{array}{rrr} 1 \amp  1 \amp  1 \\ \frac{1}{3} \amp  0 \amp  \frac{1}{2} \\ -\frac{1}{3} \amp  -\frac{1}{3} \amp  -\frac{1}{2} \end{array} \right) \begin{pmatrix}2\amp  0 \amp  0\\0 \amp  1 \amp  0\\0 \amp  0 \amp  1 \end{pmatrix} \left(\begin{array}{rrr} 3 \amp  3 \amp  9 \\ 0 \amp  -3 \amp  -3 \\ -2 \amp  0 \amp  -6 \end{array} \right)=\left(\begin{array}{rrr} 8 \amp  3 \amp  21 \\ 3 \amp  2 \amp  9 \\ -3 \amp  -1 \amp  -8 \end{array} \right)
          </me>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Let <m>A=\begin{pmatrix}-1\amp -1\amp -2\\8\amp -11\amp -8\\-10\amp 11\amp 7 \end{pmatrix}</m> and <m>B=\begin{pmatrix}1\amp -4\amp -4\\8\amp -11\amp -8\\-8\amp 8\amp 5 \end{pmatrix}</m>.
        </p>
        <p>
          It is easy to check that <m>A</m> and <m>B</m> have same characteristic polynomial <m>\lambda^3+5\lambda^2+3\lambda-9=(\lambda-1)(\lambda+3)^2</m>.
          Also We can show that <m>A</m> has only one linearly independent eigenvectors corresponding to eigenvalue <m>-3</m>.
          This implies <m>A</m> has only two eigenvectors and hence <m>A</m> is not diagonalizable.
        </p>
        <p>
          Similarly, We can show that <m>B</m> has two linearly independent eigenvectors corresponding to eigenvalue <m>-3</m>.
          This implies <m>B</m> has three eigenvectors and hence <m>B</m> is diagonalizable.
        </p>
      </statement>
    </example>
    <p>
      We mention another criteria of diagonalizability without proof.
    </p>
    <theorem xml:id="diagonalizable-thm2">
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> real matrix with distinct eigenvalues
          <m>\lambda_1,\ldots, \lambda_k</m> and algebraic multiplicity <m>m_1,\ldots,
          m_k</m> respectively.
          Then <m>A</m> is diagonalizable if and only if algebraic multiplicity is same as geometric multiplicity for each eigenvalue.
          That is <m>m_i=\dim{(E_{\lambda_i})}</m> for <m>i=1,2,\ldots,k</m>.
        </p>
      </statement>
    </theorem>
    <example>
      <statement>
        <p>
          In view of <xref ref="diagonalizable-thm2">theorem</xref>,
          the matrix in the <xref ref="eigen-eg117">example</xref> is not diagonalizable.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          A travel company has a fleet of 6000 cars for renting.
          A car rented at one location can be returned to any of the three locations A, B and C. The various fractions of cars returned to the three locations are given in the table below.
        </p>
        <tabular>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
          </row>
          <row>
            <cell>Depots</cell>
            <cell>A</cell>
            <cell>B</cell>
            <cell>C</cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
          </row>
          <row>
            <cell>A</cell>
            <cell>0.3</cell>
            <cell>0.4</cell>
            <cell>0.5</cell>
          </row>
          <row>
            <cell>B</cell>
            <cell>0.3</cell>
            <cell>0.4</cell>
            <cell>0.3</cell>
          </row>
          <row>
            <cell>C</cell>
            <cell>0.4</cell>
            <cell>0.2</cell>
            <cell>0.2</cell>
          </row>
          <row bottom="minor">
            <cell></cell>
            <cell></cell>
            <cell></cell>
            <cell></cell>
          </row>
        </tabular>
        <p>
          Suppose on Monday there are 2000 cars at each location A. What will be the approximate distribution of cars on Thursday.
          How should the company distribute these cars at various locations in order to serve the customers smoothly.
        </p>
        <p>
          Let the initial distributions of cars at three location be denoted by the vector <m>X = \begin{bmatrix}20000\\2000\\2000 \end{bmatrix}</m>.
          The proportion of cars that are returned to various locations can be represented by the matrix <m>P = \begin{bmatrix}0.3 \amp 0.4 \amp 0.5\\ 0.3 \amp 0.4 \amp 0.3 \\ 0.4 \amp 0.2 \amp 0.2 \end{bmatrix}</m>,
          which is stochastic matrix. (Any square matrix with non negative entries with column sum 1 is called columns stochastic or Markov matrix. )
        </p>
        <p>
          Number of cars at location <m>A</m> after one day is <m>0.3\times 2000 +0.4\times 2000+0.5\times 2000</m>
        </p>
        <p>
          Number of cars at location <m>A</m> after one day is <m>0.3\times 2000 +0.4\times 2000+0.3\times 2000</m>
        </p>
        <p>
          Number of cars at location <m>A</m> after one day is <m>0.4\times 2000 +0.2\times 2000+0.2\times 2000</m>
        </p>
        <p>
          It is easy to see that distribution of cars after day one is <m>PX</m>.
        </p>
        <p>
          Similarly after day two it is <m>P(PX)=P^2X</m>.
          Thus on Thursday the car distribution at various location is given by <m>P^3X=\begin{bmatrix}2336\\ 2000.\\ 1664.0 \end{bmatrix}</m>.
        </p>
        <p>
          After say 100 days the car distribution at various location is given by <m>P^{50}X\approx=\begin{bmatrix}2333.33\\2000.\\1666.67 \end{bmatrix}</m>.
          In fact after <m>n</m> large <m>P^nX</m> is constant which is approximately <m>\begin{bmatrix}2333.33\\2000.\\1666.67 \end{bmatrix}</m>.
          Thus in long run car distribution is <m>\begin{bmatrix}2333\\2000\\1667 \end{bmatrix}</m>.
        </p>
        <p>
          The higher power of <m>P</m> can be obtained by diagonalizing <m>P</m>.
          In this can eigenvalues of <m>P</m> are
          <m>1, 1/10, -1/5</m> and the corresponding eigenvectors are <m>\begin{bmatrix}1 \\6/7\\5/7 \end{bmatrix} , \begin{bmatrix}1 \\-3\\2 \end{bmatrix} , \begin{bmatrix}1 \\0\\-1 \end{bmatrix}</m>.
          Let us define <m>D= \left(\begin{array}{rrr} 1 \amp  0 \amp  0 \\ 0 \amp  \frac{1}{10} \amp  0 \\ 0 \amp  0 \amp  -\frac{1}{5} \end{array} \right)</m> and <m>Q=\left(\begin{array}{rrr} 1 \amp  1 \amp  1 \\ \frac{6}{7} \amp  -3 \amp  0 \\ \frac{5}{7} \amp  2 \amp  -1 \end{array} \right)</m>.
          Then <m>QDQ^{-1}=A</m>.
          Hence <m>A^n = QD^nQ^{-1}</m>.
          Here <m>Q^{-1} =\left(\begin{array}{rrr} \frac{7}{18} \amp  \frac{7}{18} \amp  \frac{7}{18} \\ \frac{1}{9} \amp  -\frac{2}{9} \amp  \frac{1}{9} \\ \frac{1}{2} \amp  -\frac{1}{6} \amp  -\frac{1}{2} \end{array} \right)</m>.
          Hence
          <md>
            <mrow>A^n \amp =\amp  \left(\begin{array}{rrr} 1 \amp  1 \amp  1 \\ \frac{6}{7} \amp  -3 \amp  0 \\ \frac{5}{7} \amp  2 \amp  -1 \end{array} \right) \left(\begin{array}{rrr} 1 \amp  0 \amp  0 \\ 0 \amp  \left(\frac{1}{10}\right)^n \amp  0 \\ 0 \amp  0 \amp  \left(-\frac{1}{5}\right)^n \end{array} \right)\left(\begin{array}{rrr} \frac{7}{18} \amp  \frac{7}{18} \amp  \frac{7}{18} \\ \frac{1}{9} \amp  -\frac{2}{9} \amp  \frac{1}{9} \\ \frac{1}{2} \amp  -\frac{1}{6} \amp  -\frac{1}{2} \end{array} \right)</mrow>
            <mrow>\amp  =\amp \left(\begin{array}{rrr} 1 \amp  1 \amp  1 \\ \frac{6}{7} \amp  -3 \amp  0 \\ \frac{5}{7} \amp  2 \amp  -1 \end{array} \right) \left(\begin{array}{rrr} 1 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \\ 0 \amp  0 \amp  0 \end{array} \right)\left(\begin{array}{rrr} \frac{7}{18} \amp  \frac{7}{18} \amp  \frac{7}{18} \\ \frac{1}{9} \amp  -\frac{2}{9} \amp  \frac{1}{9} \\ \frac{1}{2} \amp  -\frac{1}{6} \amp  -\frac{1}{2} \end{array} \right) \text{ for large \(n\) }</mrow>
            <mrow>\amp =\amp  \left(\begin{array}{rrr} \frac{7}{18} \amp  \frac{7}{18} \amp  \frac{7}{18} \\ \frac{1}{3} \amp  \frac{1}{3} \amp  \frac{1}{3} \\ \frac{5}{18} \amp  \frac{5}{18} \amp  \frac{5}{18} \end{array} \right)</mrow>
          </md>
        </p>
        <p>
          Suppose we define <m>\pi = \left(\begin{array}{r} \frac{7}{18}\\ \frac{1}{3} \\ \frac{5}{18} \end{array} \right)</m>.
          Then it is easy to check that <m>A\pi=\pi</m>.
          That is <m>\pi</m> is an eigenvector of <m>A</m> corresponding to the eigenvalue 1.
          This is called the <em>steady state vector.</em>
        </p>
      </statement>
    </example>
    <exercise>
      <statement>
        <p>
          Suppose <m>D</m> is a diagonal matrix given by
          <m>D= \left(\begin{array}{rrr} 1 \amp 0 \amp 0 \\ 0 \amp 0.3 \amp 0 \\ 0 \amp 0 \amp 0.2 \end{array} \right)</m> and <m>v\in \R^3</m>.
          What happens to the vector <m>v</m> geometrically when we do <m>D^nv</m> for large <m>n</m>? (<em>It sucks vector <m>v</m> into <m>u_1</m> direction.</em>)
        </p>
        <p>
          Next let us consider a matrix which is diagonalizable with eigenvectors
          <m>u_1, u_2, u_3</m> and corresponding eigenvalues
          <m>\lambda_1=1, \lambda_2=0.3</m> and <m>\lambda_3=0.2</m> respectively.
          Then what happens to the vector <m>v</m> geometrically when we do <m>A^nv</m> for large <m>n</m>? (<em>It makes <m>y</m> and <m>z</m> coordinates very small and it sucks vector <m>v</m> into <m>x</m>-axis.</em>)
        </p>
      </statement>
    </exercise>
    <exercise>
      <statement>
        <p>
          132 fishes are placed in a box with nine rooms.
          See <xref ref="mouse1">Figure</xref>.
        </p>
        <figure>
          <image>
<latex-image>
<![CDATA[\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1cm,y=1cm]
\clip(-1.0,-1.0) rectangle (10.0,10.);
\draw [line width=1pt] (0,0)-- (9,0);
\draw [line width=1pt] (9,0)-- (9,9);
\draw [line width=1pt] (0,9)-- (9,9);
\draw [line width=1pt] (0,9)-- (0,0);
\draw [line width=1pt] (3,9)-- (3,8);
\draw [line width=1pt] (0,6)-- (1,6);
\draw [line width=1pt] (3,7)-- (3,5);
\draw [line width=1pt] (2,6)-- (4,6);
\draw [line width=1pt] (0,3)-- (1,3);
\draw [line width=1pt] (3,4)-- (3,2);
\draw [line width=1pt] (2,3)-- (4,3);
\draw [line width=1pt] (3,1)-- (3,0);
\draw [line width=1pt] (6,9)-- (6,8);
\draw [line width=1pt] (9,6)-- (8,6);
\draw [line width=1pt] (9,3)-- (8,3);
\draw [line width=1pt] (6,7)-- (6,5);
\draw [line width=1pt] (5,6)-- (7,6);
\draw [line width=1pt] (5,3)-- (7,3);
\draw [line width=1pt] (6,4)-- (6,2);
\draw [line width=1pt] (6,0)-- (6,1);
\draw (1.0,8.0) node[anchor=north west] {1};
\draw (4.0,8) node[anchor=north west] {2};
\draw (7,8) node[anchor=north west] {3};
\draw (1,5.0) node[anchor=north west] {4};
\draw (4,5.0) node[anchor=north west] {5};
\draw (7,5.0) node[anchor=north west] {6};
\draw (1,2) node[anchor=north west] {7};
\draw (4,2) node[anchor=north west] {8};
\draw (7,2) node[anchor=north west] {9};\end{tikzpicture}]]>
</latex-image>
          </image>
        </figure>
        <p>
          Assume that, at regular intervals of time,
          it is equally likely that fishes will decide to go through any door in the room or stay in the room.
        </p>
        <p>
          Find how many fishes can be found in each room in long run.
        </p>
        <p>
          Solve this problem using a
          <m>3 \times 3</m> matrix stochastic matrix.
        </p>
      </statement>
    </exercise>
  </subsection>
</section>