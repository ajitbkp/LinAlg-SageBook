<section>
  <title>Orthogonal Complement</title>
  <introduction>
    <definition>
      <statement>
        <p>
          Let <m>U\subset \R^n</m>.
          Then <m>U^\perp:=\{x\in \R^n:
          x\cdot u=0, \forall u\in U\}</m> is called the orthogonal complement of <m>U</m>.
        </p>
      </statement>
    </definition>
    <exercise>
      <statement>
        <p>
          (i) <m>\{0\}^\perp = \R^n</m>
        </p>
        <p>
          (ii) <m>{\R^n}^{\perp}=\{0\}</m>.
        </p>
        <p>
          (iii) Let <m>U\subset \R^n</m>.
          Then <m>U^\perp</m> is a subspace of <m>\R^n</m>.
          Note that <m>U</m> need not be a subspace of <m>\R^n</m>.
        </p>
      </statement>
    </exercise>
    <example>
      <statement>
        <p>
          Let <m>U=span(\{(2,-1,1),(1,2,-1)\})</m>.
          Then
          <md>
            <mrow>U^\perp \amp =\amp  \{(x_1,x_2,x_3): 2x_1-x_2+x_3=0,x_1+2x_2-x_3=0\}</mrow>
            <mrow>\amp =\amp null\left(\begin{array}{rrr} 2 \amp  -1 \amp  1 \\ 1 \amp  2 \amp  -1 \end{array} \right)= \{t(1,-3,-5):t\in \R\}</mrow>
          </md>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Find the orthogonal complement of <m>span(\{\left(2,\,0,\,1,\,-1\right), \left(1,\,2,\,0,\,-1\right)\})</m>.
          <md>
            <mrow>U^\perp \amp =\amp  \{(x_1,x_2,x_3,x_3): 2x_1+x_3-x_4=0,x_1+2x_2-x_4=0\}</mrow>
            <mrow>\amp =\amp null\left(\begin{array}{rrrr} 2 \amp  0 \amp  1 \amp  -1 \\ 1 \amp  2 \amp  0 \amp  -1 \end{array} \right)</mrow>
            <mrow>\amp =\amp  span(\{(1,  0, -1,  1),(0,  1,  2,  2)\})</mrow>
          </md>
        </p>
      </statement>
    </example>
    <definition xml:id="subspace-projection1">
      <statement>
        <p>
          Let <m>U</m> be a subspace of <m>\R^n</m> with and orthogonal basis <m>\{u_1,\ldots,u_k\}</m>.
          If <m>x\in \R^n</m>, then
          <men xml:id="proj_onto_subspace">
            \proj_U(x)=\frac{x\cdot u_1}{\norm{u_1}^2}u_1+\frac{x\cdot u_2}{\norm{u_2}^2}u_2+\cdots + \frac{x\cdot u_k}{\norm{u_k}^2}u_k
          </men>.
        </p>
      </statement>
    </definition>
    <exercise>
      <statement>
        <p>
          Let <m>U</m> be a subspace of <m>\R^n</m> and <m>p=\proj_U(x)</m>.
          Then
        </p>
        <p>
          (i) <m>p\in U</m> and <m>(x-p) \in U^\perp</m>.
        </p>
        <p>
          (ii) <m>p</m> is a vector in <m>U</m>,
          which is closet to <m>x</m>.
          That is for all <m>y\in U</m>,
          <m>\norm{x-p}\leq \norm{x-y}</m>.
          Note that <m>(x-p)\perp (y-p)</m>.
          Hence by the Pythagoras theorem,
          <m>\norm{x-p}^2+\norm{p-y}^2=\norm{x-y}^2</m>.
          <me>
            \norm{x-y}^2=\norm{x-p+p-y}^2=\norm{x-p}^2+\norm{y-p}^2\geq \norm{x-p}^2
          </me>.
        </p>
      </statement>
    </exercise>
    <example>
      <statement>
        <p>
          Consider the plane <m>\pi=\{(x,y,z\in \R^3:3x+y-5z=0\}</m>.
          It is easy to see that <m>v_1=(2,-1,1),
          v_2=(1,2,1)</m> lie on the plane <m>\pi</m>.
          Using the Gram-Schmidt process we can find an orthogonal basis <m>u_1 = (2,-1,1),
          u_2=(2/3, 13/6, 5/6)</m> on <m>\pi</m>.
          Let us find the orthogonal projection of <m>v=(1,2,3)</m> onto <m>\pi</m>.
          The required vector
          <me>
            \proj_\pi(v)=\frac{v\cdot u_1}{\norm{u_1}^2}u_1+\frac{v\cdot u_2}{\norm{u_2}^2}u_2=(13/7, 16/7, 11/7)
          </me>.
        </p>
      </statement>
    </example>
    <p>
      How to find the orthogonal projection of a vector on to the subspace spanned by a set of vectors in <m>\R^n</m>?
      Let <m>\beta = \{w_1,\ldots, w_k\}</m> be a basis of <m>W</m>.
      We want to find the vector <m>p</m> which the orthogonal projection of <m>v</m> onto <m>W</m>.
    </p>
    <p>
      Note that <m>p\in W</m>, therefore,
      there exist scalars <m>x_1,\ldots, x_k</m> such that
      <me>
        p=x_1 v_1+\cdots + x_k v_k=Ax
      </me>
      where <m>A=[v_1~\cdots ~ v_k]</m> and <m>x=[x_1~\cdots~x_k]^T</m>.
    </p>
    <p>
      It is clear that <m>v-p=v-Ax\in W^T</m>.
      Hence <m>(v_i)^T(v-Ax)=0</m> for <m>1\leq i\leq k</m>.
      This is same as
      <me>
        A^T(v-Ax)=0 \implies x=(A^TA)^{-1}(A^Tv)
      </me>.
    </p>
    <p>
      Hence
      <men xml:id="orth-Proj-W">
        p=A(A^TA)^{-1}(A^Tv)
      </men>
    </p>
    <p>
      The matrix <m>Q=A(A^TA)^{-1}A^T</m> is called the
      <q>projection matrix for the subspace <m>W</m></q>.
    </p>
  </introduction>
  <subsection>
    <title>Orthogonal Diagonalization</title>
    <exercise xml:id="orthogonal-ex1">
      <statement>
        <p>
          Let <m>P</m> be an <m>n\times n</m> matrix.
          Then the following are equivalent:
        </p>
        <p>
          (i) <m>P</m> is non-singular and <m>A^{-1}=A^T</m>.
        </p>
        <p>
          (ii) The rows of <m>P</m> are orthonormal vectors in <m>\R^n</m>. (iii) The columns of <m>P</m> are orthonormal vectors in <m>\R^n</m>.
        </p>
      </statement>
    </exercise>
    <definition>
      <statement>
        <p>
          A square matrix <m>P</m> is called an orthogonal matrix if it satisfies any one
          (and hence all)
          the conditions of Ex.
          <xref ref="orthogonal-ex1"></xref>.
        </p>
      </statement>
    </definition>
    <example>
      <statement>
        <p>
          (i) The matrix <m>\begin{pmatrix}\cos \theta \amp -\sin\theta\\\sin\theta \amp \cos\theta \end{pmatrix}</m> is an orthogonal matrix.
        </p>
        <p>
          (ii) <m>\left(\begin{array}{rrr} -\frac{1}{3} \, \sqrt{3} \amp \sqrt{\frac{2}{3}} \amp 0 \\ \frac{1}{3} \, \sqrt{3} \amp \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp -\sqrt{\frac{1}{2}} \\ \frac{1}{3} \, \sqrt{3} \amp \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp \sqrt{\frac{1}{2}} \end{array} \right)</m> is an orthogonal matrix.
        </p>
      </statement>
    </example>
    <definition>
      <statement>
        <p>
          An <m>n\times n</m> matrix is called
          <em>orthogonally diagonalizable </em>
          if there exists an orthogonal matrix <m>P</m> such that
          <m>P^{-1}AP</m> is a diagonal matrix.
        </p>
      </statement>
    </definition>
    <exercise>
      <statement>
        <p>
          Let <m>A</m> be a symmetric matrix and <m>\lambda_1</m> and
          <m>\lambda_2</m> are distinct eigenvalues of <m>A</m>.
          If <m>v_1</m> and <m>v_2</m> are eigenvectors corresponding to
          <m>\lambda_1</m> and <m>\lambda_2</m> respectively.
          Then <m>v_1</m> and <m>v_2</m> are orthogonal.
        </p>
      </statement>
    </exercise>
    <theorem>
      <statement>
        <p>
          Let <m>A</m> be an <m>n\times n</m> matrix.
          Then the following are equivalent.
        </p>
        <p>
          (i) <m>A</m> has an orthonormal set of eigenvectors.
        </p>
        <p>
          (ii) <m>A</m> is orthogonally diagonalizable.
        </p>
        <p>
          (iii) <m>A</m> is symmetric.
        </p>
      </statement>
    </theorem>
    <example>
      <statement>
        <p>
          Consider a matrix <m>A=\left(\begin{array}{rrr} 5 \amp  -2 \amp  -4 \\ -2 \amp  8 \amp  -2 \\ -4 \amp  -2 \amp  5 \end{array} \right)</m>.
          Clearly <m>A</m> is symmetric and hence it is orthogonally diagonalizable.
          The characteristic polynomial of <m>A</m> is
          <me>
            det(xI-A)=x^3 - 18*x^2 + 81*x=x(x-9)^2
          </me>.
        </p>
        <p>
          Hence <m>0, 9, 9</m> are eigenvalues of <m>A</m>.
          Its is easy to find that <m>v_1=(1, 1/2, 1)</m> is an eigenvector corresponding to the eigenvalue 0.
          <m>v_2=(1, 0, -1), v_2=(0, 1, -1/2)</m> are eigenvectors corresponding to eigenvalue 9.
          Hence <m>P:=\left(\begin{array}{rrr} 1 \amp  1 \amp  0 \\ \frac{1}{2} \amp  0 \amp  1 \\ 1 \amp  -1 \amp  -\frac{1}{2} \end{array} \right)</m>.
          Then
          <me>
            P^{-1}AP=\left(\begin{array}{rrr} 0 \amp  0 \amp  0 \\ 0 \amp  9 \amp  0 \\ 0 \amp  0 \amp  9 \end{array} \right)
          </me>.
        </p>
      </statement>
    </example>
    <exercise>
      <statement>
        <p>
          For the following matrices find an orthogonal matrix <m>P</m> such that
          <m>P^{-1}AP</m> is a diagonal matrix.
          <me>
            \begin{pmatrix}2 \amp  -1 \\-1 \amp  1 \end{pmatrix} , \begin{pmatrix}1 \amp  0 \amp  -1\\0 \amp  1 \amp  2\\-1 \amp  2 \amp  5 \end{pmatrix}
          </me>
        </p>
      </statement>
    </exercise>
    <exercise>
      <statement>
        <p>
          The following are equivalent for an <m>n\times n</m> matrix <m>A</m>.
        </p>
        <p>
          (i) <m>A</m> is orthogonal
        </p>
        <p>
          (ii) <m>\norm{Ax}=\norm{A}</m> for all <m>x\in \R^n</m>.
        </p>
        <p>
          (iii) <m>\norm{Ax-Ay}=\norm{x-y}</m> for all <m>x,y\in \R^n</m>.
        </p>
        <p>
          (iv) <m>Ax\cdot Ay = (Ay)^TAx=x\cdot y</m>.
        </p>
      </statement>
    </exercise>
  </subsection>
  <subsection>
    <title>QR-Factorization</title>
    <p>
      Let <m>A</m> be an <m>m\times n</m> matrix whose columns are <m>a_1,\ldots, a_n</m>.
      Further assume that columns of <m>A</m> are linearly independent.
      Using the Gram-Schmidt orthogonalization process
      <md>
        <mrow>u_1: \amp =\amp  a_1</mrow>
        <mrow>u_2: \amp =\amp  a_2 -\frac{a_2\cdot u_1}{\norm{u_1}^2}u_1</mrow>
        <mrow>\vdots \amp \amp  \vdots</mrow>
        <mrow>u_k: \amp =\amp  a_k-\frac{a_k\cdot u_1}{\norm{u_1}^2}u_1-\cdots -\frac{a_k\cdot u_{k-1}}{\norm{u_{k-1}}^2}u_{k-1}</mrow>
        <mrow>\vdots \amp \amp  \vdots</mrow>
      </md>
      and <m>q_k = \frac{u_k}{\norm{u_k}}, k=1,\ldots, n</m>.
      This implies
      <md>
        <mrow>\norm{u_k}q_k \amp = \amp u_k = a_k-\frac{a_k\cdot u_1}{\norm{u_1}^2}u_1-\cdots -\frac{a_k\cdot u_{k-1}}{\norm{u_{k-1}}^2}u_{k-1}</mrow>
        <mrow>\amp =\amp a_k-(a_k\cdot q_1)q_1-(a_k\cdot q_2)q_2+\cdots - (a_k\cdot q_{k-1})q_{k-1}</mrow>
      </md>.
    </p>
    <p>
      Hence
      <me>
        a_k = (a_k\cdot q_1)q_1+(a_k\cdot q_2)q_2+\cdots + (a_k\cdot q_{k-1})q_{k-1}+\norm{u_k}q_k, k=1,2,\ldots, n
      </me>.
    </p>
    <p>
      Thus we have
      <md>
        <mrow>a_1: \amp =\amp  \norm{u_1}q_1</mrow>
        <mrow>a_2: \amp =\amp  (a_2\cdot q_1)q_1+\norm{u_2}q_2</mrow>
        <mrow>\vdots \amp \amp  \vdots</mrow>
        <mrow>a_k: \amp =\amp  (a_k\cdot q_1)q_1+(a_k\cdot q_2)q_2+\cdots + (a_k\cdot q_{k-1})q_{k-1}+\norm{u_k}q_k</mrow>
        <mrow>\vdots \amp \amp  \vdots</mrow>
      </md>
    </p>
    <p>
      Thus
      <md>
        <mrow>A \amp =\amp  [a_1~a_2~a_3~\cdots~a_n]</mrow>
        <mrow>\amp =\amp  [q_1~q_2~q_3~\cdots~q_n] \begin{bmatrix}\norm{u_1} \amp  a_2\cdot q_1 \amp   a_3\cdot q_1\amp  \cdots \amp  a_n\cdot q_1\\ 0\amp   \norm{u_2} \amp  a_3\cdot q_2 \amp   \cdots \amp  a_n\cdot q_2\\ 0\amp 0   \amp  \norm{u_3}\amp   \cdot \amp  a_n\cdot q_3\\ \vdots \amp  \vdots \amp  \vdots \amp  \ddots \amp  \vdots\\ 0\amp  0 \amp  0 \amp  \cdots \amp  \norm{u_n} \end{bmatrix}</mrow>
        <mrow>\amp =\amp QR</mrow>
      </md>
    </p>
    <p>
      Here <m>Q</m> has orthogonal columns and <m>R</m> is upper triangular whose diagonal entries which are positive,
      hence non-singular.
      This is what is known as <m>QR</m> factorization.
      Thus we have the following result.
    </p>
    <theorem>
      <statement>
        <p>
          Every <m>m\times n</m> matrix with linearly independent columns has a <m>QR</m> factorization,
          <m>A=QR</m>,
          where columns of <m>Q</m> are orthonormal and <m>R</m> is an upper triangular matrix with positive diagonal entries.
        </p>
      </statement>
    </theorem>
    <example xml:id="QR-eg1">
      <statement>
        <p>
          let <m>A = \begin{pmatrix}0 \amp  1 \amp  1\\ 1 \amp  1 \amp  -2\\ 1 \amp  1 \amp  2 \end{pmatrix} = \begin{bmatrix}a_1 \amp  a_2 \amp  a_3 \end{bmatrix}</m>.
          Let us find the <m>QR</m> factorization of <m>A</m>.
          Note that columns of <m>A</m> are vectors
          <m>v_1, v_2, v_3</m> in the <xref ref="gram-schmidt-eg1">Example</xref>.
          We have found <m>q_1, q_2, q_3</m> in this example.
          Hence
          <me>
            Q = \begin{bmatrix}q_1 \amp  q_2 \amp  q_3 \end{bmatrix}  = \begin{pmatrix}0            \amp  1 \amp  0 \\ \frac{1}{\sqrt{2}} \amp  0 \amp  \frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \amp   0 \amp  \frac{1}{\sqrt{2}} \end{pmatrix}
          </me>.
          We also have
          <me>
            u_1= (0, 1, 1), u_2=(1,0,0), u_3=(0,-2,2)
          </me>
        </p>
        <p>
          Hence
          <me>
            R = \begin{pmatrix}\norm{u_1} \amp  a_2 \cdot q_1 \amp  a_3 \cdot q_1\\ 0 \amp  \norm{u_2} \amp  a_3 \cdot q_2\\ 0  \amp  0 \amp  \norm{u_3} \end{pmatrix}  = \begin{pmatrix}\sqrt{2} \amp  \sqrt(2) \amp  0\\ 0 \amp  1 \amp  1 \\ 0 \amp   0 \amp  2\sqrt{2} \end{pmatrix}
          </me>
        </p>
        <p>
          Note that once we have <m>Q</m>, then <m>R = Q^TA</m>.
        </p>
      </statement>
    </example>
    <example>
      <statement>
        <p>
          Find the QR-factorization of <m>A=\left(\begin{array}{rrr} -1 \amp  1 \amp  -1 \\ 1 \amp  -1 \amp  0 \\ 0 \amp  0 \amp  2 \\ 1 \amp  1 \amp  1 \end{array} \right)=\begin{bmatrix}a_1\amp  a_2\amp  a_3 \end{bmatrix}</m>.
          It is easy to check that columns of <m>A</m> are linearly independent.
          In fact, columns of <m>A</m> are rows of the matrix defined in the Example<nbsp/><xref ref="gram-schmidt-eg2"/>.
          From this example, we have
          <me>
            u_1 = \begin{pmatrix}-1\\ 1\\ 0\\ 1 \end{pmatrix} , q_1 = \begin{pmatrix}-\frac{1}{\sqrt{3}}\\\frac{1}{\sqrt{3}}\\0\\\frac{1}{\sqrt{3}} \end{pmatrix} u_2 = \begin{pmatrix}2/3\\ -2/3\\ 0\\ 4/3 \end{pmatrix} , q_2 = \begin{pmatrix}\frac{1}{\sqrt{6}}\\ -\frac{1}{\sqrt{6}}\\0\\\sqrt{\frac{2}{3}} \end{pmatrix} u_3 = \begin{pmatrix}-1/2\\-1/2\\2\\0 \end{pmatrix} , q_3 = \begin{pmatrix}-\frac{1}{3} \, \sqrt{\frac{1}{2}}\\-\frac{1}{3} \, \sqrt{\frac{1}{2}}\\\frac{4}{3} \, \sqrt{\frac{1}{2}}\\0 \end{pmatrix}
          </me>
        </p>
        <p>
          Hence
          <me>
            Q = [q_1~q_2~q_3]=\left(\begin{array}{rrr} -\frac{1}{3} \, \sqrt{3} \amp  \frac{1}{2} \, \sqrt{\frac{2}{3}} \amp  -\frac{1}{3} \, \sqrt{\frac{1}{2}} \\ \frac{1}{3} \, \sqrt{3} \amp  -\frac{1}{2} \, \sqrt{\frac{2}{3}} \amp  -\frac{1}{3} \, \sqrt{\frac{1}{2}} \\ 0 \amp  0 \amp  \frac{4}{3} \, \sqrt{\frac{1}{2}} \\ \frac{1}{3} \, \sqrt{3} \amp  \sqrt{\frac{2}{3}} \amp  0 \end{array} \right)
          </me>.
          Also
          <me>
            R = Q^TA=\left(\begin{array}{rrr} \sqrt{3} \amp  -\frac{1}{3} \, \sqrt{3} \amp  \frac{2}{3} \, \sqrt{3} \\ 0 \amp  2 \, \sqrt{\frac{2}{3}} \amp  \frac{1}{2} \, \sqrt{\frac{2}{3}} \\ 0 \amp  0 \amp  3 \, \sqrt{\frac{1}{2}} \end{array} \right)
          </me>.
        </p>
      </statement>
    </example>
    <remark>
      <p>
        If a matrix <m>A</m> has independent rows,
        then we apply <m>QR</m> factorization to <m>A^T</m>.
        Thus
        <me>
          A^T = (QR)^T=R^TQ^T=LP
        </me>
        where <m>L</m> is the the invertible lower triangular matrix with positive diagonal entries and and <m>P</m> has orthogonal rows.
      </p>
    </remark>
    <remark>
      <p>
        In case a matrix <m>A</m> has linearly independent columns then the <m>QR</m> factorization is unique.
        That is, if <m>A= Q_1R_1=Q_2R_2</m>,
        then <m>Q_1=Q_2</m> and <m>R_1=R_2</m>.
      </p>
    </remark>
    <exercise>
      <statement>
        <p>
          Find the QR-factorization of the following matrices:
          <me>
            \begin{bmatrix}2 \amp  1 \\ 1 \amp  11 \end{bmatrix} ,  \begin{bmatrix}1 \amp  -1 \amp  1\\ 2 \amp  0  \amp  1\\2 \amp  1 \amp  -2 \end{bmatrix} , \begin{bmatrix}1 \amp  1 \amp  0 \\-1 \amp  0 1\\0 \amp  1 \amp  1\\1 \amp  -1 \amp  0 \end{bmatrix}
          </me>
        </p>
      </statement>
    </exercise>
  </subsection>
</section>